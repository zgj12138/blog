<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[学习总结 2017-10]]></title>
    <url>%2F2017%2F10%2F05%2F%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%932017-10%2F</url>
    <content type="text"><![CDATA[前言记录个人在10月的记录总结 2017-10-05windows查看端口占用和杀进程查看端口占用netstat -aon|findstr &quot;49157&quot; 找到进程pid后，杀掉该进程 taskkill /pid 12188 /f 2017-10-10Java中的十个”单行代码编程”(One Liner)本文列举了十个使用一行代码即可独立完成(不依赖其他代码)的业务逻辑，主要依赖的是Java8中的Lambda和Stream等新特性以及try-with-resources、JAXB等。 对列表/数组中的每个元素都乘以2123// Range是半开区间int[] ia = range(1, 10).map(i -&gt; i * 2).toArray();List&lt;Integer&gt; result = range(1, 10).map(i -&gt; i * 2).boxed().collect(toList()); 计算集合/数组中的数字之和1234range(1, 1000).sum();range(1, 1000).reduce(0, Integer::sum);Stream.iterate(0, i -&gt; i + 1).limit(1000).reduce(0, Integer::sum);IntStream.iterate(0, i -&gt; i + 1).limit(1000).reduce(0, Integer::sum); 验证字符串是否包含集合中的某一字符串12345final List&lt;String&gt; keywords = Arrays.asList(&quot;brown&quot;, &quot;fox&quot;, &quot;dog&quot;, &quot;pangram&quot;);final String tweet = &quot;The quick brown fox jumps over a lazy dog. #pangram http://www.rinkworks.com/words/pangrams.shtml&quot;;keywords.stream().anyMatch(tweet::contains);keywords.stream().reduce(false, (b, keyword) -&gt; b || tweet.contains(keyword), (l, r) -&gt; l || r); 读取文件内容 原作者认为try with resources也是一种单行代码编程。 1234567891011try (BufferedReader reader = new BufferedReader(new FileReader("data.txt"))) &#123; String fileText = reader.lines().reduce("", String::concat);&#125;try (BufferedReader reader = new BufferedReader(new FileReader("data.txt"))) &#123; List&lt;String&gt; fileLines = reader.lines().collect(toCollection(LinkedList&lt;String&gt;::new));&#125;try (Stream&lt;String&gt; lines = Files.lines(new File("data.txt").toPath(), Charset.defaultCharset())) &#123; List&lt;String&gt; fileLines = lines.collect(toCollection(LinkedList&lt;String&gt;::new));&#125; 输出歌曲《Happy Birthday to You!》 - 根据集合中不同的元素输出不同的字符串1range(1, 5).boxed().map(i -&gt; &#123; out.print("Happy Birthday "); if (i == 3) return "dear NAME"; else return "to You"; &#125;).forEach(out::println); 过滤并分组集合中的数字1Map&lt;String, List&lt;Integer&gt;&gt; result = Stream.of(49, 58, 76, 82, 88, 90).collect(groupingBy(forPredicate(i -&gt; i &gt; 60, "passed", "failed"))); 获取并解析xml协议的Web Service12FeedType feed = JAXB.unmarshal(new URL(&quot;http://search.twitter.com/search.atom?&amp;q=java8&quot;), FeedType.class);JAXB.marshal(feed, System.out); 获得集合中最小/最大的数字1234567int min = Stream.of(14, 35, -7, 46, 98).reduce(Integer::min).get();min = Stream.of(14, 35, -7, 46, 98).min(Integer::compare).get();min = Stream.of(14, 35, -7, 46, 98).mapToInt(Integer::new).min();int max = Stream.of(14, 35, -7, 46, 98).reduce(Integer::max).get();max = Stream.of(14, 35, -7, 46, 98).max(Integer::compare).get();max = Stream.of(14, 35, -7, 46, 98).mapToInt(Integer::new).max(); 并行处理1long result = dataList.parallelStream().mapToInt(line -&gt; processItem(line)).sum(); 集合上的各种查询(LINQ in Java)12345678910111213141516List&lt;Album&gt; albums = Arrays.asList(unapologetic, tailgates, red);//筛选出至少有一个track评级4分以上的专辑，并按照名称排序后打印出来。albums.stream() .filter(a -&gt; a.tracks.stream().anyMatch(t -&gt; (t.rating &gt;= 4))) .sorted(comparing(album -&gt; album.name)) .forEach(album -&gt; System.out.println(album.name));//合并所有专辑的trackList&lt;Track&gt; allTracks = albums.stream() .flatMap(album -&gt; album.tracks.stream()) .collect(toList());//根据track的评分对所有track分组Map&lt;Integer, List&lt;Track&gt;&gt; tracksByRating = allTracks.stream() .collect(groupingBy(Track::getRating)); 2017-10-17Jackson的@JsonFormat注解日期少一天问题项目中使用了@JsonFormat来进行格式化日期，却意外发现日期少了一天，比如数据库存的日期是2017-10-15,转成json则变成了2017-10-14于是查资料，发现这个注解有个坑,JsonFormat默认是不带时区解决办法:1234@JsonFormat(pattern="yyyy-MM-dd") public Date getRegistDate() &#123; return this.registDate; &#125; 改成1234@JsonFormat(pattern="yyyy-MM-dd",timezone="GMT+8") public Date getRegistDate() &#123; return this.registDate; &#125; 加上时区即可,中国是东八区 2017-10-18mysql如何解决幻读问题mysql默认的事务隔离级别是repeatable-read,也就是可重复读，按照sql事务标准的话，这个隔壁级别是可以解决脏读和不可重复的问题的，但是不能解决幻读的问题，但是mysql却解决了幻读这个问题，那到底是怎么实现的呢？ 官方文档123http://dev.mysql.com/doc/refman/5.0/en/innodb-record-level-locks.htmlBy default, InnoDB operates in REPEATABLE READ transaction isolation level and with the innodb_locks_unsafe_for_binlog system variable disabled. In this case, InnoDB uses next-key locks for searches and index scans, which prevents phantom rows (see Section 13.6.8.5, “Avoiding the Phantom Problem Using Next-Key Locking”). 准备的理解是，当隔离级别是可重复读，且禁用innodb_locks_unsafe_for_binlog的情况下，在搜索和扫描index的时候使用的next-key locks可以避免幻读。 关键点在于，是InnoDB默认对一个普通的查询也会加next-key locks，还是说需要应用自己来加锁呢？如果单看这一句，可能会以为InnoDB对普通的查询也加了锁，如果是，那和序列化（SERIALIZABLE）的区别又在哪里呢？ MySQL manual里还有一段：1234513.2.8.5. Avoiding the Phantom Problem Using Next-Key Locking (http://dev.mysql.com/doc/refman/5.0/en/innodb-next-key-locking.html)To prevent phantoms, InnoDB uses an algorithm called next-key locking that combines index-row locking with gap locking.You can use next-key locking to implement a uniqueness check in your application: If you read your data in share mode and do not see a duplicate for a row you are going to insert, then you can safely insert your row and know that the next-key lock set on the successor of your row during the read prevents anyone meanwhile inserting a duplicate for your row. Thus, the next-key locking enables you to “lock” the nonexistence of something in your table. 我的理解是说，InnoDB提供了next-key locks，但需要应用程序自己去加锁。manual里提供一个例子：1SELECT * FROM child WHERE id &gt; 100 FOR UPDATE; SHOW ENGINE INNODB STATUS 来查看是否给表加上了锁。 mySQL manual里对可重复读里的锁的详细解释：123http://dev.mysql.com/doc/refman/5.0/en/set-transaction.html#isolevel_repeatable-readFor locking reads (SELECT with FOR UPDATE or LOCK IN SHARE MODE),UPDATE, and DELETE statements, locking depends on whether the statement uses a unique index with a unique search condition, or a range-type search condition. For a unique index with a unique search condition, InnoDB locks only the index record found, not the gap before it. For other search conditions, InnoDB locks the index range scanned, using gap locks or next-key (gap plus index-record) locks to block insertions by other sessions into the gaps covered by the range. 实验一致性读和提交读，先看实验，实验四：1234567891011121314151617181920212223242526272829303132333435363738394041424344t Session A Session B|| START TRANSACTION; START TRANSACTION;|| SELECT * FROM t_bitfly;| +----+-------+| | id | value || +----+-------+| | 1 | a || +----+-------+| INSERT INTO t_bitfly| VALUES (2, &apos;b&apos;);| COMMIT;|| SELECT * FROM t_bitfly;| +----+-------+| | id | value || +----+-------+| | 1 | a || +----+-------+|| SELECT * FROM t_bitfly LOCK IN SHARE MODE;| +----+-------+| | id | value || +----+-------+| | 1 | a || | 2 | b || +----+-------+|| SELECT * FROM t_bitfly FOR UPDATE;| +----+-------+| | id | value || +----+-------+| | 1 | a || | 2 | b || +----+-------+|| SELECT * FROM t_bitfly;| +----+-------+| | id | value || +----+-------+| | 1 | a || +----+-------+v 如果使用普通的读，会得到一致性的结果，如果使用了加锁的读，就会读到“最新的”“提交”读的结果。 本身，可重复读和提交读是矛盾的。在同一个事务里，如果保证了可重复读，就会看不到其他事务的提交，违背了提交读；如果保证了提交读，就会导致前后两次读到的结果不一致，违背了可重复读。 可以这么讲，InnoDB提供了这样的机制，在默认的可重复读的隔离级别里，可以使用加锁读去查询最新的数据。123456http://dev.mysql.com/doc/refman/5.0/en/innodb-consistent-read.htmlIf you want to see the “freshest” state of the database, you should use either the READ COMMITTED isolation level or a locking read:SELECT * FROM t_bitfly LOCK IN SHARE MODE;------ mvcc在官方文档中写道123http://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.htmlA consistent read means that InnoDB uses multi-versioning to present to a query a snapshot of the database at a point in time. The query sees the changes made by transactions that committed before that point of time, and no changes made by later or uncommitted transactions. The exception to this rule is that the query sees the changes made by earlier statements within the same transaction. This exception causes the following anomaly: If you update some rows in a table, a SELECT sees the latest version of the updated rows, but it might also see older versions of any rows. If other sessions simultaneously update the same table, the anomaly means that you might see the table in a state that never existed in the database. 一致性读是通过 MVCC 为查询提供了一个基于时间的点的快照。这个查询只能看到在自己之前提交的数据，而在查询开始之后提交的数据是不可以看到的。一个特例是,这个查询可以看到于自己开始之后的同一个事务产生的变化。这个特例会产生一些反常的现象1If the transaction isolation level is REPEATABLE READ (the default level), all consistent reads within the same transaction read the snapshot established by the first such read in that transaction. You can get a fresher snapshot for your queries by committing the current transaction and after that issuing new queries. 在默认隔离级别REPEATABLE READ下，同一事务的所有一致性读只会读取第一次查询时创建的快照 结论结论：MySQL InnoDB的可重复读并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是next-key locks。 MySQL MVCC简介什么是MVCCMVCC是一种多版本并发控制机制。 MVCC是为了解决什么问题?大多数的MYSQL事务型存储引擎,如,InnoDB，Falcon以及PBXT都不使用一种简单的行锁机制.事实上,他们都和MVCC–多版本并发控制来一起使用.大家都应该知道,锁机制可以控制并发操作,但是其系统开销较大,而MVCC可以在大多数情况下代替行级锁,使用MVCC,能降低其系统开销. MVCC实现MVCC是通过保存数据在某个时间点的快照来实现的. 不同存储引擎的MVCC. 不同存储引擎的MVCC实现是不同的,典型的有乐观并发控制和悲观并发控制. 具体实现分析下面,我们通过InnoDB的MVCC实现来分析MVCC使怎样进行并发控制的.InnoDB的MVCC,是通过在每行记录后面保存两个隐藏的列来实现的,这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，没开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID.下面看一下在REPEATABLE READ隔离级别下,MVCC具体是如何操作的. 简单的小例子123create table zhang( id int primary key auto_increment, name varchar(20)); 假设系统的版本号从1开始. INSERTInnoDB为新插入的每一行保存当前系统版本号作为版本号.第一个事务ID为1；12345start transaction;insert into yang values(NULL,&apos;zhang&apos;) ;insert into yang values(NULL,&apos;guo&apos;);insert into yang values(NULL,&apos;ji&apos;);commit; 对应在数据中的表如下(后面两列是隐藏列,我们通过查询语句并看不到) id name 创建时间(事务ID) 删除时间(事务ID) 1 zhang 1 undefined 2 guo 1 undefined 3 ji 1 undefined SELECTInnoDB会根据以下两个条件检查每行记录:a.InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的.b.行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除.只有a,b同时满足的记录，才能返回作为查询结果. DELETEInnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识.看下面的具体例子分析:第二个事务,ID为2;1234start transaction;select * from zhang; //(1)select * from zhang; //(2)commit; 假设1 假设在执行这个事务ID为2的过程中,刚执行到(1),这时,有另一个事务ID为3往这个表里插入了一条数据;第三个事务ID为3;123start transaction;insert into zhang values(NULL,&apos;tian&apos;);commit; 这时表中的数据如下: id name 创建时间(事务ID) 删除时间(事务ID) 1 zhang 1 undefined 2 guo 1 undefined 3 ji 1 undefined 4 tian 3 undefined 然后接着执行事务2中的(2),由于id=4的数据的创建时间(事务ID为3),执行当前事务的ID为2,而InnoDB只会查找事务ID小于等于当前事务ID的数据行,所以id=4的数据行并不会在执行事务2中的(2)被检索出来,在事务2中的两条select 语句检索出来的数据都只会下表: id name 创建时间(事务ID) 删除时间(事务ID) 1 zhang 1 undefined 2 guo 1 undefined 3 ji 1 undefined 假设2 假设在执行这个事务ID为2的过程中,刚执行到(1),假设事务执行完事务3后，接着又执行了事务4;第四个事务:123start transaction; delete from zhang where id=1;commit; 此时数据库中的表如下: id name 创建时间(事务ID) 删除时间(事务ID) 1 zhang 1 4 2 guo 1 undefined 3 ji 1 undefined 4 tian 3 undefined 接着执行事务ID为2的事务(2),根据SELECT 检索条件可以知道,它会检索创建时间(创建事务的ID)小于当前事务ID的行和删除时间(删除事务的ID)大于当前事务的行,而id=4的行上面已经说过,而id=1的行由于删除时间(删除事务的ID)大于当前事务的ID,所以事务2的(2)select * from yang也会把id=1的数据检索出来.所以,事务2中的两条select 语句检索出来的数据都如下: id name 创建时间(事务ID) 删除时间(事务ID) 1 zhang 1 undefined 2 guo 1 undefined 3 ji 1 undefined UPDATEInnoDB执行UPDATE，实际上是新插入了一行记录，并保存其创建时间为当前事务的ID，同时保存当前事务ID到要UPDATE的行的删除时间. 假设3 假设在执行完事务2的(1)后又执行,其它用户执行了事务3,4,这时，又有一个用户对这张表执行了UPDATE操作:第5个事务:123start transaction;update yang set name=&apos;Guo&apos; where id=2;commit; 根据update的更新原则:会生成新的一行,并在原来要修改的列的删除时间列上添加本事务ID,得到表如下: id name 创建时间(事务ID) 删除时间(事务ID) 1 zhang 1 4 2 guo 1 5 3 ji 1 undefined 4 tian 3 undefined 5 Guo 5 undefined 继续执行事务2的(2),根据select 语句的检索条件,得到下表: id name 创建时间(事务ID) 删除时间(事务ID) 1 zhang 1 undefined 2 guo 1 undefined 3 ji 1 undefined 还是和事务2中(1)select 得到相同的结果. 2017-10-23《Java8实战》笔记 行为参数化，就是一个方法接受多个不同的做为参数，并在内部使用它们，完成不同行为的能力。 行为参数化可以让代码更好的适应不断变化的要求，减轻代码的工作量。 传递代码，就是将新行为作为参数传递给方法。对比之前的实现方式，是通过类-》匿名内部类-》lambda表达式不断演变的，是代码变得越来越简洁]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java Socket编程那些事（2）]]></title>
    <url>%2F2017%2F09%2F10%2FJava%20Socket%20%E7%BC%96%E7%A8%8B%E9%82%A3%E4%BA%9B%E4%BA%8B(2)%2F</url>
    <content type="text"><![CDATA[前言在上一篇博客中，我们使用了BIO,也就是同步阻塞式IO实现了Socket通信。Java Socket编程那些事(1))现在我们使用jdk1.4之后的NIO来实现，NIO(new io / no-blocking io)，同步非阻塞IO。 基本原理服务端打开一个通道（ServerSocketChannel），并向通道中注册一个选择器（Selector），这个选择器是与一些感兴趣的操作的标识（SelectionKey，即通过这个标识可以定位到具体的操作，从而进行响应的处理）相关联的，然后基于选择器（Selector）轮询通道（ServerSocketChannel）上注册的事件，并进行相应的处理。客户端在请求与服务端通信时，也可以向服务器端一样注册（比服务端少了一个SelectionKey.OP_ACCEPT操作集合），并通过轮询来处理指定的事件，而不必阻塞。 服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107package com.richstonedt.socket;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;import java.util.Iterator;/** * @author zhangguoji * @date 2017/9/8 20:47 */public class NIOServer &#123; private Selector selector; /** * 获得一个ServerSocket通道，并对该通道做一些初始化的工作 * @param port 绑定的端口号 * @throws IOException */ public void initServer(int port) throws IOException &#123; // 获得一个ServerSocket通道 ServerSocketChannel serverChannel = ServerSocketChannel.open(); // 设置通道为非阻塞 serverChannel.configureBlocking(false); // 将该通道对应的ServerSocket绑定到本地port端口 serverChannel.socket().bind(new InetSocketAddress(port)); // 获得一个通道管理器 this.selector = Selector.open(); //将通道管理器和该通道绑定，并为该通道注册SelectionKey.OP_ACCEPT事件,注册该事件后， //当该事件到达时，selector.select()会返回，如果该事件没到达selector.select()会一直阻塞。 serverChannel.register(selector, SelectionKey.OP_ACCEPT); &#125; /** * 采用轮询的方式监听selector上是否有需要处理的事件，如果有，则进行处理 * * @throws IOException */ public void listen() throws IOException &#123; System.out.println("服务端启动成功！"); // 轮询访问selector while (true) &#123; //当注册的事件到达时，方法返回；否则,该方法会一直阻塞 selector.select(); // 获得selector中选中的项的迭代器，选中的项为注册的事件 Iterator&lt;SelectionKey&gt; iterator = this.selector.selectedKeys().iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); // 删除已选的key,以防重复处理 iterator.remove(); // 客户端请求连接事件 if (key.isAcceptable()) &#123; ServerSocketChannel server = (ServerSocketChannel) key .channel(); // 获得和客户端连接的通道 SocketChannel channel = server.accept(); // 设置成非阻塞 channel.configureBlocking(false); //在这里可以给客户端发送信息 channel.write(ByteBuffer.wrap("向客户端发送了一条信息".getBytes())); //在和客户端连接成功之后，为了可以接收到客户端的信息，需要给通道设置读的权限。 channel.register(this.selector, SelectionKey.OP_READ); // 获得了可读的事件 &#125; else if (key.isReadable()) &#123; read(key); &#125; &#125; &#125; &#125; /** * 处理读取客户端发来的信息 的事件 * @param key * @throws IOException */ public void read(SelectionKey key) throws IOException &#123; // 服务器可读取消息:得到事件发生的Socket通道 SocketChannel channel = (SocketChannel) key.channel(); // 创建读取的缓冲区 ByteBuffer buffer = ByteBuffer.allocate(512); channel.read(buffer); byte[] data = buffer.array(); String msg = new String(data).trim(); System.out.println("服务端收到信息：" + msg); ByteBuffer outBuffer = ByteBuffer.wrap(msg.getBytes()); channel.write(outBuffer);// 将消息回送给客户端 &#125; /** * 启动服务端测试 * * @throws IOException */ public static void main(String[] args) throws IOException &#123; NIOServer server = new NIOServer(); server.initServer(8000); server.listen(); &#125;&#125; 服务端连接过程1、创建ServerSocketChannel实例serverSocketChannel，并bind到指定端口。2、创建Selector实例selector；3、将serverSocketChannel注册到selector，并指定事件OP_ACCEPT。4、while循环执行：4.1、调用select方法，该方法会阻塞等待，直到有一个或多个通道准备好了I/O操作或等待超时。4.2、获取选取的键列表；4.3、循环键集中的每个键：4.3.a、获取通道，并从键中获取附件（如果添加了附件）；4.3.b、确定准备就绪的操纵并执行，如果是accept操作，将接收的信道设置为非阻塞模式，并注册到选择器；4.3.c、如果需要，修改键的兴趣操作集；4.3.d、从已选键集中移除键 客户端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.richstonedt.socket;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.util.Iterator;/** * NIO客户端 * * @author zhangguoji * @date 2017/9/8 21:43 */public class NIOClient &#123; //通道管理器 private Selector selector; /** * 获得一个Socket通道，并对该通道做一些初始化的工作 * * @param ip 连接的服务器的ip * @param port 连接的服务器的端口号 * @throws IOException */ public void initClient(String ip, int port) throws IOException &#123; // 获得一个Socket通道 SocketChannel channel = SocketChannel.open(); // 设置通道为非阻塞 channel.configureBlocking(false); // 获得一个通道管理器 this.selector = Selector.open(); // 客户端连接服务器,其实方法执行并没有实现连接，需要在listen（）方法中调 //用channel.finishConnect();才能完成连接 channel.connect(new InetSocketAddress(ip, port)); //将通道管理器和该通道绑定，并为该通道注册SelectionKey.OP_CONNECT事件。 channel.register(selector, SelectionKey.OP_CONNECT); &#125; /** * 采用轮询的方式监听selector上是否有需要处理的事件，如果有，则进行处理 * * @throws IOException */ public void listen() throws IOException &#123; // 轮询访问selector while (true) &#123; selector.select(); // 获得selector中选中的项的迭代器 Iterator&lt;SelectionKey&gt; iterator = this.selector.selectedKeys().iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); // 删除已选的key,以防重复处理 iterator.remove(); // 连接事件发生 if (key.isConnectable()) &#123; SocketChannel channel = (SocketChannel) key .channel(); // 如果正在连接，则完成连接 if (channel.isConnectionPending()) &#123; channel.finishConnect(); &#125; // 设置成非阻塞 channel.configureBlocking(false); //在这里可以给服务端发送信息哦 channel.write(ByteBuffer.wrap("向服务端发送了一条信息".getBytes())); //在和服务端连接成功之后，为了可以接收到服务端的信息，需要给通道设置读的权限。 channel.register(this.selector, SelectionKey.OP_READ); // 获得了可读的事件 &#125; else if (key.isReadable()) &#123; read(key); &#125; &#125; &#125; &#125; /** * 处理读取服务端发来的信息 的事件 * * @param key * @throws IOException */ public void read(SelectionKey key) throws IOException &#123; // 客户端可读取消息:得到事件发生的Socket通道 SocketChannel channel = (SocketChannel) key.channel(); // 创建读取的缓冲区 ByteBuffer buffer = ByteBuffer.allocate(512); channel.read(buffer); byte[] data = buffer.array(); String msg = new String(data).trim(); System.out.println("客户端收到信息：" + msg); ByteBuffer outBuffer = ByteBuffer.wrap(msg.getBytes()); channel.write(outBuffer);// 将消息回送给服务端 &#125; /** * 启动客户端测试 * * @throws IOException */ public static void main(String[] args) throws IOException &#123; NIOClient client = new NIOClient(); client.initClient("localhost", 8000); client.listen(); &#125;&#125; 客户端连接过程：(和服务器端类似)1、创建SocketChannel实例socketChannel，并连接到服务器端口2、创建Selector实例selector；3、将socketChannel注册到selector，并指定事件OP_CONNECT。4、while循环执行：4.1、调用select方法，该方法会阻塞等待，直到有一个或多个通道准备好了I/O操作或等待超时。4.2、获取选取的键列表；4.3、循环键集中的每个键：4.3.a、获取通道，并从键中获取附件（如果添加了附件）；4.3.b、确定准备就绪的操纵并执行，如果是accept操作，将接收的信道设置为非阻塞模式，并注册到选择器；4.3.c、如果需要，修改键的兴趣操作集；4.3.d、从已选键集中移除键 运行结果最终这两段代码的运行结果就是客户端和服务器之间不断发送信息server:1234567891011121314151617服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息服务端收到信息：向服务端发送了一条信息向客户端发送了一条信息... client:123456789101112131415161718192021客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息客户端收到信息：向服务端发送了一条信息向客户端发送了一条信息... 实现原理其实Java的NIO使用了IO多路复用，，I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。目前支持的IO多路复用有select,poll和epoll。与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 selectselect本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是： select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024。 一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048. 对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低。 当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度，不管哪个Socket是活跃的，都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。 需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。 poll基本原理：poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样有一个缺点： 大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。 poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。 epollepoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。基本原理：epoll支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。epoll的优点： 没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。 只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。 epoll原理 epoll是Linux下的一种IO多路复用技术，可以非常高效的处理数以百万计的socket句柄。c封装后的3个epoll系统调用 int epoll_create(int size)epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。 nt epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)epoll_ctl可以操作epoll_create创建的epoll，如将socket句柄加入到epoll中让其监控，或把epoll正在监控的某个socket句柄移出epoll。 int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout)epoll_wait在调用时，在给定的timeout时间内，所监控的句柄中有事件发生时，就返回用户态的进程。 大概看看epoll内部是怎么实现的： epoll初始化时，会向内核注册一个文件系统，用于存储被监控的句柄文件，调用epoll_create时，会在这个文件系统中创建一个file节点。同时epoll会开辟自己的内核高速缓存区，以红黑树的结构保存句柄，以支持快速的查找、插入、删除。还会再建立一个list链表，用于存储准备就绪的事件。当执行epoll_ctl时，除了把socket句柄放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后，就把socket插入到就绪链表里。当epoll_wait调用时，仅仅观察就绪链表里有没有数据，如果有数据就返回，否则就sleep，超时时立刻返回。epoll的两种工作模式： LT：level-trigger，水平触发模式，只要某个socket处于readable/writable状态，无论什么时候进行epoll_wait都会返回该socket。ET：edge-trigger，边缘触发模式，只有某个socket从unreadable变为readable或从unwritable变为writable时，epoll_wait才会返回该socket。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>io</tag>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal解析]]></title>
    <url>%2F2017%2F08%2F18%2FThreadLocal%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言在各大公司招聘笔试和面试题题中，都遇到了很多ThreadLocal的问题，最近博主在面试的时候也被两次问到过这个问题，之前也在网上看到过一些此类的文章，其中有很多文章将ThreadLocal与线程同步机制混为一谈，特别注意的是ThreadLocal与线程同步无关，并不是为了解决多线程共享变量问题，我们今天就来研究一下ThreadLocal的原理 ThreadLocal是什么我们首先来看一下JDK中的源码是怎么写的 This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its {@code get} or {@code set} method) has its own, independently initialized copy of the variable. {@code ThreadLocal} instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID 翻译过来就是： ThreadLocal类用来提供线程内部的局部变量。这种变量在多线程环境下访问(通过get或set方法访问)时能保证各个线程里的变量相对独立于其他线程内的变量。ThreadLocal实例通常来说都是private static类型的，用于关联线程和线程的上下文。 总结来说就是：ThreadLocal的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度。ThreadLocal不是为了解决多线程访问共享变量，而是为每个线程创建一个单独的变量副本，提供了保持对象的方法和避免参数传递的复杂性。 ThreadLocal的主要应用场景为按线程多实例（每个线程对应一个实例）的对象的访问，并且这个对象很多地方都要用到。例如：同一个网站登录用户，每个用户服务器会为其开一个线程，每个线程中创建一个ThreadLocal，里面存用户基本信息等，在很多页面跳转时，会显示用户信息或者得到用户的一些信息等频繁操作，这样多线程之间并没有联系而且当前线程也可以及时获取想要的数据。 实现原理ThreadLocal可以看做是一个容器，容器里面存放着属于当前线程的变量。ThreadLocal类提供了四个对外开放的接口方法，这也是用户操作ThreadLocal类的基本方法： public void set(Object value)设置当前线程的线程局部变量的值。 public Object get()该方法返回当前线程所对应的线程局部变量。 public void remove()将当前线程局部变量的值删除，目的是为了减少内存的占用，该方法是JDK 5.0新增的方法。需要指出的是，当线程结束后，对应该线程的局部变量将自动被垃圾回收，所以显式调用该方法清除线程的局部变量并不是必须的操作，但它可以加快内存回收的速度。 protected Object initialValue()返回该线程局部变量的初始值，该方法是一个protected的方法，显然是为了让子类覆盖而设计的。这个方法是一个延迟调用方法，在线程第1次调用get()或set(Object)时才执行，并且仅执行1次，ThreadLocal中的缺省实现直接返回一个null。 可以通过上述的几个方法实现ThreadLocal中变量的访问，数据设置，初始化以及删除局部变量，那ThreadLocal内部是如何为每一个线程维护变量副本的呢？ 其实在ThreadLocal类中有一个静态内部类ThreadLocalMap(其类似于Map)，用键值对的形式存储每一个线程的变量副本，ThreadLocalMap中元素的key为当前ThreadLocal对象，而value对应线程的变量副本，每个线程可能存在多个ThreadLocal。 源代码get()方法12345678910111213public T get() &#123; Thread t = Thread.currentThread();//当前线程 ThreadLocalMap map = getMap(t);//获取当前线程对应的ThreadLocalMap if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this);//获取对应ThreadLocal的变量值 if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();//若当前线程还未创建ThreadLocalMap，则返回调用此方法并在其中调用createMap方法进行创建并返回初始值。&#125; set()方法12345678910//设置变量的值public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; setInitialValue()方法12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; createMap()方法12345678/**为当前线程创建一个ThreadLocalMap的threadlocals,并将第一个值存入到当前map中@param t the current thread@param firstValue value for the initial entry of the map*/void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; remove()方法123456//删除当前线程中ThreadLocalMap对应的ThreadLocalpublic void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this);&#125; ThreadLocalMapThreadLocalMap是定制的hashMap，仅用于维护当前线程的本地变量值。仅ThreadLocal类对其有操作权限，是Thread的私有属性。为避免占用空间较大或生命周期较长的数据常驻于内存引发一系列问题，hash table的key是弱引用WeakReferences。当空间不足时，会清理未被引用的entry。 getMap(t)返回当前线程的成员变量ThreadLocalMap（Thread的成员变量有ThreadLocalMap，这一点可以查看Thread的源码，如下）很明确的说明了ThreadLocal属于线程，ThreadLocalMap由ThreadLocal持有，说到底，ThreadLocalMap 也是线程所持有。每个线程Thread都有自己的ThreadLocalMap。下面看一下该类的源代码：12345678910111213141516171819static class ThreadLocalMap &#123; //map中的每个节点Entry,其键key是ThreadLocal并且还是弱引用，这也导致了后续会产生内存泄漏问题的原因。 static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; /** * 初始化容量为16，以为对其扩充也必须是2的指数 */ private static final int INITIAL_CAPACITY = 16; /** * 真正用于存储线程的每个ThreadLocal的数组，将ThreadLocal和其对应的值包装为一个Entry。 */ private Entry[] table; ///....其他的方法和操作都和map的类似 注意问题 ThreadLocal并未解决多线程访问共享对象的问题； ThreadLocal并不是每个线程拷贝一个对象，而是直接new（新建）一个； 如果ThreadLocal.set()的对象是多线程共享的，那么还是涉及并发问题。 ThreadLocal的内存泄漏在上面提到过，每个thread中都存在一个map, map的类型是ThreadLocal.ThreadLocalMap. Map中的key为一个threadlocal实例. 这个Map的确使用了弱引用,不过弱引用只是针对key. 每个key都弱引用指向threadlocal. 当把threadlocal实例置为null以后,没有任何强引用指向threadlocal实例,所以threadlocal将会被gc回收. 但是,我们的value却不能回收,因为存在一条从current thread连接过来的强引用. 只有当前thread结束以后, current thread就不会存在栈中,强引用断开, Current Thread, Map, value将全部被GC回收。 所以得出一个结论就是只要这个线程对象被gc回收，就不会出现内存泄露，但在threadLocal设为null和线程结束这段时间不会被回收的，就发生了我们认为的内存泄露。其实这是一个对概念理解的不一致，也没什么好争论的。最要命的是线程对象不被回收的情况，这就发生了真正意义上的内存泄露。比如使用线程池的时候，线程结束是不会销毁的，会再次使用的。就可能出现内存泄露。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>threadlocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之——原型模式]]></title>
    <url>%2F2017%2F08%2F05%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E2%80%94%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言在 Java 中，我们可以使用 new 关键字指定类名来生成类的实例。但是，有的时候，我们也会在不指定类名的前提下生成实例，例如像图形编辑器中拖动现有的模型工具制作图形的实例，这种是非常典型的生成实例的过程太过复杂，很难根据类来生成实例场景，因此需要根据现有的实例来生成新的实例。像这样根据实例来生成新的实例的模式，我们称之为 原型模式。在软件开发过程中，我们经常会遇到需要创建多个相同或者相似对象的情况，因此 原型模式 的使用频率还是很高的。 定义定义： 用原型实例指定创建对象的种类，并通过拷贝这些原型创建新的对象。类型： 类的创建模式 UML图在原型模式中涉及 Prototype、 ConcretePrototype、 Client 三个角色。代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * @author zhangguoji * 2017/8/4 21:46 */interface Prototype &#123; Prototype clone();&#125;class ConcretePrototype implements Prototype &#123; private String attr; // 成员属性 /** * 克隆方法 */ @Override public ConcretePrototype clone() &#123; // 创建新对象 ConcretePrototype prototype = new ConcretePrototype(); prototype.setAttr(this.attr); return prototype; &#125; @Override public String toString() &#123; return "ConcretePrototype[attr=" + attr + "]"; &#125; public void setAttr(String attr) &#123; this.attr = attr; &#125; public String getAttr() &#123; return this.attr; &#125;&#125;public class Client &#123; public static void main(String[] args) &#123; ConcretePrototype prototype = new ConcretePrototype(); prototype.setAttr("Kevin Zhang"); ConcretePrototype prototype2 = prototype.clone(); System.out.println(prototype.toString()); System.out.println(prototype2.toString()); &#125;&#125; 输出结果：1234ConcretePrototype[attr=Kevin Zhang]ConcretePrototype[attr=Kevin Zhang]Process finished with exit code 0 Java的原型模式Java 中使用原型模式很简单， 它为我们提供了复制实例的 clone() 方法。 实际上，所有的 Java 类都继承自 java.lang.Object。Object 类提供一个 clone() 方法，因此，在 Java 中可以直接使用 Object 提供的 clone() 方法来实现对象的克隆。 值得注意的是，被复制对象的类必须实现 java.lang.Cloneable 接口，如果没有实现 java.lang.Cloneable 接口的实例调用了 clone() 方法，会在运行时抛出CloneNotSupportedException 异常。 Cloneable 是一个标记接口，在 Cloneable 接口中并没有声明任何方法，它只是被用来标记可以使用 clone() 方法进行复制。12public interface Cloneable &#123;&#125; 案例分析现在，我有一个消息系统的需求，希望复制原先消息模板进行快速创建，然后可以进行修改后保存。 下面，我们通过 Java 的 clone()方法来实现对象的克隆。 Message 就是具体原型类，复制实现 clone() 方法。Message代码：12345678910111213141516171819202122232425262728293031323334353637383940414243/** * @author zhangguoji * @date 2017/8/4 21:56 */class Message implements Cloneable&#123; private String author; private String content; public String getAuthor() &#123; return author; &#125; public void setAuthor(String author) &#123; this.author = author; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125; @Override protected Object clone() &#123; Message message = null; try &#123; message = (Message) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return message; &#125; @Override public String toString() &#123; return "Message&#123;" + "author='" + author + '\'' + ", content='" + content + '\'' + '&#125;'; &#125;&#125; Client代码：123456789101112131415161718/** * @author zhangguoji * @date 2017/8/4 22:00 */public class Client &#123; public static void main(String[] args) &#123; Message message = new Message(); message.setAuthor("ZGJ"); message.setContent("2017-08-04【消息】"); Message message1 = (Message) message.clone(); message.setContent("2017-08-05【消息】"); System.out.println(message); System.out.println(message1); &#125;&#125; 结果：12Message&#123;author=&apos;ZGJ&apos;, content=&apos;2017-08-05【消息】&apos;&#125;Message&#123;author=&apos;ZGJ&apos;, content=&apos;2017-08-04【消息】&apos;&#125; 思考既然要创建新的实例，为什么不直接使用 new XXX()，而要设计出一个原型模式进行实例的复制呢？ 有的时候，我们也会在不指定类名的前提下生成实例，例如像图形编辑器中拖动现有的模型工具制作图形的实例，这种是非常典型的生成实例的过程太过复杂，很难根据类来生成实例场景，因此需要根据现有的实例来生成新的实例。Prototype原型模式是一种创建型设计模式，它主要面对的问题是：“某些结构复杂的对象”的创建工作；由于需求的变化，这些对象经常面临着剧烈的变化，但是他们却拥有比较稳定一致的接口。 还比如，类初始化需要消化非常多的资源，我们也可以考虑使用原型模式。因为，原型模式是在内存进行二进制流的拷贝，要比直接 new 一个对象性能好很多。 克隆的对象是全新的吗？原型模式通过 clone() 方法创建的对象是全新的对象,它在内存中拥有新的地址,通过==符号判断返回是flase。 深克隆和浅克隆clone() 方法使用的是浅克隆。浅克隆对于要克隆的对象, 会复制其基本数据类型和 String 类型或其他final类型的属性的值给新的对象. 而对于非基本数据类型的属性，例如数组、集合, 仅仅复制一份引用给新产生的对象, 即新产生的对象和原始对象中的非基本数据类型的属性都指向的是同一个对象。 此外，还存在深克隆。深克隆对于要克隆的对象, 对于非基本数据类型的属性，例如数组、集合支持复制。换句话说，在深克隆中，除了对象本身被复制外，对象所包含的所有成员变量也将复制。 浅克隆和深克隆的主要区别在于，是否支持引用类型的成员变量的复制。 在 Java 中，如果需要实现深克隆，可以通过序列化等方式来实现。我们在消息中增加一个附件类123456789101112131415161718192021222324252627import java.io.Serializable;/** * @author zhangguoji * @date 2017/8/4 22:10 */public class Attachment implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return "Attachment&#123;" + "name='" + name + '\'' + '&#125;'; &#125;&#125; 在消息中加入这个附件类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.io.*;/** * @author zhangguoji * 2017/8/4 21:56 */class Message implements Serializable &#123; private static final long serialVersionUID = 1L; private String author; private String content; private Attachment attachment; public String getAuthor() &#123; return author; &#125; public void setAuthor(String author) &#123; this.author = author; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125; public Attachment getAttachment() &#123; return attachment; &#125; public void setAttachment(Attachment attachment) &#123; this.attachment = attachment; &#125; public Message deepClone() throws IOException, ClassNotFoundException &#123; /** * 将对象序列化到对象数组流中 */ ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); /** * 从流中读取对象 */ ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); return (Message) ois.readObject(); &#125;&#125; 使用序列化实现深度克隆，然后再客户端中测试是否深度克隆成功1234567891011121314151617181920212223242526import java.io.IOException;/** * @author zhangguoji * @date 2017/8/4 22:30 */public class Client &#123; public static void main(String[] args) &#123; Message message = new Message(); message.setAuthor("ZGJ"); message.setContent("2017-08-04【消息】"); Attachment attachment = new Attachment(); attachment.setName("附件"); message.setAttachment(attachment); Message message1 = null; try &#123; message1 = message.deepClone(); &#125; catch (Exception e) &#123; e.printStackTrace(); System.out.println("克隆失败"); &#125; System.out.println("消息是否相同：" + (message == message1)); System.out.println("附件是否相同：" + (message.getAttachment() == message1.getAttachment())); &#125;&#125; 结果如下：12消息是否相同：false附件是否相同：false 注意一般而言，Java语言中的clone()方法满足： 对任何对象x，都有x.clone() != x，即克隆对象与原型对象不是同一个对象； 对任何对象x，都有x.clone().getClass() == x.getClass()，即克隆对象与原型对象的类型一样； 如果对象x的equals()方法定义恰当，那么x.clone().equals(x)应该成立。 为了获取对象的一份拷贝，我们可以直接利用Object类的clone()方法，具体步骤如下： 在派生类中覆盖基类的clone()方法，并声明为public； 在派生类的clone()方法中，调用super.clone()；3.派生类需实现Cloneable接口。此时，Object类相当于抽象原型类，所有实现了Cloneable接口的类相当于具体原型类。 总结原型模式的目的在于，根据实例来生成新的实例，我们可以很方便的快速的创建实例。 在 Java 中使用原型模式很简单， 被复制对象的类必须实现 java.lang.Cloneable 接口，并重写 clone() 方法。 使用原型模式的时候，尤其需要注意浅克隆和深克隆问题。在 Java 中，如果需要实现深克隆，可以通过序列化等方式来实现]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习总结 2017-08]]></title>
    <url>%2F2017%2F08%2F03%2F%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%932017-08%2F</url>
    <content type="text"><![CDATA[前言记录个人在2017年08月的学习和总结，不定期更新 2017-08-02有序的MapHashMap是无序的，有序的Map是TreeMap和LinkedHashMap,TreeMap里的元素排序要key实现Comparable接口或者传入一个Comparator比较器，LinkedHashMap里面元素的顺序和插入的顺序一致效率比较：TreeMap采用红黑树的数据结构实现，而LinkedHashMap采用链表的方式实现，查找效率上来说TreeMap会比LinkedHashMap高 TCP创建连接和断开连接的过程TCP特点 三次握手 四次挥手 可靠连接 丢包重传 核心：tcp是可以可靠传输协议，它的所有特点都为这个可靠传输服务。 创建连接时的三次握手来看一个java代码连接数据库的三次握手过程 第一步：client 发送 syn 到server 发起握手； 第二步：server 收到 syn后回复syn+ack给client； 第三步：client 收到syn+ack后，回复server一个ack表示收到了server的syn+ack（此时client的48287端口的连接已经是established）握手的核心目的是告知对方seq（绿框是client的初始seq，蓝色框是server 的初始seq），对方回复ack（收到的seq+包的大小），这样发送端就知道有没有丢包了。 握手的次要目的是告知和协商一些信息，图中黄框。 MSS–最大传输包SACK_PERM–是否支持Selective ack(用户优化重传效率）WS–窗口计算指数（有点复杂的话先不用管）这就是tcp为什么要握手建立连接，就是为了解决tcp的可靠传输。 断开连接时的四次挥手再来看java连上mysql后，执行了一个SQL： select sleep(2); 然后就断开了连接四个红框表示断开连接的四次挥手：第一步： client主动发送fin包给server第二步： server回复ack（对应第一步fin包的ack）给client，表示server知道client要断开了第三步： server发送fin包给client，表示server也可以断开了第四部： client回复ack给server，表示既然双发都发送fin包表示断开，那么就真的断开吧 为什么是握手是三次而挥手是四次这是因为当client发送fin包给服务器的时候，server可能还需要有一些后续工作要做，比如OS通知应用层要关闭，这里应用层可能需要做些准备工作，或者说server还有一些数据需要发送给client，等准备工作做完或者是数据发送完毕，就可以发送fin包了 2017-08-03JVM 参数初探堆参数-XX:+PrintGC 使用这个参数，虚拟机启动后，只要遇到GC就会打印日志-XX:+UseSerialGC 配置串行回收器-XX:+PrintGCDetails 可以查看详细信息，包括各个区的情况-Xms 设置最小堆-Xmx 设置最大堆-Xmx20m -Xms5m -XX:+PrintCommandLineFlags 可以将隐式或者显示传递给虚拟机的参数输出，打印虚拟机配置 ★技巧：JVM初始分配的内存由-Xms指定，默认是物理内存的1/64；JVM最 大分配的内存由-Xmx指定，默认是物理内存的1/4。默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70% 时，JVM会减少堆直到-Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小。 新生代配置-Xmn 可以设置新生代的大小，设置一个比较大的新生代会减少老年代的大小，这个参数对系统性能以及GC行为有很大的影响，新生代GC频繁，老年代相对少，新生代大小一般会设置整合堆空间的1/3到1/4左右。-XX:SurvivorRatio 用来设置新生代中eden区和from/to区的比例，默认8:1:1含义：-XX:SurvivorRatio=eden/from=eden/to★技巧：不同的堆分布情况，对系统执行会产生一定影响，在实际情况下，应该根据系统的特点做出合理的配置，基本策略：尽可能将对象预留在新生代，减少老年代的GC次数。除了可以设置新生代的绝对大小-Xmn，还可以使用-XX:NewRatio来设置新生代和老年代的比例：-XX:NewRatio=新生代/老年代 堆溢出处理堆溢出处理在Java程序的运行过程中，如果堆空间不足，则会抛出内存溢出的错误Out of Memory（OOM），一旦这类问题发生在生产环境，就可能引起严重的业务中断，java虚拟机提供了-XX:HeapDumpOnOutOfMemoryError，使用该参数可以在内存溢出时导出整个堆信息，与之配合使用的还有参数。-XX:HeapDumpPath，可以设置导出堆的存放路径内存分析工具：Memory Analyzer dump信息：-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=d:/Test03.dump 栈参数-Xss 指定线程最大栈空间，整合参数也直接决定了函数可调用的最大深度 方法区JDK8以前，方法区和java堆一样，是一块所有线程共享的内存区域，它用于保存系统的类信息，方法区（永久区）可以保存多少信息可以对其进行配置，在默认情况下，-XX:MaxPermSize为64M，如果系统运行时产生大量的类，就需要设置一个相对合适的方法区，以免出现永久区0内存溢出的问题。-XX:MaxPermSize=64M -XX:PermSize=64Mjdk1.7以后不分client或者server模式 JDK8以后，永久区被移除，使用本地内存来存储类元数据信息并称之为：元空间（Metaspace） 2017-08-04Spring IOC和AOP特点 降低了组件之间的耦合性 ，实现了软件各层之间的解耦 可以使用容易提供的众多服务，如事务管理，消息服务等 容器提供单例模式支持 容器提供了AOP技术，利用它很容易实现如权限拦截，运行期监控等功能 容器提供了众多的辅助类，能加快应用的开发 spring对于主流的应用框架提供了集成支持，如hibernate，JPA，struts等 spring属于低侵入式设计，代码的污染极低 独立于各种应用服务器 spring的DI机制降低了业务对象替换的复杂性 spring的高度开放性，并不强制应用完全依赖于Spring，开发者可以自由选择spring的部分或全部 根本目的根本目的：简化Java开发。为了降低Java开发的复杂性，Spring采取以下4种关键策略： 基于POJO的轻量级和最小侵入性编程 通过依赖注入和面向接口实现松耦合 基于切面和惯例进行声明式编程 通过切面和模版减少样板示代码 IOC概念Control，即“控制反转”，不是什么技术，而是一种设计思想。在Java开发中，Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。如何理解好Ioc呢？理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下： 谁控制谁，控制什么：传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对 象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。 为何是反转，哪些方面反转了：有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 IoC能做什么IoC 不是一种技术，只是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是 松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。 其实IoC对编程带来的最大改变不是从代码上，而是从思想上，发生了“主从换位”的变化。应用程序原本是老大，要获取什么资源都是主动出击，但是在IoC/DI思想中，应用程序就变成被动的了，被动的等待IoC容器来创建并注入它所需要的资源了。 IoC很好的体现了面向对象设计法则之一—— 好莱坞法则：“别找我们，我们找你”；即由IoC容器帮对象找相应的依赖对象并注入，而不是由对象主动去找。 IoC和DIDI—Dependency Injection，即“依赖注入”：组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。 理解DI的关键是：“谁依赖谁，为什么需要依赖，谁注入谁，注入了什么”，那我们来深入分析一下： 谁依赖于谁：当然是应用程序依赖于IoC容器； 为什么需要依赖：应用程序需要IoC容器来提供对象需要的外部资源； 谁注入谁：很明显是IoC容器注入应用程序某个对象，应用程序依赖的对象； 注入了什么：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。 IoC和DI由什么关系呢？其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以2004年大师级人物Martin Fowler又给出了一个新的名字：“依赖注入”，相对IoC 而言，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。 AOP什么是AOPAOP（Aspect-OrientedProgramming，面向方面编程），可以说是OOP（Object-Oriented Programing，面向对象编程）的补充和完善。OOP引入封装、继承和多态性等概念来建立一种对象层次结构，用以模拟公共行为的一个集合。当我们需要为分散的对象引入公共行为的时候，OOP则显得无能为力。也就是说，OOP允许你定义从上到下的关系，但并不适合定义从左到右的关系。例如日志功能。日志代码往往水平地散布在所有对象层次中，而与它所散布到的对象的核心功能毫无关系。对于其他类型的代码，如安全性、异常处理和透明的持续性也是如此。这种散布在各处的无关的代码被称为横切（cross-cutting）代码，在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。 而AOP技术则恰恰相反，它利用一种称为“横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其名为“Aspect”，即方面。所谓“方面”，简单地说，就是将那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可操作性和可维护性。AOP代表的是一个横向的关系，如果说“对象”是一个空心的圆柱体，其中封装的是对象的属性和行为；那么面向方面编程的方法，就仿佛一把利刃，将这些空心圆柱体剖开，以获得其内部的消息。而剖开的切面，也就是所谓的“方面”了。然后它又以巧夺天功的妙手将这些剖开的切面复原，不留痕迹。 使用“横切”技术，AOP把软件系统分为两个部分：核心关注点和横切关注点。业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。横切关注点的一个特点是，他们经常发生在核心关注点的多处，而各处都基本相似。比如权限认证、日志、事务处理。Aop 的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来。正如Avanade公司的高级方案构架师Adam Magee所说，AOP的核心思想就是“将应用程序中的商业逻辑同对其提供支持的通用服务进行分离。” 实现AOP的技术，主要分为两大类：一是采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行；二是采用静态织入的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码。 使用场景AOP用来封装横切关注点，具体可以在下面的场景中使用: Authentication 权限 Caching 缓存 Context passing 内容传递 Error handling 错误处理 Lazy loading 懒加载 Debugging 调试 logging, tracing, profiling and monitoring 记录跟踪 优化 校准 Performance optimization 性能优化 Persistence 持久化 Resource pooling 资源池 Synchronization 同步 Transactions 事务 概念首先让我们从一些重要的AOP概念和术语开始。这些术语不是Spring特有的。不过AOP术语并不是特别的直观，如果Spring使用自己的术语，将会变得更加令人困惑。 切面（Aspect）：一个关注点的模块化，这个关注点可能会横切多个对象。事务管理是J2EE应用中一个关于横切关注点的很好的例子。在Spring AOP中，切面可以使用基于模式）或者基于@Aspect注解的方式来实现。 连接点（Joinpoint）：在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候。在Spring AOP中，一个连接点总是表示一个方法的执行。 通知（Advice）：在切面的某个特定的连接点上执行的动作。其中包括了“around”、“before”和“after”等不同类型的通知（通知的类型将在后面部分进行讨论）。许多AOP框架（包括Spring）都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链。 切入点（Pointcut）：匹配连接点的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时）。切入点表达式如何和连接点匹配是AOP的核心：Spring缺省使用AspectJ切入点语法。 引入（Introduction）：用来给一个类型声明额外的方法或属性（也被称为连接类型声明（inter-type declaration））。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用引入来使一个bean实现IsModified接口，以便简化缓存机制。 目标对象（Target Object）： 被一个或者多个切面所通知的对象。也被称做被通知（advised）对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个被代理（proxied）对象。 AOP代理（AOP Proxy）：AOP框架创建的对象，用来实现切面契约（例如通知方法执行等等）。在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 织入（Weaving）：把切面连接到其它的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。 通知类型： 前置通知（Before advice）：在某连接点之前执行的通知，但这个通知不能阻止连接点之前的执行流程（除非它抛出一个异常）。 后置通知（After returning advice）：在某连接点正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。 异常通知（After throwing advice）：在方法抛出异常退出时执行的通知。 最终通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 环绕通知（Around Advice）：包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。 环绕通知是最常用的通知类型。和AspectJ一样，Spring提供所有类型的通知，我们推荐你使用尽可能简单的通知类型来实现需要的功能。例如，如果你只是需要一个方法的返回值来更新缓存，最好使用后置通知而不是环绕通知，尽管环绕通知也能完成同样的事情。用最合适的通知类型可以使得编程模型变得简单，并且能够避免很多潜在的错误。比如，你不需要在JoinPoint上调用用于环绕通知的proceed()方法，就不会有调用的问题。 2017-08-06线程和进程什么是线程？线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，你可以使用多线程对运算密集型任务提速。比如，如果一个线程完成一个任务要100毫秒，那么用十个线程完成改任务只需10毫秒。Java在语言层面对多线程提供了卓越的支持，它也是一个很好的卖点。 线程和进程有什么区别？线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据。 2017-08-07Java栈上分配对象我们在学习使用Java的过程中，一般认为new出来的对象都是被分配在堆上，但是这个结论不是那么的绝对，通过对Java对象分配的过程分析，可以知道有两个地方会导致Java中new出来的对象并不一定分别在所认为的堆上。这两个点分别是Java中的逃逸分析和TLAB（Thread Local Allocation Buffer） 逃逸分析定义逃逸分析，是一种可以有效减少Java 程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。通过逃逸分析，Java Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。在计算机语言编译器优化原理中，逃逸分析是指分析指针动态范围的方法，它同编译器优化原理的指针分析和外形分析相关联。当变量（或者对象）在方法中分配后，其指针有可能被返回或者被全局引用，这样就会被其他过程或者线程所引用，这种现象称作指针（或者引用）的逃逸(Escape)。Java在java SE 6u23以及以后的版本中支持并默认开启了逃逸分析的选项。Java的 HotSpot JIT编译器，能够在方法重载或者动态加载代码的时候对代码进行逃逸分析，同时Java对象在堆上分配和内置线程的特点使得逃逸分析成Java的重要功能。 逃逸分析的方法Java Hotspot编译器使用的是1Choi J D, Gupta M, Serrano M, et al. Escape analysis for Java[J]. Acm Sigplan Notices, 1999, 34(10): 1-19. Jong-Deok Choi, Manish Gupta, Mauricio Seffano,Vugranam C. Sreedhar, Sam Midkiff等在论文《Escape Analysis for Java》中描述的算法进行逃逸分析的。该算法引入了连通图，用连通图来构建对象和对象引用之间的可达性关系，并在次基础上，提出一种组合数据流分析法。由于算法是上下文相关和流敏感的，并且模拟了对象任意层次的嵌套关系，所以分析精度较高，只是运行时间和内存消耗相对较大。绝大多数逃逸分析的实现都基于一个所谓“封闭世界(closed world)”的前提：所有可能被执行的，方法在做逃逸分析前都已经得知，并且，程序的实际运行不会改变它们之间的调用关系 。但当真实的 Java 程序运行时，这样的假设并不成立。Java 程序拥有的许多特性，例如动态类加载、调用本地函数以及反射程序调用等等，都将打破所谓“封闭世界”的约定。 逃逸分析后的处理经过逃逸分析之后，可以得到三种对象的逃逸状态。 GlobalEscape（全局逃逸）， 即一个对象的引用逃出了方法或者线程。例如，一个对象的引用是复制给了一个类变量，或者存储在在一个已经逃逸的对象当中，或者这个对象的引用作为方法的返回值返回给了调用方法。 ArgEscape（参数级逃逸），即在方法调用过程当中传递对象的引用给一个方法。这种状态可以通过分析被调方法的二进制代码确定。 NoEscape（没有逃逸），一个可以进行标量替换的对象。可以不将这种对象分配在传统的堆上。编译器可以使用逃逸分析的结果，对程序进行一下优化。 堆分配对象变成栈分配对象。一个方法当中的对象，对象的引用没有发生逃逸，那么这个方法可能会被分配在栈内存上而非常见的堆内存上。 消除同步。线程同步的代价是相当高的，同步的后果是降低并发性和性能。逃逸分析可以判断出某个对象是否始终只被一个线程访问，如果只被一个线程访问，那么对该对象的同步操作就可以转化成没有同步保护的操作，这样就能大大提高并发程度和性能。 矢量替代。逃逸分析方法如果发现对象的内存存储结构不需要连续进行的话，就可以将对象的部分甚至全部都保存在CPU寄存器内，这样能大大提高访问速度。 下面，我们看一下逃逸分析的例子。12345678910111213141516171819class Main &#123; public static void main(String[] args) &#123; example(); &#125; public static void example() &#123; Foo foo = new Foo(); //alloc Bar bar = new Bar(); //alloc bar.setFoo(foo); &#125; &#125; class Foo &#123;&#125; class Bar &#123; private Foo foo; public void setFoo(Foo foo) &#123; this.foo = foo; &#125; &#125; 在这个例子当中，我们创建了两个对象，Foo对象和Bar对象，同时我们把Foo对象的应用赋值给了Bar对象的方法。此时，如果Bar对在堆上就会引起Foo对象的逃逸，但是，在本例当中，编译器通过逃逸分析，可以知道Bar对象没有逃出example()方法，因此这也意味着Foo也没有逃出example方法。因此，编译器可以将这两个对象分配到栈上。 编译器经过逃逸分析的效果测试代码123456789101112131415161718192021 class EscapeAnalysis &#123; private static class Foo &#123; private int x; private static int counter; public Foo() &#123; x = (++counter); &#125; &#125; public static void main(String[] args) &#123; long start = System.nanoTime(); for (int i = 0; i &lt; 1000 * 1000 * 10; ++i) &#123; Foo foo = new Foo(); &#125; long end = System.nanoTime(); System.out.println("Time cost is " + (end - start)); &#125; &#125; 未开启逃逸分析设置为：1-server -verbose:gc 开启逃逸分析设置为：1-server -verbose:gc -XX:+DoEscapeAnalysis 在未开启逃逸分析的状况下运行情况如下：12345678910[GC 5376K-&gt;427K(63872K), 0.0006051 secs] [GC 5803K-&gt;427K(63872K), 0.0003928 secs] [GC 5803K-&gt;427K(63872K), 0.0003639 secs] [GC 5803K-&gt;427K(69248K), 0.0003770 secs] [GC 11179K-&gt;427K(69248K), 0.0003987 secs] [GC 11179K-&gt;427K(79552K), 0.0003817 secs] [GC 21931K-&gt;399K(79552K), 0.0004342 secs] [GC 21903K-&gt;399K(101120K), 0.0002175 secs] [GC 43343K-&gt;399K(101184K), 0.0001421 secs] Time cost is 58514571 开启逃逸分析的状况下，运行情况如下：1Time cost is 10031306 未开启逃逸分析时，运行上诉代码，JVM执行了GC操作，而在开启逃逸分析情况下，JVM并没有执行GC操作。同时，操作时间上，开启逃逸分析的程序运行时间是未开启逃逸分析时间的1/5。 TLABJVM在内存新生代Eden Space中开辟了一小块线程私有的区域，称作TLAB（Thread-local allocation buffer）。默认设定为占用Eden Space的1%。在Java程序中很多对象都是小对象且用过即丢，它们不存在线程共享也适合被快速GC，所以对于小对象通常JVM会优先分配在TLAB上，并且TLAB上的分配由于是线程私有所以没有锁开销。因此在实践中分配多个小对象的效率通常比分配一个大对象的效率要高。也就是说，Java中每个线程都会有自己的缓冲区称作TLAB（Thread-local allocation buffer），每个TLAB都只有一个线程可以操作，TLAB结合bump-the-pointer技术可以实现快速的对象分配，而不需要任何的锁进行同步，也就是说，在对象分配的时候不用锁住整个堆，而只需要在自己的缓冲区分配即可。关于对象分配的JDK源码可以参见JVM 之 Java对象创建[初始化]中对OpenJDK源码的分析。 Java对象分配的过程编译器通过逃逸分析，确定对象是在栈上分配还是在堆上分配。如果是在堆上分配，则进入选项2.如果tlab_top + size &lt;= tlab_end，则在在TLAB上直接分配对象并增加tlab_top 的值，如果现有的TLAB不足以存放当前对象则3.重新申请一个TLAB，并再次尝试存放当前对象。如果放不下，则4.在Eden区加锁（这个区是多线程共享的），如果eden_top + size &lt;= eden_end则将对象存放在Eden区，增加eden_top 的值，如果Eden区不足以存放，则5.执行一次Young GC（minor collection）。经过Young GC之后，如果Eden区任然不足以存放当前对象，则直接分配到老年代。对象不在堆上分配主要的原因还是堆是共享的，在堆上分配有锁的开销。无论是TLAB还是栈都是线程私有的，私有即避免了竞争（当然也可能产生额外的问题例如可见性问题），这是典型的用空间换效率的做法。 2017-08-08UnSafe类JDK源码中，在研究AQS框架时，会发现很多地方都使用了CAS操作，在并发实现中CAS操作必须具备原子性，而且是硬件级别的原子性，Java被隔离在硬件之上，明显力不从心，这时为了能直接操作操作系统层面，肯定要通过用C++编写的native本地方法来扩展实现。JDK提供了一个类来满足CAS的要求，sun.misc.Unsafe，从名字上可以大概知道它用于执行低级别、不安全的操作，AQS就是使用此类完成硬件级别的原子操作。 Unsafe是一个很强大的类，它可以分配内存、释放内存、可以定位对象某字段的位置、可以修改对象的字段值、可以使线程挂起、使线程恢复、可进行硬件级别原子的CAS操作等等，但平时我们没有这么特殊的需求去使用它，而且必须在受信任代码（一般由JVM指定）中调用此类，例如直接Unsafe unsafe = Unsafe.getUnsafe();获取一个Unsafe实例是不会成功的，因为这个类的安全性很重要，设计者对其进行了如下判断，它会检测调用它的类是否由启动类加载器Bootstrap ClassLoader（它的类加载器为null）加载，由此保证此类只能由JVM指定的类使用。 123456public static Unsafe getUnsafe() &#123; Class cc = sun.reflect.Reflection.getCallerClass(2); if (cc.getClassLoader() != null) throw new SecurityException(&quot;Unsafe&quot;); return theUnsafe; &#125; 当然可以通过反射绕过上面的限制，用下面的getUnsafeInstance方法可以获取Unsafe实例，这段代码演示了如何获取java对象的相对地址偏移量及使用Unsafe完成CAS操作，最终输出的是flag字段的内存偏移量及CAS操作后的值。分别为12和101。另外如果使用开发工具如Eclipse，可能会编译通不过，只要把编译错误提示关掉即可。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.example.demo;import sun.misc.Unsafe;import java.lang.reflect.Field;/** * @author zhangguoji * @date 2017/8/8 11:46 */public class UnsafeTest &#123; private int flag = 100; private static long offset; private static Unsafe unsafe = null; static &#123; try &#123; unsafe = getUnsafeInstance(); offset = unsafe.objectFieldOffset(UnsafeTest.class .getDeclaredField("flag")); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws Exception &#123; int expect = 100; int update = 101; UnsafeTest unsafeTest = new UnsafeTest(); System.out.println("unsafeTest对象的flag字段的地址偏移量为：" + offset); unsafeTest.doSwap(offset, expect, update); System.out.println("CAS操作后的flag值为：" + unsafeTest.getFlag()); &#125; private boolean doSwap(long offset, int expect, int update) &#123; return unsafe.compareAndSwapInt(this, offset, expect, update); &#125; public int getFlag() &#123; return flag; &#125; private static Unsafe getUnsafeInstance() throws SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException &#123; Field theUnsafeInstance = Unsafe.class.getDeclaredField("theUnsafe"); theUnsafeInstance.setAccessible(true); return (Unsafe) theUnsafeInstance.get(Unsafe.class); &#125;&#125; 结果12unsafeTest对象的flag字段的地址偏移量为：12CAS操作后的flag值为：101 Unsafe类让我们明白了java是如何实现对操作系统操作的，一般我们使用java是不需要在内存中处理java对象及内存地址位置的，但有的时候我们确实需要知道java对象相关的地址，于是我们使用Unsafe类，尽管java对其提供了足够的安全管理。 Java语言的设计者们极力隐藏涉及底层操作系统的相关操作，但此节我们本着对AQS框架实现的目的，不得不剖析了Unsafe类，因为AQS里面即是使用Unsafe获取对象字段的地址偏移量、相关原子操作来实现CAS操作的。 MySQL value和vlues的区别两者功能一样，可以混合使用，但是value插入多条数据时性能较好，values插入单条数据时性能较好，跟我们的逻辑相反的，很奇怪单是SQL Sever只能使用values ConcurrentHashMap跨版本问题背景知识javacjavac有两个指令：-source和-target，比如下述指令：1/usr/lib/java/jdk1.8.0_131/bin/javac -source 1.7 -target 1.7 HelloWorld.java -source：表示我的代码将采用哪个java版本来编译。该值影响的是编译器对语法规则的校验。比如HelloWorld.java中含有jdk8的语法，但是你的-source为1.7，那么编译器就会报错。 -target：表示生成的字节码将会在哪个版本（及以上）的jvm上运行。比如HelloWorld.java指定了-target为1.8，那么HelloWorld.class只能在1.8即以上的jvm中运行，如果在1.7的jvm上运行，就会报错。 rt.jarjdk的rt.jar里面包含了jdk的核心类，比如String，集合等。JVM在加载类时，对于rt.jar包里面的所有的类持有最高的信任而不做任何校验。 ConcurrentHashMapConcurrentHashMap类有一个方法叫做keySet，用来返回当前map中的key集合。虽然返回的是key的集合，但是在1.7和1.8中用来表示该集合的类却完全不同。在1.7中，返回的是Set：1234public Set&lt;K&gt; More ...keySet() &#123; Set&lt;K&gt; ks = keySet; return (ks != null) ? ks : (keySet = new KeySet());&#125; 然而在1.8中返回的是KeySetView：1234public KeySetView&lt;K,V&gt; keySet() &#123; KeySetView&lt;K,V&gt; ks; return (ks = keySet) != null ? ks : (keySet = new KeySetView&lt;K,V&gt;(this, null));&#125; 其中KeySetView其实是Set接口的一个实现类。我们再来看下述代码：12345678import java.util.Set;import java.util.concurrent.ConcurrentHashMap;public class HelloWorld &#123; public static void main(String[] args) &#123; ConcurrentHashMap&lt;String, String&gt; test = new ConcurrentHashMap&lt;&gt;(); Set&lt;String&gt; keySet = test.keySet(); &#125;&#125; 然后我们用jdk8的javac来进行编译：123$ /usr/lib/java8/bin/javac -source 1.7 -target 1.7 HelloWorld.javawarning: [options] bootstrap class path not set in conjunction with -source 1.71 warning 或者中文版的报错信息如下：12警告: [options] 未与 -source 1.7 一起设置引导类路径1 个警告 但是上述代码是可以通过编译的，因为KeySetView是Set的实现类，所以1.7的语法没有任何问题。但是编译生成的class文件无法在1.7版本的jvm上运行。我们看一下字节码的实际内容：12345678910import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.ConcurrentHashMap.KeySetView;public class HelloWorld&#123; public static void main(String[] paramArrayOfString) &#123; ConcurrentHashMap localConcurrentHashMap = new ConcurrentHashMap(); ConcurrentHashMap.KeySetView localKeySetView = localConcurrentHashMap.keySet(); &#125;&#125; 我们可以看到，在字节码中，实际上keySet返回的是1.8中指定的KeySetView类，但是这个类在jdk1.7中是不存在的，所以当用1.7的jvm运行时，会抛出NoSuchMethodError的异常。 解决方法为了解决这个问题，还是要看编译时的警告信息（不能忽视任何一个警告）。从warning的信息中我们可以得知，当指定了-source时，我们还需要一起指定引导类即bootstrap类，否则可能会出现某些兼容性的问题，比如刚才我们遇到的ConcurrentHashMap的问题。所以我们在编译的时候需要再加上引导类：1$ /usr/lib/java8/bin/javac -source 1.7 -target 1.7 HelloWorld.java -bootclasspath /usr/lib/java7/jre/lib/rt.jar 我们先来反编译生成的class文件12345678910import java.util.Set;import java.util.concurrent.ConcurrentHashMap;public class HelloWorld&#123; public static void main(String[] paramArrayOfString) &#123; ConcurrentHashMap localConcurrentHashMap = new ConcurrentHashMap(); Set localSet = localConcurrentHashMap.keySet(); &#125;&#125; 我们可以看到现在class文件中返回的类变为了Set，然后我们在用1.7的jvm来运行，发现一切正常，问题被解决了！ 总结以后在指定-source时，还需要同时指定-bootclasspath，否则就会默认使用当前javac所用到的jdk版本的核心jar包（比如rt.jar）。 2017-08-11Iterator和EnumerationEnumeration是一个接口，它的源码如下： 代码12345678package java.util;public interface Enumeration&lt;E&gt; &#123; boolean hasMoreElements(); E nextElement();&#125; Iterator也是一个接口，它的源码如下： 123456789package java.util;public interface Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); void remove();&#125; 区别 函数接口不同Enumeration只有2个函数接口。通过Enumeration，我们只能读取集合的数据，而不能对数据进行修改。Iterator只有3个函数接口。Iterator除了能读取集合的数据之外，也能数据进行删除操作。 Iterator支持fail-fast机制，而Enumeration不支持。Enumeration 是JDK 1.0添加的接口。使用到它的函数包括Vector、Hashtable等类，这些类都是JDK 1.0中加入的，Enumeration存在的目的就是为它们提供遍历接口。Enumeration本身并没有支持同步，而在Vector、Hashtable实现Enumeration时，添加了同步。而Iterator 是JDK 1.2才添加的接口，它也是为了HashMap、ArrayList等集合提供遍历接口。Iterator是支持fail-fast机制的：当多个线程对同一个集合的内容进行操作时，就可能会产生fail-fast事件。 遍历速度比较下面，我们编写一个Hashtable，然后分别通过 Iterator 和 Enumeration 去遍历它，比较它们的效率。代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.Enumeration;import java.util.Hashtable;import java.util.Iterator;import java.util.Random;public class IteratorEnumeration &#123; public static void main(String[] args) &#123; int val; Random r = new Random(); Hashtable table = new Hashtable(); for (int i=0; i&lt;10000000; i++) &#123; // 随机获取一个[0,100)之间的数字 val = r.nextInt(100); table.put(String.valueOf(i), val); &#125; // 通过Iterator遍历Hashtable iterateHashtable(table) ; // 通过Enumeration遍历Hashtable enumHashtable(table); &#125; /* * 通过Iterator遍历Hashtable */ private static void iterateHashtable(Hashtable table) &#123; long startTime = System.currentTimeMillis(); Iterator iter = table.entrySet().iterator(); while(iter.hasNext()) &#123; //System.out.println("iter:"+iter.next()); iter.next(); &#125; long endTime = System.currentTimeMillis(); countTime(startTime, endTime); &#125; /* * 通过Enumeration遍历Hashtable */ private static void enumHashtable(Hashtable table) &#123; long startTime = System.currentTimeMillis(); Enumeration enu = table.elements(); while(enu.hasMoreElements()) &#123; //System.out.println("enu:"+enu.nextElement()); enu.nextElement(); &#125; long endTime = System.currentTimeMillis(); countTime(startTime, endTime); &#125; private static void countTime(long start, long end) &#123; System.out.println("time: "+(end-start)+"ms"); &#125;&#125; 运行结果如下：12time: 9mstime: 5ms 从中，我们可以看出。Enumeration 比 Iterator 的遍历速度更快。为什么呢？这是因为，Hashtable中Iterator是通过Enumeration去实现的，而且Iterator添加了对fail-fast机制的支持；所以，执行的操作自然要多一些。 2017-08-12HashMap原理概述本文将从几个常用方法下手，来阅读HashMap的源码。按照从构造方法-&gt;常用API（增、删、改、查）的顺序来阅读源码，并会讲解阅读方法中涉及的一些变量的意义。了解HashMap的特点、适用场景。 概括的说，HashMap 是一个关联数组、哈希表，它是线程不安全的，允许key为null,value为null。遍历时无序。其底层数据结构是数组称之为哈希桶，每个桶里面放的是链表，链表中的每个节点，就是哈希表中的每个元素。在JDK8中，当链表长度达到8，会转化成红黑树，以提升它的查询、插入效率，它实现了Map&lt;K,V&gt;, Cloneable, Serializable接口。 因其底层哈希桶的数据结构是数组，所以也会涉及到扩容的问题。 当HashMap的容量达到threshold域值时，就会触发扩容。扩容前后，哈希桶的长度一定会是2的次方。这样在根据key的hash值寻找对应的哈希桶时，可以用位运算替代取余操作，更加高效。 而key的hash值，并不仅仅只是key对象的hashCode()方法的返回值，还会经过扰动函数的扰动，以使hash值更加均衡。因为hashCode()是int类型，取值范围是40多亿，只要哈希函数映射的比较均匀松散，碰撞几率是很小的。但就算原本的hashCode()取得很好，每个key的hashCode()不同，但是由于HashMap的哈希桶的长度远比hash取值范围小，默认是16，所以当对hash值以桶的长度取余，以找到存放该key的桶的下标时，由于取余是通过与操作完成的，会忽略hash值的高位。因此只有hashCode()的低位参加运算，发生不同的hash值，但是得到的index相同的情况的几率会大大增加，这种情况称之为hash碰撞。 即，碰撞率会增大。 扰动函数就是为了解决hash碰撞的。它会综合hash值高位和低位的特征，并存放在低位，因此在与运算时，相当于高低位一起参与了运算，以减少hash碰撞的概率。（在JDK8之前，扰动函数会扰动四次，JDK8简化了这个操作） 扩容操作时，会new一个新的Node数组作为哈希桶，然后将原哈希表中的所有数据(Node节点)移动到新的哈希桶中，相当于对原哈希表中所有的数据重新做了一个put操作。所以性能消耗很大，可想而知，在哈希表的容量越大时，性能消耗越明显。 扩容时，如果发生过哈希碰撞，节点数小于8个。则要根据链表上每个节点的哈希值，依次放入新哈希桶对应下标位置。因为扩容是容量翻倍，所以原链表上的每个节点，现在可能存放在原来的下标，即low位， 或者扩容后的下标，即high位。 high位= low位+原哈希桶容量如果追加节点后，链表数量》=8，则转化为红黑树 由迭代器的实现可以看出，遍历HashMap时，顺序是按照哈希桶从低到高，链表从前往后，依次遍历的。属于无序集合。 HashMap的源码中，充斥个各种位运算代替常规运算的地方，以提升效率： 与运算替代模运算。用 hash &amp; (table.length-1) 替代 hash % (table.length) 用if ((e.hash &amp; oldCap) == 0)判断扩容后，节点e处于低区还是高区。 链表节点Node12345678910111213141516171819202122232425262728293031323334353637383940static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;//哈希值 final K key;//key V value;//value Node&lt;K,V&gt; next;//链表后置节点 Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; //每一个节点的hash值，是将key的hashCode 和 value的hashCode 异或运算得到的。 public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; //设置新的value 同时返回旧value public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; 由此可知，这是一个单链表每一个节点的hash值，是将key的hashCode和value的hashCode异或得到的。 构造函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788//最大容量 2的30次方static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//默认的加载因子static final float DEFAULT_LOAD_FACTOR = 0.75f;//哈希桶，存放链表。 长度是2的N次方，或者初始化时为0.transient Node&lt;K,V&gt;[] table;//加载因子，用于计算哈希表元素数量的阈值。 threshold = 哈希桶.length * loadFactor;final float loadFactor;//哈希表内元素数量的阈值，当哈希表内元素数量超过阈值时，会发生扩容resize()。int threshold;public HashMap() &#123; //默认构造函数，赋值加载因子为默认的0.75f this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;public HashMap(int initialCapacity) &#123; //指定初始化容量的构造函数 this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;//同时指定初始化容量 以及 加载因子， 用的很少，一般不会修改loadFactorpublic HashMap(int initialCapacity, float loadFactor) &#123; //边界处理 if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); //初始容量最大不能超过2的30次方 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //显然加载因子不能为负数 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; //设置阈值为 》=初始化容量的 2的n次方的值 this.threshold = tableSizeFor(initialCapacity);&#125;//新建一个哈希表，同时将另一个map m 里的所有元素加入表中public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125;//根据期望容量cap，返回2的n次方形式的 哈希桶的实际容量 length。 返回值一般会&gt;=cap static final int tableSizeFor(int cap) &#123;//经过下面的 或 和位移 运算， n最终各位都是1。 int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; //判断n是否越界，返回 2的n次方作为 table（哈希桶）的阈值 return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; //将另一个Map的所有元素加入表中，参数evict初始化时为false，其他情况为truefinal void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; //拿到m的元素数量 int s = m.size(); //如果数量大于0 if (s &gt; 0) &#123; //如果当前表是空的 if (table == null) &#123; // pre-size //根据m的元素数量和当前表的加载因子，计算出阈值 float ft = ((float)s / loadFactor) + 1.0F; //修正阈值的边界 不能超过MAXIMUM_CAPACITY int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); //如果新的阈值大于当前阈值 if (t &gt; threshold) //返回一个 》=新的阈值的 满足2的n次方的阈值 threshold = tableSizeFor(t); &#125; //如果当前元素表不是空的，但是 m的元素数量大于阈值，说明一定要扩容。 else if (s &gt; threshold) resize(); //遍历 m 依次将元素加入当前表中。 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; 先看一下扩容函数： 这是一个重点！重点！重点！ 初始化或加倍哈希桶大小。如果是当前哈希桶是null,分配符合当前阈值的初始容量目标。否则，因为我们扩容成以前的两倍。在扩容时，要注意区分以前在哈希桶相同index的节点，现在是在以前的index里，还是index+oldlength 里 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104final Node&lt;K,V&gt;[] resize() &#123; //oldTab 为当前表的哈希桶 Node&lt;K,V&gt;[] oldTab = table; //当前哈希桶的容量 length int oldCap = (oldTab == null) ? 0 : oldTab.length; //当前的阈值 int oldThr = threshold; //初始化新的容量和阈值为0 int newCap, newThr = 0; //如果当前容量大于0 if (oldCap &gt; 0) &#123; //如果当前容量已经到达上限 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; //则设置阈值是2的31次方-1 threshold = Integer.MAX_VALUE; //同时返回当前的哈希桶，不再扩容 return oldTab; &#125;//否则新的容量为旧的容量的两倍。 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY)//如果旧的容量大于等于默认初始容量16 //那么新的阈值也等于旧的阈值的两倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125;//如果当前表是空的，但是有阈值。代表是初始化时指定了容量、阈值的情况 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr;//那么新表的容量就等于旧的阈值 else &#123;&#125;//如果当前表是空的，而且也没有阈值。代表是初始化时没有任何容量/阈值参数的情况 // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;//此时新表的容量为默认的容量 16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//新的阈值为默认容量16 * 默认加载因子0.75f = 12 &#125; if (newThr == 0) &#123;//如果新的阈值是0，对应的是 当前表是空的，但是有阈值的情况 float ft = (float)newCap * loadFactor;//根据新表容量 和 加载因子 求出新的阈值 //进行越界修复 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //更新阈值 threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) //根据新的容量 构建新的哈希桶 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; //更新哈希桶引用 table = newTab; //如果以前的哈希桶中有元素 //下面开始将当前哈希桶中的所有节点转移到新的哈希桶中 if (oldTab != null) &#123; //遍历老的哈希桶 for (int j = 0; j &lt; oldCap; ++j) &#123; //取出当前的节点 e Node&lt;K,V&gt; e; //如果当前桶中有元素,则将链表赋值给e if ((e = oldTab[j]) != null) &#123; //将原哈希桶置空以便GC oldTab[j] = null; //如果当前链表中就一个元素，（没有发生哈希碰撞） if (e.next == null) //直接将这个元素放置在新的哈希桶里。 //注意这里取下标 是用 哈希值 与 桶的长度-1 。 由于桶的长度是2的n次方，这么做其实是等于 一个模运算。但是效率更高 newTab[e.hash &amp; (newCap - 1)] = e; //如果发生过哈希碰撞 ,而且是节点数超过8个，转化成了红黑树（暂且不谈 避免过于复杂， 后续专门研究一下红黑树） else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果发生过哈希碰撞，节点数小于8个。则要根据链表上每个节点的哈希值，依次放入新哈希桶对应下标位置。 else &#123; // preserve order //因为扩容是容量翻倍，所以原链表上的每个节点，现在可能存放在原来的下标，即low位， 或者扩容后的下标，即high位。 high位= low位+原哈希桶容量 //低位链表的头结点、尾节点 Node&lt;K,V&gt; loHead = null, loTail = null; //高位链表的头节点、尾节点 Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next;//临时节点 存放e的下一个节点 do &#123; next = e.next; //这里又是一个利用位运算 代替常规运算的高效点： 利用哈希值 与 旧的容量，可以得到哈希值去模后，是大于等于oldCap还是小于oldCap，等于0代表小于oldCap，应该存放在低位，否则存放在高位 if ((e.hash &amp; oldCap) == 0) &#123; //给头尾节点指针赋值 if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125;//高位也是相同的逻辑 else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125;//循环直到链表结束 &#125; while ((e = next) != null); //将低位链表存放在原index处， if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; //将高位链表存放在新index处 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 再看一下 往哈希表里插入一个节点的putVal函数,如果参数onlyIfAbsent是true，那么不会覆盖相同key的值value。如果evict是false。那么表示是在初始化时调用的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //tab存放 当前的哈希桶， p用作临时链表节点 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果当前哈希表是空的，代表是初始化 if ((tab = table) == null || (n = tab.length) == 0) //那么直接去扩容哈希表，并且将扩容后的哈希桶长度赋值给n n = (tab = resize()).length; //如果当前index的节点是空的，表示没有发生哈希碰撞。 直接构建一个新节点Node，挂载在index处即可。 //这里再啰嗦一下，index 是利用 哈希值 &amp; 哈希桶的长度-1，替代模运算 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123;//否则 发生了哈希冲突。 //e Node&lt;K,V&gt; e; K k; //如果哈希值相等，key也相等，则是覆盖value操作 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p;//将当前节点引用赋值给e else if (p instanceof TreeNode)//红黑树暂且不谈 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123;//不是覆盖操作，则插入一个普通链表节点 //遍历链表 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123;//遍历到尾部，追加新节点到尾部 p.next = newNode(hash, key, value, null); //如果追加节点后，链表数量》=8，则转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //如果找到了要覆盖的节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果e不是null，说明有需要覆盖的节点， if (e != null) &#123; // existing mapping for key //则覆盖节点值，并返回原oldValue V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; //这是一个空实现的函数，用作LinkedHashMap重写使用。 afterNodeAccess(e); return oldValue; &#125; &#125; //如果执行到了这里，说明插入了一个新的节点，所以会修改modCount，以及返回null。 //修改modCount ++modCount; //更新size，并判断是否需要扩容。 if (++size &gt; threshold) resize(); //这是一个空实现的函数，用作LinkedHashMap重写使用。 afterNodeInsertion(evict); return null;&#125; newNode如下：构建一个链表节点1234// Create a regular (non-tree) node Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(hash, key, value, next); &#125; 1234// Callbacks to allow LinkedHashMap post-actions//留给LinkedHashMap使用，当做链表的回调函数void afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125; 小结： 运算尽量都用位运算代替，更高效。 对于扩容导致需要新建数组存放更多元素时，除了要将老数组中的元素迁移过来，也记得将老数组中的引用置null，以便GC 取下标 是用 哈希值 与运算 （桶的长度-1） i = (n - 1) &amp; hash。 由于桶的长度是2的n次方，这么做其实是等于 一个模运算。但是效率更高 扩容时，如果发生过哈希碰撞，节点数小于8个。则要根据链表上每个节点的哈希值，依次放入新哈希桶对应下标位置。 因为扩容是容量翻倍，所以原链表上的每个节点，现在可能存放在原来的下标，即low位， 或者扩容后的下标，即high位。 high位= low位+原哈希桶容量 利用哈希值 与运算 旧的容量 ，if ((e.hash &amp; oldCap) == 0),可以得到哈希值去模后，是大于等于oldCap还是小于oldCap，等于0代表小于oldCap，应该存放在低位，否则存放在高位。这里又是一个利用位运算 代替常规运算的高效点 如果追加节点后，链表数量》=8，则转化为红黑树 插入节点操作时，有一些空实现的函数，用作LinkedHashMap重写使用。 增、改 往表中插入或覆盖一个key-value1234public V put(K key, V value) &#123; //先根据key，取得hash值。 再调用上一节的方法插入节点 return putVal(hash(key), key, value, false, true);&#125; 这个根据key取hash值的函数也要关注一下，它称之为“扰动函数”，关于这个函数的用处 开头已经总结过了：1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 而key的hash值，并不仅仅只是key对象的hashCode()方法的返回值，还会经过扰动函数的扰动，以使hash值更加均衡。因为hashCode()是int类型，取值范围是40多亿，只要哈希函数映射的比较均匀松散，碰撞几率是很小的。但就算原本的hashCode()取得很好，每个key的hashCode()不同，但是由于HashMap的哈希桶的长度远比hash取值范围小，默认是16，所以当对hash值以桶的长度取余，以找到存放该key的桶的下标时，由于取余是通过与操作完成的，会忽略hash值的高位。因此只有hashCode()的低位参加运算，发生不同的hash值，但是得到的index相同的情况的几率会大大增加，这种情况称之为hash碰撞。 即，碰撞率会增大。扰动函数就是为了解决hash碰撞的。它会综合hash值高位和低位的特征，并存放在低位，因此在与运算时，相当于高低位一起参与了运算，以减少hash碰撞的概率。（在JDK8之前，扰动函数会扰动四次，JDK8简化了这个操作） 往表中批量增加数据 1234public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; //这个函数上一节也已经分析过。//将另一个Map的所有元素加入表中，参数evict初始化时为false，其他情况为true putMapEntries(m, true);&#125; 只会往表中插入 key-value, 若key对应的value之前存在，不会覆盖。（jdk8增加的方法） 1234@Overridepublic V putIfAbsent(K key, V value) &#123; return putVal(hash(key), key, value, true, true);&#125; 删以key为条件删除如果key对应的value存在，则删除这个键值对。 并返回value。如果不存在 返回null。12345public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125; 从哈希表中删除某个节点， 如果参数matchValue是true，则必须key 、value都相等才删除。如果movable参数是false，在删除节点时，不移动其他节点123456789101112131415161718192021222324252627282930313233343536373839404142434445final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; // p 是待删除节点的前置节点 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //如果哈希表不为空，则根据hash值算出的index下 有节点的话。 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; //node是待删除节点 Node&lt;K,V&gt; node = null, e; K k; V v; //如果链表头的就是需要删除的节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p;//将待删除节点引用赋给node else if ((e = p.next) != null) &#123;//否则循环遍历 找到待删除节点，赋值给node if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //如果有待删除节点node， 且 matchValue为false，或者值也相等 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p)//如果node == p，说明是链表头是待删除节点 tab[index] = node.next; else//否则待删除节点在表中间 p.next = node.next; ++modCount;//修改modCount --size;//修改size afterNodeRemoval(node);//LinkedHashMap回调函数 return node; &#125; &#125; return null;&#125; 留给LinkedHashMap回调的接口1void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; 以key value 为条件删除12345@Overridepublic boolean remove(Object key, Object value) &#123; //这里传入了value 同时matchValue为true return removeNode(hash(key), key, value, true, true) != null;&#125; 查以key为条件，找到返回value。没找到返回null1234567891011121314151617181920212223242526public V get(Object key) &#123; Node&lt;K,V&gt; e; //传入扰动后的哈希值 和 key 找到目标节点Node return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;//传入扰动后的哈希值 和 key 找到目标节点Nodefinal Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //查找过程和删除基本差不多， 找到返回节点，否则返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 判断是否包含该key123public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null;&#125; 判断是否包含value123456789101112131415public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; //遍历哈希桶上的每一个链表 if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; //如果找到value一致的返回true if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false;&#125; java8新增，带默认值的get方法以key为条件，找到了返回value。否则返回defaultValue12345@Overridepublic V getOrDefault(Object key, V defaultValue) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? defaultValue : e.value;&#125; 遍历1234567//缓存 entrySettransient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; */public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es; return (es = entrySet) == null ? (entrySet = new EntrySet()) : es;&#125; EntrySet12345678910111213141516171819202122232425262728final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; //一般我们用到EntrySet，都是为了获取iterator public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return new EntryIterator(); &#125; //最终还是调用getNode方法 public final boolean contains(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;) o; Object key = e.getKey(); Node&lt;K,V&gt; candidate = getNode(hash(key), key); return candidate != null &amp;&amp; candidate.equals(e); &#125; //最终还是调用removeNode方法 public final boolean remove(Object o) &#123; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;) o; Object key = e.getKey(); Object value = e.getValue(); return removeNode(hash(key), key, value, true, true) != null; &#125; return false; &#125; //。。。 &#125; EntryIterator的实现1234final class EntryIterator extends HashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125;&#125; HashIterator1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253abstract class HashIterator &#123; Node&lt;K,V&gt; next; // next entry to return Node&lt;K,V&gt; current; // current entry int expectedModCount; // for fast-fail int index; // current slot HashIterator() &#123; //因为hashmap也是线程不安全的，所以要保存modCount。用于fail-fast策略 expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; //next 初始时，指向 哈希桶上第一个不为null的链表头 if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; //由这个方法可以看出，遍历HashMap时，顺序是按照哈希桶从低到高，链表从前往后，依次遍历的。属于无序集合。 final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; //fail-fast策略 if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); //依次取链表下一个节点， if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; //如果当前链表节点遍历完了，则取哈希桶下一个不为null的链表头 do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e; &#125; public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); ////fail-fast策略 if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; //最终还是利用removeNode 删除节点 removeNode(hash(key), key, null, false, false); expectedModCount = modCount; &#125; &#125; 和HashTable相比 与之相比HashTable是线程安全的，且不允许key、value是null。 HashTable默认容量是11。 HashTable是直接使用key的hashCode(key.hashCode())作为hash值，不像HashMap内部使用static final int hash(Object key)扰动函数对key的hashCode进行扰动后作为hash值。 HashTable取哈希桶下标是直接用模运算%.（因为其默认容量也不是2的n次方。所以也无法用位运算替代模运算） 扩容时，新容量是原来的2倍+1。int newCapacity = (oldCapacity &lt;&lt; 1) + 1; Hashtable是Dictionary的子类同时也实现了Map接口，HashMap是Map接口的一个实现类；———2017-08-15Runnable、Callable、Future、FutureTask的区别Runnable 其中Runnable应该是我们最熟悉的接口，它只有一个run()函数，用于将耗时操作写在其中，该函数没有返回值。然后使用某个线程去执行该runnable即可实现多线程，Thread类在调用start()函数后就是执行的是Runnable的run()函数。Runnable的声明如下123456789101112public interface Runnable &#123; /** * When an object implementing interface &lt;code&gt;Runnable&lt;/code&gt; is used * to create a thread, starting the thread causes the object's * &lt;code&gt;run&lt;/code&gt; method to be called in that separately executing * thread. * &lt;p&gt; * * @see java.lang.Thread#run() */ public abstract void run(); &#125; CallableCallable与Runnable的功能大致相似，Callable中有一个call()函数，但是call()函数有返回值，而Runnable的run()函数不能将结果返回给客户程序。Callable的声明如下 :123456789public interface Callable&lt;V&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception; &#125; FutureExecutor就是Runnable和Callable的调度容器，Future就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果、设置结果操作。get方法会阻塞，直到任务返回结果(Future简介)。Future声明如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * @see FutureTask * @see Executor * @since 1.5 * @author Doug Lea * @param &lt;V&gt; The result type returned by this Future's &lt;tt&gt;get&lt;/tt&gt; method */ public interface Future&lt;V&gt; &#123; /** * Attempts to cancel execution of this task. This attempt will * fail if the task has already completed, has already been cancelled, * or could not be cancelled for some other reason. If successful, * and this task has not started when &lt;tt&gt;cancel&lt;/tt&gt; is called, * this task should never run. If the task has already started, * then the &lt;tt&gt;mayInterruptIfRunning&lt;/tt&gt; parameter determines * whether the thread executing this task should be interrupted in * an attempt to stop the task. * */ boolean cancel(boolean mayInterruptIfRunning); /** * Returns &lt;tt&gt;true&lt;/tt&gt; if this task was cancelled before it completed * normally. */ boolean isCancelled(); /** * Returns &lt;tt&gt;true&lt;/tt&gt; if this task completed. * */ boolean isDone(); /** * Waits if necessary for the computation to complete, and then * retrieves its result. * * @return the computed result */ V get() throws InterruptedException, ExecutionException; /** * Waits if necessary for at most the given time for the computation * to complete, and then retrieves its result, if available. * * @param timeout the maximum time to wait * @param unit the time unit of the timeout argument * @return the computed result */ V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; &#125; FutureTaskFutureTask则是一个RunnableFuture，而RunnableFuture实现了Runnbale又实现了Futrue这两个接口，12345678910public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; ### RunnableFuture```javapublic interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; /** * Sets this Future to the result of its computation * unless it has been cancelled. */ void run(); &#125; 另外它还可以包装Runnable和Callable， 由构造函数注入依赖。123456public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable &#125; 1234public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable &#125; 可以看到，Runnable注入会被Executors.callable()函数转换为Callable类型，即FutureTask最终都是执行Callable类型的任务。该适配函数的实现如下 ：12345public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); return new RunnableAdapter&lt;T&gt;(task, result); &#125; RunnableAdapter适配器123456789101112131415/** * A callable that runs given task and returns given result */ static final class RunnableAdapter&lt;T&gt; implements Callable&lt;T&gt; &#123; final Runnable task; final T result; RunnableAdapter(Runnable task, T result) &#123; this.task = task; this.result = result; &#125; public T call() &#123; task.run(); return result; &#125; &#125; 由于FutureTask实现了Runnable，因此它既可以通过Thread包装来直接执行，也可以提交给ExecuteService来执行。并且还可以直接通过get()函数获取执行结果，该函数会阻塞，直到结果返回。因此FutureTask既是Future、Runnable，又是包装了(Callable如果是Runnable最终也会被转换为Callable )， 它是这两者的合体。 代码演示123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384```javaimport java.util.concurrent.*;public class RunnableFutureTask &#123; /** * ExecutorService */ static ExecutorService mExecutor = Executors.newSingleThreadExecutor(); /** * * @param args */ public static void main(String[] args) &#123; runnableDemo(); futureDemo(); &#125; /** * runnable, 无返回值 */ static void runnableDemo() &#123; new Thread(() -&gt; System.out.println(fibc(20))).start(); &#125; /** * 其中Runnable实现的是void run()方法，无返回值；Callable实现的是 V * call()方法，并且可以返回执行结果。其中Runnable可以提交给Thread来包装下 * ，直接启动一个线程来执行，而Callable则一般都是提交给ExecuteService来执行。 */ static void futureDemo() &#123; try &#123; /** * 提交runnable则没有返回值, future没有数据 */ Future&lt;?&gt; result = mExecutor.submit(() -&gt; fibc(20)); System.out.println("future result from runnable : " + result.get()); /** * 提交Callable, 有返回值, future中能够获取返回值 */ Future&lt;Integer&gt; result2 = mExecutor.submit(() -&gt; fibc(20)); System.out .println("future result from callable : " + result2.get()); /** * FutureTask则是一个RunnableFuture&lt;V&gt;，即实现了Runnbale又实现了Futrue&lt;V&gt;这两个接口， * 另外它还可以包装Runnable(实际上会转换为Callable)和Callable * &lt;V&gt;，所以一般来讲是一个符合体了，它可以通过Thread包装来直接执行，也可以提交给ExecuteService来执行 * ，并且还可以通过v get()返回执行结果，在线程体没有执行完成的时候，主线程一直阻塞等待，执行完则直接返回结果。 */ FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(() -&gt; fibc(20)); // 提交futureTask mExecutor.submit(futureTask) ; System.out.println("future result from futureTask : " + futureTask.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; /** * 效率底下的斐波那契数列, 耗时的操作 * * @param num * @return */ static int fibc(int num) &#123; if (num == 0) &#123; return 0; &#125; if (num == 1) &#123; return 1; &#125; return fibc(num - 1) + fibc(num - 2); &#125; 2017-08-16linux根目录下的文件详解linux哲学思想： 一切皆文件； 体积小，目的单一的小程序组成；组合小程序，完成复杂的任务； 尽量避免捕获用户接口； 通过配置文件保存程序的配置信息，而配置文件通常是纯文本文件； 根目录下的文件： /boot 该目录默认下存放的是Linux的启动文件和内核。 /initrd 它的英文含义是boot loader initialized RAM disk,就是由boot loader初始化的内存盘。在linux内核启动前，boot loader会将存储介质(一般是硬盘)中的initrd文件加载到内存，内核启动时会在访问真正的根文件系统前先访问该内存中的initrd文件系统。 /bin 该目录中存放Linux的常用命令。 /sbin 该目录用来存放系统管理员使用的管理程序。 /var 该目录存放那些经常被修改的文件，包括各种日志、数据文件。 /etc 该目录存放系统管理时要用到的各种配置文件和子目录，例如网络配置文件、文件系统、X系统配置文件、设备配置信息、设置用户信息等。 /dev 该目录包含了Linux系统中使用的所有外部设备，它实际上是访问这些外部设备的端口，访问这些外部设备与访问一个文件或一个目录没有区别。 /mnt 临时将别的文件系统挂在该目录下。 /root 如果你是以超级用户的身份登录的，这个就是超级用户的主目录。 /home 如果建立一个名为“xx”的用户，那么在/home目录下就有一个对应的“/home/xx”路径，用来存放该用户的主目录。 /usr 用户的应用程序和文件几乎都存放在该目录下。 /lib 该目录用来存放系统动态链接共享库，几乎所有的应用程序都会用到该目录下的共享库。 /opt 第三方软件在安装时默认会找这个目录,所以你没有安装此类软件时它是空的,但如果你一旦把它删除了,以后在安装此类软件时就有可能碰到麻烦。 /tmp 用来存放不同程序执行时产生的临时文件，该目录会被系统自动清理干净。 /proc 可以在该目录下获取系统信息，这些信息是在内存中由系统自己产生的，该目录的内容不在硬盘上而在内存里。 /misc 可以让多用户堆积和临时转移自己的文件。 /lost＋found 该目录在大多数情况下都是空的。但当突然停电、或者非正常关机后，有些文件就临时存放在这里。 2017-08-18线程池ThreadPoolExecutor解析JDK1.5中引入了强大的concurrent包，其中最常用的莫过了线程池的实现ThreadPoolExecutor，它给我们带来了极大的方便，但同时，对于该线程池不恰当的设置也可能使其效率并不能达到预期的效果，甚至仅相当于或低于单线程的效率。线程池的构造函数123456789101112131415161718192021public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; ThreadPoolExecutor类可设置的参数主要有： corePoolSize 核心线程数 核心线程会一直存活，即使没有任务需要处理。 当线程数小于核心线程数时，即使现有的线程空闲，线程池也会优先创建新线程来处理任务，而不是直接交给现有的线程处理。 核心线程在allowCoreThreadTimeout被设置为true时会超时退出，默认情况下不会退出。 maxPoolSize 最大线程数 当线程数大于或等于核心线程，且任务队列已满时，线程池会创建新的线程，直到线程数量达到maxPoolSize。 如果线程数已等于maxPoolSize，且任务队列已满，则已超出线程池的处理能力，线程池会拒绝处理任务而抛出异常。 keepAliveTime 线程空闲时间 当线程空闲时间达到keepAliveTime，该线程会退出，直到线程数量等于corePoolSize。 如果allowCoreThreadTimeout设置为true，则所有线程均会退出直到线程数量为0。 allowCoreThreadTimeout 允许核心线程超时 是否允许核心线程空闲退出，默认值为false queueCapacity 任务队列容量 当核心线程数达到最大时，新任务会放在队列中排队等待执行 rejectedExecutionHandler：任务拒绝处理器 两种情况会拒绝处理任务： 当线程数已经达到maxPoolSize，切队列已满，会拒绝新任务 当线程池被调用shutdown()后，会等待线程池里的任务执行完毕，再shutdown。如果在调用shutdown()和线程池真正shutdown之间提交任务，会拒绝新任务 线程池会调用rejectedExecutionHandler来处理这个任务。如果没有设置默认是AbortPolicy，会抛出异常 ThreadPoolExecutor类有几个内部实现类来处理这类情况： AbortPolicy 丢弃任务，抛运行时异常 CallerRunsPolicy 执行任务 DiscardPolicy 忽视，什么都不会发生 DiscardOldestPolicy 从队列中踢出最先进入队列（最后一个执行）的任务,如果使用的任务队列是优先队列PriorityBlockingQueue，那么抛弃权重最高的任务 实现RejectedExecutionHandler接口，可自定义处理器 如何执行任务线程池按以下行为执行任务： 当线程数小于核心线程数时，创建线程。 当线程数大于等于核心线程数，且任务队列未满时，将任务放入任务队列。 当线程数大于等于核心线程数，且任务队列已满 若线程数小于最大线程数，创建线程 若线程数等于最大线程数，抛出异常，拒绝任务 如何设置参数默认值：123456corePoolSize=1 queueCapacity=Integer.MAX_VALUE maxPoolSize=Integer.MAX_VALUE keepAliveTime=60s allowCoreThreadTimeout=false rejectedExecutionHandler=AbortPolicy() 需要根据几个值来决定 tasks ：每秒的任务数，假设为500~1000 taskcost：每个任务花费时间，假设为0.1s responsetime：系统允许容忍的最大响应时间，假设为1s做几个计算 corePoolSize = 每秒需要多少个线程处理？ threadcount = tasks / (1 / taskcost) = tasks taskcout = (500~1000) * 0.1 = 50~100 个线程。corePoolSize设置应该大于50 根据8020原则，如果80%的每秒任务数小于800，那么corePoolSize设置为80即可 queueCapacity = (coreSizePool/taskcost) * responsetime 计算可得 queueCapacity = 80 / 0.1 * 1 = 80。意思是队列里的线程可以等待1s，超过了的需要新开线程来执行 切记不能设置为Integer.MAX_VALUE，这样队列会很大，线程数只会保持在corePoolSize大小，当任务陡增时，不能新开线程来执行，响应时间会随之陡增。 maxPoolSize = (max(tasks)- queueCapacity) / (1 / taskcost) 计算可得 maxPoolSize = (1000 - 80) / 10 = 92 * （最大任务数-队列容量）/ 每个线程每秒处理能力 = 最大线程数 rejectedExecutionHandler：根据具体情况来决定，任务不重要可丢弃，任务重要则要利用一些缓冲机制来处理 keepAliveTime和allowCoreThreadTimeout采用默认通常能满足 要想合理的配置线程池，就必须首先分析任务特性，可以从以下几个角度来进行分析： 任务的性质：CPU密集型任务，IO密集型任务和混合型任务。任务的优先级：高，中和低。任务的执行时间：长，中和短。任务的依赖性：是否依赖其他系统资源，如数据库连接。任务性质不同的任务可以用不同规模的线程池分开处理。CPU密集型任务配置尽可能小的线程，如配置Ncpu+1个线程的线程池。IO密集型任务则由于线程并不是一直在执行任务，则配置尽可能多的线程，如2*Ncpu。混合型的任务，如果可以拆分，则将其拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐率要高于串行执行的吞吐率，如果这两个任务执行时间相差太大，则没必要进行分解。我们可以通过Runtime.getRuntime().availableProcessors()方法获得当前设备的CPU个数。 优先级不同的任务可以使用优先级队列PriorityBlockingQueue来处理。它可以让优先级高的任务先得到执行，需要注意的是如果一直有优先级高的任务提交到队列里，那么优先级低的任务可能永远不能执行。 执行时间不同的任务可以交给不同规模的线程池来处理，或者也可以使用优先级队列，让执行时间短的任务先执行。 依赖数据库连接池的任务，因为线程提交SQL后需要等待数据库返回结果，如果等待的时间越长CPU空闲时间就越长，那么线程数应该设置越大，这样才能更好的利用CPU。 建议使用有界队列，有界队列能增加系统的稳定性和预警能力，可以根据需要设大一点，比如几千。有一次我们组使用的后台任务线程池的队列和线程池全满了，不断的抛出抛弃任务的异常，通过排查发现是数据库出现了问题，导致执行SQL变得非常缓慢，因为后台任务线程池里的任务全是需要向数据库查询和插入数据的，所以导致线程池里的工作线程全部阻塞住，任务积压在线程池里。如果当时我们设置成无界队列，线程池的队列就会越来越多，有可能会撑满内存，导致整个系统不可用，而不只是后台任务出现问题。当然我们的系统所有的任务是用的单独的服务器部署的，而我们使用不同规模的线程池跑不同类型的任务，但是出现这样问题时也会影响到其他任务。 线程池的监控通过线程池提供的参数进行监控。线程池里有一些属性在监控线程池的时候可以使用 taskCount：线程池需要执行的任务数量。 completedTaskCount：线程池在运行过程中已完成的任务数量。小于或等于taskCount。 largestPoolSize：线程池曾经创建过的最大线程数量。通过这个数据可以知道线程池是否满过。如等于线程池的最大大小，则表示线程池曾经满了。 getPoolSize:线程池的线程数量。如果线程池不销毁的话，池里的线程不会自动销毁，所以这个大小只增不+ getActiveCount：获取活动的线程数。通过扩展线程池进行监控。通过继承线程池并重写线程池的beforeExecute，afterExecute和terminated方法，我们可以在任务执行前，执行后和线程池关闭前干一些事情。如监控任务的平均执行时间，最大执行时间和最小执行时间等。这几个方法在线程池里是空方法。如：1protected void beforeExecute(Thread t, Runnable r) &#123; &#125; 2017-08-19Linux后台运行jar文件方式一1java -jar XXX.jar 特点：当前ssh窗口被锁定，可按CTRL + C打断程序运行，或直接关闭窗口，程序退出那如何让窗口不锁定？ 方式二1java -jar XXX.jar &amp; &amp;代表在后台运行。特定：当前ssh窗口不被锁定，但是当窗口关闭时，程序中止运行。继续改进，如何让窗口关闭时，程序仍然运行？ 方式三1nohup java -jar XXX.jar &amp; nohup 意思是不挂断运行命令,当账户退出或终端关闭时,程序仍然运行当用 nohup 命令执行作业时，缺省情况下该作业的所有输出被重定向到nohup.out的文件中，除非另外指定了输出文件。 方式四1nohup java -jar XXX.jar &gt;result.log 2&gt;error.log &amp; command &gt;out.filecommand &gt;out.file是将command的输出重定向到out.file文件，即输出内容不打印到屏幕上，而是输出到out.file文件中这里我们将结果输出重定向到result.log中2 &gt; out.file是指将错误的输出重定向到文件，我们将错误的输出重定向到error.log 查看命令可通过jobs命令查看后台运行任务1jobs 那么就会列出所有后台执行的作业，并且每个作业前面都有个编号。如果想将某个作业调回前台控制，只需要 fg + 编号即可。1fg 23 __ 2017-08-23内部类为什么可以访问外部类的属性内部类定义内部类就是定义在一个类内部的类。定义在类内部的类有两种情况：一种是被static关键字修饰的， 叫做静态内部类， 另一种是不被static关键字修饰的， 就是普通内部类。 在下文中所提到的内部类都是指这种不被static关键字修饰的普通内部类。 静态内部类虽然也定义在外部类的里面， 但是它只是在形式上（写法上）和外部类有关系， 其实在逻辑上和外部类并没有直接的关系。而一般的内部类，不仅在形式上和外部类有关系（写在外部类的里面）， 在逻辑上也和外部类有联系。 这种逻辑上的关系可以总结为以下两点： 内部类对象的创建依赖于外部类对象； 内部类对象持有指向外部类对象的引用。 上边的第二条可以解释为什么在内部类中可以访问外部类的成员。就是因为内部类对象持有外部类对象的引用。但是我们不禁要问， 为什么会持有这个引用？在源代码层面， 我们无法看到原因，因为Java为了语法的简介， 省略了很多该写的东西， 也就是说很多东西本来应该在源代码中写出， 但是为了简介起见， 不必在源码中写出，编译器在编译时会加上一些代码。 现在我们就看看Java的编译器为我们加上了什么？首先建一个工程TestInnerClass用于测试。 在该工程中为了简单起见， 没有创建包， 所以源代码直接在默认包中。在该工程中， 只有下面一个简单的文件。123456789public class Outer &#123; int outerField = 0; class Inner&#123; void InnerMethod()&#123; int i = outerField; &#125; &#125;&#125; 编译后产生两个class文件，分别是Outer$Inner.class和Outer.class，这里我们看起来内部类除了前面有个外部类的名字之外，和其他类并没有区别，别的类和外部类也是两个不同的class文件，为什么内部类就可以访问呢？我们这样想，java总归还是java，再怎么变也不会超过这个语言的特性，能访问这个类说明，肯定是内部类持有一个引用，指向了外部类，编译器应该是帮我们做了这些事，我们不知道而已。 反编译这里我们的目的是探究内部类的行为， 所以只反编译内部类的class文件Outer$Inner.class 。 在命令行中， 切换到工程的bin目录， 输入以下命令反编译这个类文件：1javap -classpath . -v Outer$Inner -classpath . 说明在当前目录下寻找要反编译的class文件 -v 加上这个参数输出的信息比较全面。包括常量池和方法内的局部变量表， 行号， 访问标志等等。注意， 如果有包名的话， 要写class文件的全限定名， 如：1javap -classpath . -v com.baidu.Outer$Inner 反编译的输出结果很多， 为了篇幅考虑， 在这里我们省略了常量池。 下面给出除了常量池之外的输出信息1234567891011121314151617181920212223242526272829303132333435363738&#123; final Outer this$0; flags: ACC_FINAL, ACC_SYNTHETIC Outer$Inner(Outer); flags: Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: putfield #10 // Field this$0:LOuter; 5: aload_0 6: invokespecial #12 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 9: return LineNumberTable: line 5: 0 LocalVariableTable: Start Length Slot Name Signature 0 10 0 this LOuter$Inner; void InnerMethod(); flags: Code: stack=1, locals=2, args_size=1 0: aload_0 1: getfield #10 // Field this$0:LOuter; 4: getfield #20 // Field Outer.outerField:I 7: istore_1 8: return LineNumberTable: line 7: 0 line 8: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this LOuter$Inner; 8 1 1 i I&#125; 首先我们会看到， 第一行的信息如下:1final Outer this$0; 这句话的意思是， 在内部类Outer$Inner中， 存在一个名字为this$0 ， 类型为Outer的成员变量， 并且这个变量是final的。 其实这个就是所谓的“在内部类对象中存在的指向外部类对象的引用”。但是我们在定义这个内部类的时候， 并没有声明它， 所以这个成员变量是编译器加上的。虽然编译器在创建内部类时为它加上了一个指向外部类的引用， 但是这个引用是怎样赋值的呢？毕竟必须先给他赋值， 它才能指向外部类对象。 下面我们把注意力转移到构造函数上。 下面这段输出是关于构造函数的信息。123456789101112131415Outer$Inner(Outer); flags: Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: putfield #10 // Field this$0:LOuter; 5: aload_0 6: invokespecial #12 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 9: return LineNumberTable: line 5: 0 LocalVariableTable: Start Length Slot Name Signature 0 10 0 this LOuter$Inner; 我们知道， 如果在一个类中， 不声明构造方法的话， 编译器会默认添加一个无参数的构造方法。 但是这句话在这里就行不通了， 因为我们明明看到， 这个构造函数有一个构造方法， 并且类型为Outer。 所以说，编译器会为内部类的构造方法添加一个参数， 参数的类型就是外部类的类型。下面我们看看在构造参数中如何使用这个默认添加的参数。 我们来分析一下构造方法的字节码。 下面是每行字节码的意义： aload_0 ： 将局部变量表中的第一个引用变量加载到操作数栈。 这里有几点需要说明。 局部变量表中的变量在方法执行前就已经初始化完成；局部变量表中的变量包括方法的参数；成员方法的局部变量表中的第一个变量永远是this；操作数栈就是执行当前代码的栈。所以这句话的意思是： 将this引用从局部变量表加载到操作数栈。 aload_1：将局部变量表中的第二个引用变量加载到操作数栈。 这里加载的变量就是构造方法中的Outer类型的参数。 putfield #10 // Field this$0:LOuter;使用操作数栈顶端的引用变量为指定的成员变量赋值。 这里的意思是将外面传入的Outer类型的参数赋给成员变量this$0 。这一句putfield字节码就揭示了， 指向外部类对象的这个引用变量是如何赋值的。下面几句字节码和本文讨论的话题无关， 只做简单的介绍。 下面几句字节码的含义是： 使用this引用调用父类（Object）的构造方法然后返回。 用我们比较熟悉的形式翻译过来， 这个内部类和它的构造函数有点像这样： （注意， 这里不符合Java的语法， 只是为了说明问题） 12345678class Outer$Inner&#123; final Outer this$0; public Outer$Inner(Outer outer)&#123; this.this$0 = outer; super(); &#125; &#125; 关于在内部类中如何使用指向外部类的引用访问外部类成员， 就不用多做解释了， 其实和普通的通过引用访问成员的方式是相同的。 在内部类的InnerMethod方法中， 访问了外部类的成员变量outerField， 下面的字节码揭示了访问是如何进行的：123456789void InnerMethod(); flags: Code: stack=1, locals=2, args_size=1 0: aload_0 1: getfield #10 // Field this$0:LOuter; 4: getfield #20 // Field Outer.outerField:I 7: istore_1 8: return getfield #10 // Field this$0:LOuter;将成员变量this$0加载到操作数栈上来 getfield #20 // Field Outer.outerField:I使用上面加载的this$0引用， 将外部类的成员变量outerField加载到操作数栈 istore_1将操作数栈顶端的int类型的值保存到局部变量表中的第二个变量上（注意， 第一个局部变量被this占用， 第二个局部变量是i）。操作数栈顶端的int型变量就是上一步加载的outerField变量。 所以， 这句字节码的含义就是： 使用outerField为i赋值。 上面三步就是内部类中是如何通过指向外部类对象的引用， 来访问外部类成员的。 总结通过反编译内部类的字节码， 说明了内部类是如何访问外部类对象的成员的，除此之外， 我们也对编译器的行为有了一些了解， 编译器在编译时会自动加上一些逻辑， 这正是我们感觉困惑的原因。 关于内部类如何访问外部类的成员， 分析之后其实也很简单， 主要是通过以下几步做到的： 编译器自动为内部类添加一个成员变量， 这个成员变量的类型和外部类的类型相同， 这个成员变量就是指向外部类对象的引用； 编译器自动为内部类的构造方法添加一个参数， 参数的类型是外部类的类型， 在构造方法内部使用这个参数为1中添加的成员变量赋值； 在调用内部类的构造函数初始化内部类对象时， 会默认传入外部类的引用。 其实内部类可以访问类这个细节我们都知道，可是为什么呢？这就需要我们有思考问题的能力，深入探究细节，知其然和知其所以然，思维方式需要转变，深入的去考虑问题，不要只停留在表面，这也是自己需要提升的地方。 2017-08-29字节数组转16进制字符串对每一个字节，先和0xFF做与运算，然后使用Integer.toHexString()函数，如果结果只有1位，需要在前面加0。12345678910111213141516/* * 字节数组转16进制字符串 */ public static String bytes2HexString(byte[] b) &#123; String r = ""; for (int i = 0; i &lt; b.length; i++) &#123; String hex = Integer.toHexString(b[i] &amp; 0xFF); if (hex.length() == 1) &#123; hex = '0' + hex; &#125; r += hex.toUpperCase(); &#125; return r; &#125; 2017-08-31crontab 定时任务是什么简而言之呢，crontab就是一个自定义定时器。通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常适合周期性的日志分析或数据备份等工作 配置文件 其一：/var/spool/cron/该目录下存放的是每个用户（包括root）的crontab任务，文件名以用户名命名 其二：/etc/cron.d/这个目录用来存放任何要执行的crontab文件或脚本。 crontab时间说明12345678# .---------------- minute (0 - 59) # | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR#sun,mon,tue,wed,thu,fri,sat # | | | | |# * * * * * command to be executed minute：代表一小时内的第几分，范围 0-59。 hour：代表一天中的第几小时，范围 0-23。 mday：代表一个月中的第几天，范围 1-31。 month：代表一年中第几个月，范围 1-12。 wday：代表星期几，范围 0-7 (0及7都是星期天)。 who：要使用什么身份执行该指令，当您使用 crontab -e 时，不必加此字段。 command：所要执行的指令。 crontab服务状态12345sudo service crond start #启动服务sudo service crond stop #关闭服务sudo service crond restart #重启服务sudo service crond reload #重新载入配置sudo service crond status #查看服务状态 crontab命令查看crontab定时任务1crontab -l 编辑定时任务【删除-添加-修改】1crontab -e 添加定时任务【推荐】Step-One : 编辑任务脚本【分目录存放】【ex: backup.sh】Step-Two : 编辑定时文件【命名规则:backup.cron】Step-Three : crontab命令添加到系统crontab backup.cronStep-Four : 查看crontab列表 crontab -l crontab时间举例12345678910111213141516171819202122232425262728293031323334353637383940414243# 每天早上6点 0 6 * * * echo &quot;Good morning.&quot; &gt;&gt; /tmp/test.txt //注意单纯echo，从屏幕上看不到任何输出，因为cron把任何输出都email到root的信箱了。# 每两个小时 0 */2 * * * echo &quot;Have a break now.&quot; &gt;&gt; /tmp/test.txt # 晚上11点到早上8点之间每两个小时和早上八点 0 23-7/2，8 * * * echo &quot;Have a good dream&quot; &gt;&gt; /tmp/test.txt# 每个月的4号和每个礼拜的礼拜一到礼拜三的早上11点 0 11 4 * 1-3 command line# 1月1日早上4点 0 4 1 1 * command line SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root //如果出现错误，或者有数据输出，数据作为邮件发给这个帐号 HOME=/ # 每小时（第一分钟）执行/etc/cron.hourly内的脚本01 * * * * root run-parts /etc/cron.hourly# 每天（凌晨4：02）执行/etc/cron.daily内的脚本02 4 * * * root run-parts /etc/cron.daily # 每星期（周日凌晨4：22）执行/etc/cron.weekly内的脚本22 4 * * 0 root run-parts /etc/cron.weekly # 每月（1号凌晨4：42）去执行/etc/cron.monthly内的脚本 42 4 1 * * root run-parts /etc/cron.monthly # 注意: &quot;run-parts&quot;这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是文件夹名。 # 每天的下午4点、5点、6点的5 min、15 min、25 min、35 min、45 min、55 min时执行命令。 5，15，25，35，45，55 16，17，18 * * * command# 每周一，三，五的下午3：00系统进入维护状态，重新启动系统。00 15 * *1，3，5 shutdown -r +5# 每小时的10分，40分执行用户目录下的innd/bbslin这个指令： 10，40 * * * * innd/bbslink # 每小时的1分执行用户目录下的bin/account这个指令： 1 * * * * bin/account# 每天早晨三点二十分执行用户目录下如下所示的两个指令（每个指令以;分隔）： 203 * * * （/bin/rm -f expire.ls logins.bad;bin/expire$#@62;expire.1st）]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hadoop初探]]></title>
    <url>%2F2017%2F07%2F30%2Fhadoop%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[前言近几年，大数据，云计算，机器学习成为了非常热门的话题，这些技术运用在了很多的领域，也是在未来很有发展前景的技术。自己最近在接触一些大数据的东西，学习大数据的话自然很有必要先学习Hadoop和Spark。这里我们就来一探Hadoop的究竟吧。 Hadoop是什么Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。核心设计： HDFS: HDFS为海量的数据提供了存储 MapReduce: MapReduce为海量的数据提供了计算 Hadoop的作者是Doug Cutting，他受到谷歌的三篇论文的启发GFS(分布式存储系统)，MapReduce(分布式运行模型), BigTable(大表)，然后用Java去实现了这三个功能，然后就有了Hadoop，不得不感叹，牛人真的是牛人啊 Hadoop是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式,数据来源可以来自任何的形式，无论数据采用什么形式，最终都会转换成key-value的形式，key/value是基本数据单元简单总结来说，Hadoop是一种分布式计算的解决方案 解决了什么问题Hadoop就是解决了大数据（大到一台计算机无法进行存储，一台计算机无法在要求的时间内进行处理）的可靠存储和处理的问题。也就是两个核心的设计，HDFS和MapReduce HDFS设计特点1、大数据文件，非常适合上T级别的大文件或者一堆大数据文件的存储，现在互联网上的数据量非常庞大，动不动上T的数据，所以非常适合Hadoop。2、文件分块存储，HDFS会将一个完整的大文件平均分块存储到不同计算器上，它的意义在于读取文件时可以同时从多个主机取不同区块的文件，多主机读取比单主机读取效率要高得多。3、流式数据访问，一次写入多次读写，这种模式跟传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化也只能在文件末添加内容。4、廉价硬件，HDFS可以应用在普通PC机上，这种机制能够让给一些公司用几十台廉价的计算机就可以撑起一个大数据集群。5、硬件故障，HDFS认为所有计算机都可能会出问题，为了防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其它某几个主机上，如果其中一台主机失效，可以迅速找另一块副本取文件。 关键元素 Block：将一个文件进行分块，通常是128M。 NameNode：保存整个文件系统的目录信息、文件信息及分块信息，这是由唯一一台主机专门保存，当然这台主机如果出错，NameNode就失效了。在Hadoop2.*开始支持activity-standy模式—-如果主NameNode失效，启动备用主机运行NameNode。 DataNode：分布在廉价的计算机上，用于存储Block块文件。 MapReduceMapReduce是一套从海量源数据提取分析元素最后返回结果集的编程模型，将文件分布式存储到硬盘是第一步，而从海量数据中提取分析我们需要的内容就是MapReduce做的事了。 举个例子吧，假如说你想统计一个巨大的文本文件存储在HDFS上，你想要知道这个文本里各个词的出现频率。我们把我们要运算的逻辑分发到各个节点上，在每个节点上进行运算和统计，假如在各个节点上对这些单词进行统计，我们输入的格式是一行一行的文本，而统计的结果像key-value的形式，比如在第一个节点上(hello, 30), (world, 22), (hadoop, 60),第二个节点上(hello, 20), (world, 32), (spark, 70)，也就是说将任何形式的数据转换成key-value的形式，这个过程就是Map。然后我们要统计整个文本的单词出现的次数，就要对这些节点上的数据进行汇总，将这些节点上的数据按照key分组，合并，也就是(a, num1),(a, num2), (b, num3),(b, num4(合并后就变成(a, num1 + num2), (b, num3 + num4),按照上面的结果合并就是(hello, 50), (world, 54), (hadoop, 60), spark(70)，这个过程就是Reduce 适用场景hadoop擅长离线日志分析，facebook就用Hive来进行日志分析，2009年时facebook就有非编程人员的30%的人使用HiveQL进行数据分析；淘宝搜索中的自定义筛选也使用的Hive；利用Pig还可以做高级的数据处理，包括Twitter、LinkedIn上用于发现您可能认识的人，可以实现类似Amazon.com的协同过滤的推荐效果。淘宝的商品推荐也是！在Yahoo！的40%的Hadoop作业是用pig运行的，包括垃圾邮件的识别和过滤，还有用户特征建模。（2012年8月25新更新，天猫的推荐系统是hive，少量尝试mahout！）不过从现在企业的使用趋势来看,Pig慢慢有点从企业的视野中淡化了。 Hadoop伪分布式的安装好了，我们了解和学习了Hadoop的概念之后就来学习一下如何安装Hadoop吧，这里我们先来学习伪分布式的安装，也就是NameNode和DataNoe都在同一台服务器上而且salve也是自己 环境准备 虚拟机Vmware Centos 6.9 Hadoop 2.7.3 JDK 1.8 配置网络IP我们首先先安装好Centos，然后配置好网络，虚拟机与主机的连接方式选择NAT,然后cmd命令输入ipconfig，记录下VMware Network Adapter VMnet8 下的IP,在虚拟机中输入1vim /etc/sysconfig/network-scripts/ifcfg-eth0 IP地址要和VMware Network Adapter VMnet8 下的IP在同一个网段,我的IP是192.168.109.1，贴一个自己的配置123456789DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=staticIPADDR=192.168.109.3NETMASK=255.255.255.0GATEWAY=192.168.109.2DNS1=192.168.109.2 然后使用命令service network restart重启网络 修改主机名1vim /etc/sysconfig/network NETWORKING=yesHOSTNAME=hadoop1 修改主机名和IP的映射关系1vim /etc/hosts 192.168.109.3 hadoop1 关闭防火墙这里我们需要关闭我们的防火墙，开启防火墙会有访问限制，在虚拟机局域网内我们也不需要做访问限制，索性就把防火墙关了 12345678#查看防火墙状态service iptables status#关闭防火墙service iptables stop#查看防火墙开机启动状态chkconfig iptables --list#关闭防火墙开机启动chkconfig iptables off 创建用户我们一般不直接使用root用户，会创建一个新的用户来完成我们的实验，这里我们新建一个hadoop用户user add hadoop接下来为hadoop用户设置密码passwd hadoop然后我们为hadoop用户授予root权限vim /etc/sudoer找到root ALL=(ALL) ALL 并下面加入以下123## Allow root to run any commands anywhere root ALL=(ALL) ALLhadoop ALL=(ALL) ALL 切换用户现在我们切换到hadoop用户下进行操作 su hadoop 安装软件我们在根目录下创建一个app文件夹, mkdir app,然后我们将需要弄的文件都解压到app文件夹里面用winscp上传JDK,Hadoop的文件，解压JDK, 执行命令 tar -zxvf jdk-8u131-linux-x64.tar.gz -C app 解压Hadoop tar -zxvf hadoop-2.7.3.tar.gz -C app 这个时候我们将jdk和hadoop都解压到app目录下，接下来我们就开始配置环境了 配置环境配置JDK1vim /etc/profile 在文件最后添加如下： 12export JAVA_HOME=/home/hadoop/app/jdk1.8.0_131export PATH=$PATH:$JAVA_HOME/bin 刷新配置 1source /etc/profile 配置Hadoop注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop,这里我们的目录就是在/home/hadoop/app/hadoop-2.7.3/etc/下伪分布式需要修改5个配置文件 hadoop-env.sh这个文件表示hadoop运行环境的文件，找到25行，改成export JAVA_HOME=/home/hadoop/app/jdk1.8.0_131这个值原来是${JAVA_HOME}，但是有点问题，老是获取不到正确的值，所以这里我们就直接将它写死 core-site.xml配置如下： 123456789101112&lt;configuration&gt; &lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.7.3/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml配置如下： 1234567&lt;configuration&gt; &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml这里先执行一个命令，重命名模板文件mv mapred-site.xml.template mapred-site.xml然后再修改vim mapred-site.xml配置如下： 1234567&lt;configuration&gt; &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml配置如下： 12345678910111213&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5个文件都修改完成后，我们需要将hadoop添加到环境中vim /etc/proflie加上HADOOP_HOME, 修改文件内容如下：1234export JAVA_HOME=/home/hadoop/app/jdk1.8.0_131export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/home/hadoop/app/hadoop-2.7.3export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 格式化hdfs第一次启动的时候我们需要格式化namenode，对namenode进行初始化，执行以下命令 1hdfs namenode -format (hadoop namenode -format) 成功的话，会看到 “successfully formatted” 和 “Exitting with status 0″ 的提示，若为 “Exitting with status 1″ 则是出错。 启动hadoop先启动HDFS: start-dfs.sh再启动YARN: start-yarn.sh期间会让你多次输入密码，我们在后面配置SSH免密登录之后就不用输入密码了验证是否启动成功，使用jps命令验证12345627408 NameNode28218 Jps27643 SecondaryNameNode28066 NodeManager27803 ResourceManager27512 DataNode 看到以上进程的时候，就说明我们启动成功了 配置SSH免登录生成ssh免登陆密钥，进入到我的home目录cd ~/.ssh,执行 1ssh-keygen -t rsa （四个回车） 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）将公钥拷贝到要免登陆的机器上ssh-copy-id localhost然后我们执行ssh localhost 就可以不用输入密码登录这台机器了 进入Web管理界面我们在浏览器里面输入http://192.168.109.3:50070/可以看到如下图片 说明我们的hadooop已经开启成功了 运行mapreduce程序好了，我们的环境也搭建成功了，现在来试着跑一下mapreduce程序，进入hadoop的share目录下 1cd /home/hadoop/app/hadoop-2.7.3/share/hadoop/mapreduce 看到有以下文件：123456789101112hadoop-mapreduce-client-app-2.7.3.jarhadoop-mapreduce-client-common-2.7.3.jarhadoop-mapreduce-client-core-2.7.3.jarhadoop-mapreduce-client-hs-2.7.3.jarhadoop-mapreduce-client-hs-plugins-2.7.3.jarhadoop-mapreduce-client-jobclient-2.7.3.jarhadoop-mapreduce-client-jobclient-2.7.3-tests.jarhadoop-mapreduce-client-shuffle-2.7.3.jarhadoop-mapreduce-examples-2.7.3.jarliblib-examplessources 运算PI圆周率这里我们用hadoop-mapreduce-examples-2.7.3.jar的例子跑一下，执行的命令为hadoop jar hadoop-mapreduce-examples-2.7.3.jar pi 5 10pi是运算圆周率，后面的两个参数代表map的任务数量和map的取样数，取样数越大，运算的结果越精确，这里我们取了5和10作为参数，结果如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126Number of Maps = 5Samples per Map = 10Wrote input for Map #0Wrote input for Map #117/07/20 22:34:51 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)Wrote input for Map #217/07/20 22:34:51 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)Wrote input for Map #317/07/20 22:34:51 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)Wrote input for Map #4Starting Job17/07/20 22:34:51 INFO client.RMProxy: Connecting to ResourceManager at hadoop1/192.168.109.3:803217/07/20 22:34:52 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:577) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:573)17/07/20 22:34:52 INFO input.FileInputFormat: Total input paths to process : 517/07/20 22:34:52 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)17/07/20 22:34:52 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)17/07/20 22:34:52 INFO mapreduce.JobSubmitter: number of splits:517/07/20 22:34:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1500560388081_000317/07/20 22:34:53 INFO impl.YarnClientImpl: Submitted application application_1500560388081_000317/07/20 22:34:53 INFO mapreduce.Job: The url to track the job: http://hadoop1:8088/proxy/application_1500560388081_0003/17/07/20 22:34:53 INFO mapreduce.Job: Running job: job_1500560388081_000317/07/20 22:35:08 INFO mapreduce.Job: Job job_1500560388081_0003 running in uber mode : false17/07/20 22:35:08 INFO mapreduce.Job: map 0% reduce 0%17/07/20 22:36:14 INFO mapreduce.Job: map 100% reduce 0%17/07/20 22:36:28 INFO mapreduce.Job: map 100% reduce 100%17/07/20 22:36:29 INFO mapreduce.Job: Job job_1500560388081_0003 completed successfully17/07/20 22:36:29 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=116 FILE: Number of bytes written=714243 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=1320 HDFS: Number of bytes written=215 HDFS: Number of read operations=23 HDFS: Number of large read operations=0 HDFS: Number of write operations=3 Job Counters Launched map tasks=5 Launched reduce tasks=1 Data-local map tasks=5 Total time spent by all maps in occupied slots (ms)=325021 Total time spent by all reduces in occupied slots (ms)=8256 Total time spent by all map tasks (ms)=325021 Total time spent by all reduce tasks (ms)=8256 Total vcore-milliseconds taken by all map tasks=325021 Total vcore-milliseconds taken by all reduce tasks=8256 Total megabyte-milliseconds taken by all map tasks=332821504 Total megabyte-milliseconds taken by all reduce tasks=8454144 Map-Reduce Framework Map input records=5 Map output records=10 Map output bytes=90 Map output materialized bytes=140 Input split bytes=730 Combine input records=0 Combine output records=0 Reduce input groups=2 Reduce shuffle bytes=140 Reduce input records=10 Reduce output records=0 Spilled Records=20 Shuffled Maps =5 Failed Shuffles=0 Merged Map outputs=5 GC time elapsed (ms)=8989 CPU time spent (ms)=9260 Physical memory (bytes) snapshot=458428416 Virtual memory (bytes) snapshot=12371886080 Total committed heap usage (bytes)=624766976 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=590 File Output Format Counters Bytes Written=97Job Finished in 98.692 secondsEstimated value of Pi is 3.28000000000000000000 我们看最后一行，得出结果为3.28,是我们的样本数量太少了，要是样本数量大一点，结果应该更接近3.14还发现个问题，运行中出现了多次警告1234567817/07/20 22:34:52 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546) 我google了一下，再hadoop issue里面找到了答案https://issues.apache.org/jira/browse/HDFS-10429，发现是bug,忽略就好了，换个hadoop版本也许就没事了，或者修改日志的输出级别 计算单词数量wordcount首先我们先新建一个words.txt文件，内容如下123456hello worldhello tomhello kevinhello jerryhello babytom and jerry 然后在hdfs里创建一个目录1hadoop fs -mkdir -p /wordcount/input 把文件上传到该目录下1hadoop fs -put words.txt /wordcount/input 查看文件是否上传上去了1hadoop fs -ls /wordcount/input 我们可以看到我们的文件已经成功上传上去了我们发现操作hdfs的命令和操作linux的命令大致都是一样的，大家可以自行去看官方的文档回到刚才的share目录下，继续执行刚才的那个示例文件 1hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /wordcount/input /wordcount/output 这里执行的方法是wordcount，第一个参数是输入的文件位置，第二个参数是输出的结果的文件位置执行结束后，我们来看一下输出目录hadoop fs -ls /wordcount/output,发下目录下生成了两个文件123Found 2 items-rw-r--r-- 1 hadoop supergroup 0 2017-07-20 23:07 /wordcount/output/_SUCCESS-rw-r--r-- 1 hadoop supergroup 51 2017-07-20 23:07 /wordcount/output/part-r-00000 查看一下part-r-00000这个文件 1hadoop fs -cat /wordcount/output/part-r-00000 结果如下：1234567and 1baby 1hello 5jerry 2kevin 1tom 2world 1 可以看到，结果是正确的，大功告成 总结接触一个新的技术，安装和配置环境的都是一件比较麻烦的事，可能你第一天就要花费很多时间在搭建环境上面了，可能期间你会遇到各种问题，不过也是锻炼耐心的一个过程，有一个不错的方法可以解决，那就是使用docker容器技术，使用别人搭建好的环境镜像，直接拿来用就可以，这样我们就可以不必花费太多时间在环境问题上，专心学我们的技术，有兴趣的同学可以自行了解下。还有这次的演示例子也只是拿官方的例子来做演示，后面需要自己写程序实现map-reduce,官网上也有很多的例子，所以我觉得看官方其实是最快了解一门技术的方法了，而且一些比较著名的开源项目的文档都是写的比较好的，基本上你看，然后照着demo敲一遍就可以上手了，而且那些资料还是最新的。还有这里也只是演示了伪分布的安装，其实hadoop有三种安装模式：1.独立式:Hadoop运行所有的东西在无后台的单独的JVM中，这种模式适合在开发阶段测试与Debug MapReduce程序。2.伪分布式:Hadoop做为后台应用运行在本地机器，模拟小集群。3.全分布式:Hadoop做为后台应用运行真实的集群电脑中。剩下的就留给读者自己探索吧！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习总结 2017-07]]></title>
    <url>%2F2017%2F07%2F24%2F%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%932017-07%2F</url>
    <content type="text"><![CDATA[前言记录个人在平时的学习和总结，不定期更新 2017-07-23调停者模式是什么调停者模式是对象的行为模式。调停者模式包装了一系列对象相互作用的方式，使得这些对象不必相互明显引用。从而使它们可以较松散地耦合。当这些对象中的某些对象之间的相互作用发生改变时，不会立即影响到其他的一些对象之间的相互作用。从而保证这些相互作用可以彼此独立地变化。这个示意图中有大量的对象，这些对象既会影响别的对象，又会被别的对象所影响，因此常常叫做同事(Colleague)对象。这些同事对象通过彼此的相互作用形成系统的行为。从图中可以看出，几乎每一个对象都需要与其他的对象发生相互作用，而这种相互作用表现为一个对象与另一个对象的直接耦合。这就是过度耦合的系统。如下图所示：通过引入调停者对象(Mediator)，可以将系统的网状结构变成以中介者为中心的星形结构，如下图所示。在这个星形结构中，同事对象不再通过直接的联系与另一个对象发生相互作用；相反的，它通过调停者对象与另一个对象发生相互作用。调停者对象的存在保证了对象结构上的稳定，也就是说，系统的结构不会因为新对象的引入造成大量的修改工作。 举例网络拓扑图的星型总线图星型拓扑结构是用一个节点作为中心节点和其他节点直接与中心节点相连构成的网络。中心节点可以是文件服务器，也可以是连接设备。常见的中心节点为集线器。计算机中的主板主板作为电脑里面各个配件之间的交互的桥梁，电脑各个配件的交互主要是通过主板来完成的，每个部件不需要知道其他部件的接口形式，只需要知道主板的接口形式即可，屏蔽了很多交互细节 总结设计模式是很多人在编程的道路上发现了问题，然后通过思考和实践将对这些问题的解决方式抽象出来，这就是形成了设计模式。我们发现设计模式在很多地方都可以看到影子，这是因为设计模式是一种思想，一种高度抽取用来解决问题的问题的思想，也可以用来解决生活中很多的问题。 2017-07-24select,poll,epoll共同点都是解决IO多路复用的问题，好处就在于单个process就可以同时处理多个网络连接的IO。基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 不同点 select和poll的本质是一样的，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket，不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。 epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll之会把哪个流发生了怎样的I/O事件通知我们。此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)） spark是什么Apache Spark 是个通用的集群计算框架，通过将大量数据集计算任务分配到多台计算机上，提供高效内存计算。Spark 正如其名，最大的特点就是快（Lightning-fast），可比 Hadoop MapReduce 的处理速度快 100 倍。如果你熟悉 Hadoop，那么你知道分布式计算框架要解决两个问题：如何分发数据和如何分发计算。Hadoop 使用 HDFS 来解决分布式数据问题，MapReduce 计算范式提供有效的分布式计算。类似的，Spark 拥有多种语言的函数式编程 API，提供了除 map 和 reduce 之外更多的运算符，这些操作是通过一个称作弹性分布式数据集(resilient distributed datasets, RDDs)的分布式数据框架进行的。 Spark核心组件 Spark Core：包含 Spark 的基本功能；尤其是定义 RDD 的 API、操作以及这两者上的动作。其他 Spark 的库都是构建在 RDD 和 Spark Core 之上的。 Spark SQL：提供通过 Apache Hive 的 SQL 变体 Hive 查询语言（HiveQL）与 Spark 进行交互的 API。每个数据库表被当做一个 RDD，Spark SQL 查询被转换为 Spark 操作。对熟悉 Hive 和 HiveQL 的人，Spar k可以拿来就用。 Spark Streaming：允许对实时数据流进行处理和控制。很多实时数据库（如Apache Store）可以处理实时数据。Spark Streaming 允许程序能够像普通 RDD 一样处理实时数据。 MLlib：一个常用机器学习算法库，算法被实现为对 RDD 的 Spark 操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作。之前可选的大数据机器学习库 Mahout，将会转到 Spark，并在未来实现。 GraphX：控制图、并行图操作和计算的一组算法和工具的集合。GraphX 扩展了 RDD API，包含控制图、创建子图、访问路径上所有顶点的操作。 由于这些组件满足了很多大数据需求，也满足了很多数据科学任务的算法和计算上的需要，Spark 快速流行起来。不仅如此，Spark 也提供了使用 Scala、Java 和Python 编写的 API；满足了不同团体的需求，允许更多数据科学家简便地采用 Spark 作为他们的大数据解决方案。 Spark 体系架构Spark体系架构包括如下三个主要组件： 数据存储 API 管理框架 数据存储：Spark 用 HDFS 文件系统存储数据。它可用于存储任何兼容于 Hadoop 的数据源，包括HDFS，Hbase，Cassandra等。 API：利用 API，应用开发者可以用标准的 API 接口创建基于 Spark 的应用。Spark 提供 Scala，Java 和 Python 三种程序设计语言的 API。Spark基本概念： Application： 用户自己写的 Spark 应用程序，批处理作业的集合。Application 的 main 方法为应用程序的入口，用户通过 Spark 的 API，定义了 RDD 和对 RDD 的操作。 SparkContext： Spark 最重要的 API，用户逻辑与 Spark 集群主要的交互接口，它会和 Cluster Master 交互，包括向它申请计算资源等。 Driver 和 Executor：Spark 在执行每个 Application 的过程中会启动 Driver 和 Executor 两种 JVM 进程。Driver 进程为主控进程，负责执行用户 Application 中的 main 方法，提交 Job，并将 Job 转化为 Task，在各个 Executor 进程间协调 Task 的调度。运行在Worker上 的 Executor 进程负责执行 Task，并将结果返回给 Driver，同时为需要缓存的 RDD 提供存储功能。资源管理： 一组计算机的集合，每个计算机节点作为独立的计算资源，又可以虚拟出多个具备计算能力的虚拟机，这些虚拟机是集群中的计算单元。Spark 的核心模块专注于调度和管理虚拟机之上分布式计算任务的执行，集群中的计算资源则交给 Cluster Manager 这个角色来管理，Cluster Manager 可以为自带的Standalone、或第三方的 Yarn和 Mesos。 Cluster Manager 一般采用 Master-Slave 结构。以 Yarn 为例，部署 ResourceManager 服务的节点为 Master，负责集群中所有计算资源的统一管理和分配；部署 NodeManager 服务的节点为Slave，负责在当前节点创建一个或多个具备独立计算能力的 JVM 实例，在 Spark 中，这些节点也叫做 Worker。 另外还有一个 Client 节点的概念，是指用户提交Spark Application 时所在的节点。 弹性分布式数据集(RDD)： 弹性分布式数据集(RDD)是 Spark 框架中的核心概念。可以将 RDD 视作数据库中的一张表。其中可以保存任何类型的数据。Spark 将数据存储在不同分区上的 RDD 之中。 RDD 可以帮助重新安排计算并优化数据处理过程。 此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。 RDD 是不可变的。你可以用变换（Transformation）修改 RDD，但是这个变换所返回的是一个全新的RDD，而原有的 RDD 仍然保持不变。 RDD 支持两种类型的操作： 变换（Transformation） 变换的返回值是一个新的 RDD 集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个 RDD 作为参数，然后返回一个新的 RDD。 变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce。 行动（Action） 行动操作计算并返回一个新的值。当在一个 RDD 对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。 行动操作包括：reduce，collect，count，first，take，countByKey 以及 foreach。 Java客户端在本地跑数据配置1234SparkConf sparkConf = new SparkConf() .setAppName("spark01") .setMaster("local[4]");sc = new JavaSparkContext(sparkConf); local代表的是在本地跑，[4]指的是创建4个节点 总结跟着官网首先在虚拟机上安装了spark，然后使用scala连接到spark-shell，试着写了几个demo,发现hadoop的map,reduce操作也在里面，还有Java8中对stream中的操作，基本上思想是一样的，map变换，延迟执行，reduce汇聚。]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java多线程爬虫爬取京东商品信息]]></title>
    <url>%2F2017%2F07%2F15%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E4%BA%AC%E4%B8%9C%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言网络爬虫，是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。爬虫可以通过模拟浏览器访问网页，从而获取数据，一般网页里会有很多个URL,爬虫可以访问这些URL到达其他网页，相当于形成了一种数据结构——图，我们通过广度优先搜索和深度优先搜索的方式来遍历这个图，从而做到不断爬取数据的目的。最近准备做一个电商网站，商品的原型就打算从一些电商网站上爬取，这里使用了HttpClient和Jsoup实现了一个简答的爬取商品的demo,采用了多线程的方式，并将爬取的数据持久化到了数据库。 项目环境搭建整体使用技术我IDE使用了Spring Tool Suite(sts)，你也可以使用Eclipse或者是IDEA，安利使用IDEA，真的好用，谁用谁知道。整个项目使用Maven进行构建吗，使用Springboot进行自动装配，使用HttpClient对网页进行抓取，Jsoup对网页进行解析，数据库连接池使用Druild，还使用了工具类Guava和Commons.lang3。 项目结构在sts里面新建一个maven工程，创建如下的包 common 一些通用工具类 constant 系统常量 dao 数据库访问层 service 服务层 handler 调度控制层 entity 实体层这样分层的意义是使得项目结构层次清晰，每层都有着其对应的职责，便于扩展和维护 pom文件这里使用maven进行构建，还没有了解maven的童鞋自行去了解，使用maven的好处是不用自己导入jar包和完整的生命周期控制，注意，使用阿里云的镜像速度回加快很多。项目的pom.xml文件如下pom.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.exmaple&lt;/groupId&gt; &lt;artifactId&gt;spider-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spider-demo&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;jsoup.version&gt;1.10.3&lt;/jsoup.version&gt; &lt;guava.version&gt;22.0&lt;/guava.version&gt; &lt;lang3.version&gt;3.6&lt;/lang3.version&gt; &lt;mysql.version&gt;5.1.42&lt;/mysql.version&gt; &lt;druid.version&gt;1.1.0&lt;/druid.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.4.RELEASE&lt;/version&gt; &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- httpclient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- jsoup --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;/groupId&gt; &lt;artifactId&gt;jsoup&lt;/artifactId&gt; &lt;version&gt;$&#123;jsoup.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- guava --&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;$&#123;guava.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- commons-lang3 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;$&#123;lang3.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.34&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;spider-demo&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; application.yml文件spring boot的配置文件有两种形式，放在src/main/resources目录下，分别是application.yml和application.properties这里为了配置更加简洁，使用了application.yml作为我们的配置文件application.yml12345678# mysqlspring: datasource: type: com.alibaba.druid.pool.DruidDataSource driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/spider?useUnicode=true&amp;characterEncoding=UTF-8&amp;&amp;useSSL=true username: root password: 123 这里可以在url，username和pssword里换成自己环境对应的配置 sql文件这里我们创建了一个数据库和一张表，以便后面将商品信息持久化到数据库db.sql123456789USE spider;CREATE TABLE `goods_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT COMMENT 'ID', `goods_id` VARCHAR(255) NOT NULL COMMENT '商品ID', `goods_name` VARCHAR(255) NOT NULL COMMENT '商品名称', `img_url` VARCHAR(255) NOT NULL COMMENT '商品图片地址', `goods_price` VARCHAR(255) NOT NULL COMMENT '商品标价', PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT='商品信息表'; 网页的分析网址URL的分析我们要爬取的网页的URL的基本地址是https://search.jd.com/Search我们打开这个网页，在搜索框内搜索零食，我们看一下我们的浏览器的地址栏的URL的变化，发现浏览器的地址栏变成了https://search.jd.com/Search?keyword=零食&amp;enc=utf-8&amp;wq=零食&amp;pvid=2c636c9dc26c4e6e88e0dea0357b81a3我们就可以对参数进行分析，keyword和wq应该是代表要搜索的关键字，enc代表的编码，·pvid不知道是什么，我们吧这个参数去掉看能不能访问https://search.jd.com/Search?keyword=零食&amp;enc=utf-8&amp;wq=零食，发现这个URL也是可以正常访问到这个网址的，那么我们就可以暂时忽略这个参数，参数就设置就设置keyword,wq和enc这里我们要设置的参数就是 keyword 零食 wq 零食 enc utf-8 网页内容的分析我们打开我们要爬取数据的页面使用浏览器-检查元素通过查看源码，我们发现JD的商品列表放在id是J_goodsList的div下的的class是gl-warp clearfix的ul标签的gl-item的li标签下再分别审查各个元素，我们发现 li标签的data-sku的属性值就是商品的ID li标签下的class为p-name p-name-type-2的em的值就是商品的名称 li标签下的class为p-price的strong标签下的i标签的值是商品的价格 li标签下的class为p-img的img标签的src值就是商品的图片URL 对网页进行了分析以后，我们就可以通过对DOM结点的选择来筛选我们想要的数据了 代码的编写这里我们封装了HttpClientUtils作为我们的工具类，以便以后使用 HttpClientUtils工具类HttpClient.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212package com.exmaple.spider.common;import java.io.IOException;import java.io.UnsupportedEncodingException;import java.util.ArrayList;import java.util.List;import java.util.Map;import java.util.Map.Entry;import org.apache.http.HttpEntity;import org.apache.http.NameValuePair;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.client.methods.HttpPost;import org.apache.http.entity.ContentType;import org.apache.http.entity.StringEntity;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.message.BasicNameValuePair;import org.apache.http.util.EntityUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import com.exmaple.spider.constant.SysConstant;/** * HttpClient工具类 * * @author ZGJ * @date 2017年7月14日 */public class HttpClientUtils &#123; private final static Logger logger = LoggerFactory.getLogger(HttpClientUtils.class); private final static String GET_METHOD = "GET"; private final static String POST_METHOD = "POST"; /** * GET请求 * * @param url * 请求url * @param headers * 头部 * @param params * 参数 * @return */ public static String sendGet(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) &#123; // 创建HttpClient对象 CloseableHttpClient client = HttpClients.createDefault(); StringBuilder reqUrl = new StringBuilder(url); String result = ""; /* * 设置param参数 */ if (params != null &amp;&amp; params.size() &gt; 0) &#123; reqUrl.append("?"); for (Entry&lt;String, String&gt; param : params.entrySet()) &#123; reqUrl.append(param.getKey() + "=" + param.getValue() + "&amp;"); &#125; url = reqUrl.subSequence(0, reqUrl.length() - 1).toString(); &#125; logger.debug("[url:" + url + ",method:" + GET_METHOD + "]"); HttpGet httpGet = new HttpGet(url); /** * 设置头部 */ logger.debug("Header\n"); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; for (Entry&lt;String, String&gt; header : headers.entrySet()) &#123; httpGet.addHeader(header.getKey(), header.getValue()); logger.debug(header.getKey() + " : " + header.getValue()); &#125; &#125; CloseableHttpResponse response = null; try &#123; response = client.execute(httpGet); /** * 请求成功 */ if (response.getStatusLine().getStatusCode() == 200) &#123; HttpEntity entity = response.getEntity(); result = EntityUtils.toString(entity, SysConstant.DEFAULT_CHARSET); &#125; &#125; catch (IOException e) &#123; logger.error("网络请求出错，请检查原因"); &#125; finally &#123; // 关闭资源 try &#123; if (response != null) &#123; response.close(); &#125; client.close(); &#125; catch (IOException e) &#123; logger.error("网络关闭错误错，请检查原因"); &#125; &#125; return result; &#125; /** * POST请求 * * @param url * 请求url * @param headers * 头部 * @param params * 参数 * @return */ public static String sendPost(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) &#123; CloseableHttpClient client = HttpClients.createDefault(); String result = ""; HttpPost httpPost = new HttpPost(url); /** * 设置参数 */ if (params != null &amp;&amp; params.size() &gt; 0) &#123; List&lt;NameValuePair&gt; paramList = new ArrayList&lt;&gt;(); for (Entry&lt;String, String&gt; param : params.entrySet()) &#123; paramList.add(new BasicNameValuePair(param.getKey(), param.getValue())); &#125; logger.debug("[url: " + url + ",method: " + POST_METHOD + "]"); // 模拟表单提交 try &#123; UrlEncodedFormEntity entity = new UrlEncodedFormEntity(paramList, SysConstant.DEFAULT_CHARSET); httpPost.setEntity(entity); &#125; catch (UnsupportedEncodingException e) &#123; logger.error("不支持的编码"); &#125; /** * 设置头部 */ if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; logger.debug("Header\n"); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; for (Entry&lt;String, String&gt; header : headers.entrySet()) &#123; httpPost.addHeader(header.getKey(), header.getValue()); logger.debug(header.getKey() + " : " + header.getValue()); &#125; &#125; &#125; CloseableHttpResponse response = null; try &#123; response = client.execute(httpPost); HttpEntity entity = response.getEntity(); result = EntityUtils.toString(entity, SysConstant.DEFAULT_CHARSET); &#125; catch (IOException e) &#123; logger.error("网络请求出错，请检查原因"); &#125; finally &#123; try &#123; if (response != null) &#123; response.close(); &#125; client.close(); &#125; catch (IOException e) &#123; logger.error("网络关闭错误"); &#125; &#125; &#125; return result; &#125; /** * post请求发送json * @param url * @param json * @param headers * @return */ public static String senPostJson(String url, String json, Map&lt;String, String&gt; headers) &#123; CloseableHttpClient client = HttpClients.createDefault(); String result = ""; HttpPost httpPost = new HttpPost(url); StringEntity stringEntity = new StringEntity(json, ContentType.APPLICATION_JSON); httpPost.setEntity(stringEntity); logger.debug("[url: " + url + ",method: " + POST_METHOD + ", json: " + json + "]"); /** * 设置头部 */ if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; logger.debug("Header\n"); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; for (Entry&lt;String, String&gt; header : headers.entrySet()) &#123; httpPost.addHeader(header.getKey(), header.getValue()); logger.debug(header.getKey() + " : " + header.getValue()); &#125; &#125; &#125; CloseableHttpResponse response = null; try &#123; response = client.execute(httpPost); HttpEntity entity = response.getEntity(); result = EntityUtils.toString(entity, SysConstant.DEFAULT_CHARSET); &#125; catch (IOException e) &#123; logger.error("网络请求出错，请检查原因"); &#125; finally &#123; try &#123; if (response != null) &#123; response.close(); &#125; client.close(); &#125; catch (IOException e) &#123; logger.error("网络关闭错误"); &#125; &#125; return result; &#125;&#125; SyConstant.java 系统常量SysConstant.java1234567891011121314151617181920212223242526272829303132package com.exmaple.spider.constant;/** * 系统全局常量 * @author ZGJ * @date 2017年7月15日 */public interface SysConstant &#123; /** * 系统默认字符集 */ String DEFAULT_CHARSET = "utf-8"; /** * 需要爬取的网站 */ String BASE_URL = "https://search.jd.com/Search"; interface Header &#123; String ACCEPT = "Accept"; String ACCEPT_ENCODING = "Accept-Encoding"; String ACCEPT_LANGUAGE = "Accept-Language"; String CACHE_CONTROL = "Cache-Controle"; String COOKIE = "Cookie"; String HOST = "Host"; String PROXY_CONNECTION = "Proxy-Connection"; String REFERER = "Referer"; String USER_AGENT = "User-Agent"; &#125; /** * 默认日期格式 */ String DEFAULT_DATE_FORMAT = "yyy-MM-dd HH:mm:ss";&#125; GoodsInfo 商品信息GoodsInfo.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.exmaple.spider.entity;public class GoodsInfo &#123; private Integer id; private String goodsId; private String goodsName; private String imgUrl; private String goodsPrice; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getGoodsId() &#123; return goodsId; &#125; public void setGoodsId(String goodsId) &#123; this.goodsId = goodsId; &#125; public String getGoodsName() &#123; return goodsName; &#125; public void setGoodsName(String goodsName) &#123; this.goodsName = goodsName; &#125; public String getImgUrl() &#123; return imgUrl; &#125; public void setImgUrl(String imgUrl) &#123; this.imgUrl = imgUrl; &#125; public String getGoodsPrice() &#123; return goodsPrice; &#125; public void setGoodsPrice(String goodsPrice) &#123; this.goodsPrice = goodsPrice; &#125; public GoodsInfo(String goodsId, String goodsName, String imgUrl, String goodsPrice) &#123; super(); this.goodsId = goodsId; this.goodsName = goodsName; this.imgUrl = imgUrl; this.goodsPrice = goodsPrice; &#125; &#125; GoodsInfoDao 商品信息Dao层因为这里仅仅涉及到把商品信息写入到数据库比较简单的操作，并没有使用MyBatis或者Hibernate框架，只是使用了Spring的JdbcTemplate对数据进行插入操作GoodsInfoDao.java123456789101112131415161718package com.exmaple.spider.dao;import java.util.List;import com.exmaple.spider.entity.GoodsInfo;/** * 商品Dao层 * @author ZGJ * @date 2017年7月15日 */public interface GoodsInfoDao &#123; /** * 插入商品信息 * @param infos */ void saveBatch(List&lt;GoodsInfo&gt; infos);&#125; GoodsInfoDaoImpl.java1234567891011121314151617181920212223242526package com.exmaple.spider.dao.impl;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;import com.exmaple.spider.dao.GoodsInfoDao;import com.exmaple.spider.entity.GoodsInfo;@Repositorypublic class GoodsInfoDaoImpl implements GoodsInfoDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public void saveBatch(List&lt;GoodsInfo&gt; infos) &#123; String sql = "REPLACE INTO goods_info(" + "goods_id," + "goods_name," + "goods_price," + "img_url) " + "VALUES(?,?,?,?)"; for(GoodsInfo info : infos) &#123; jdbcTemplate.update(sql, info.getGoodsId(), info.getGoodsName(), info.getGoodsPrice(), info.getImgUrl()); &#125; &#125;&#125; 商品的Dao层实现了向数据库里插入商品信息，使用JdbcTemplate和占位符的方式设置sql语句 SpiderService 爬虫服务层SpiderService.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.exmaple.spider.service;import java.util.HashMap;import java.util.List;import java.util.Map;import org.apache.commons.lang3.StringUtils;import org.jsoup.Jsoup;import org.jsoup.nodes.Document;import org.jsoup.nodes.Element;import org.jsoup.select.Elements;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.alibaba.fastjson.JSON;import com.exmaple.spider.common.HttpClientUtils;import com.exmaple.spider.constant.SysConstant;import com.exmaple.spider.dao.GoodsInfoDao;import com.exmaple.spider.entity.GoodsInfo;import com.google.common.collect.Lists;@Servicepublic class SpiderService &#123; private static Logger logger = LoggerFactory.getLogger(SpiderService.class); @Autowired private GoodsInfoDao goodsInfoDao; private static String HTTPS_PROTOCOL = "https:"; public void spiderData(String url, Map&lt;String, String&gt; params) &#123; String html = HttpClientUtils.sendGet(url, null, params); if(!StringUtils.isBlank(html)) &#123; List&lt;GoodsInfo&gt; goodsInfos =parseHtml(html); goodsInfoDao.saveBatch(goodsInfos); &#125; &#125; /** * 解析html * @param html */ private List&lt;GoodsInfo&gt; parseHtml(String html) &#123; //商品集合 List&lt;GoodsInfo&gt; goods = Lists.newArrayList(); /** * 获取dom并解析 */ Document document = Jsoup.parse(html); Elements elements = document. select("ul[class=gl-warp clearfix]").select("li[class=gl-item]"); int index = 0; for(Element element : elements) &#123; String goodsId = element.attr("data-sku"); String goodsName = element.select("div[class=p-name p-name-type-2]").select("em").text(); String goodsPrice = element.select("div[class=p-price]").select("strong").select("i").text(); String imgUrl = HTTPS_PROTOCOL + element.select("div[class=p-img]").select("a").select("img").attr("src"); GoodsInfo goodsInfo = new GoodsInfo(goodsId, goodsName, imgUrl, goodsPrice); goods.add(goodsInfo); String jsonStr = JSON.toJSONString(goodsInfo); logger.info("成功爬取【" + goodsName + "】的基本信息 "); logger.info(jsonStr); if(index ++ == 9) &#123; break; &#125; &#125; return goods; &#125;&#125; Service层通过使用HttpClientUtils模拟浏览器访问页面，然后再使用Jsoup对页面进行解析，Jsoup的使用和Jquery的DOM结点选取基本相似，可以看作是java版的Jquery，如果写过Jquery的人基本上就可以看出是什么意思。每抓取一条信息就会打印一次记录，而且使用fastjson将对象转换成json字符串并输出在写测试代码的时候发现，发现爬取的数据只有前10条是完整的，后面的爬取的有些是不完整的，按道理来说是对于整个页面都是通用的，就是不知道为什么只有前面才是完整的，排查了很久没用发现原因，这里就只选择了前面的10条作为要爬取的数据我们了解到，我们要爬取数据前要分析我们要爬取的数据有哪些，再分析网友的结构，然后对网页进行解析，选取对应的DOM或者使用正则表达式筛选，思路首先要清晰，有了思路之后剩下的也只是把你的思路翻译成代码而已了。 SpiderHandler 爬虫调度处理器SpiderHandler.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.exmaple.spider.handler;import java.util.Date;import java.util.Map;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import org.apache.commons.lang3.time.FastDateFormat;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import com.exmaple.spider.constant.SysConstant;import com.exmaple.spider.service.SpiderService;import com.google.common.collect.Maps;/** * 爬虫调度处理器 * @author ZGJ * @date 2017年7月15日 */@Componentpublic class SpiderHandler &#123; @Autowired private SpiderService spiderService; private static final Logger logger = LoggerFactory.getLogger(SpiderHandler.class); public void spiderData() &#123; logger.info("爬虫开始...."); Date startDate = new Date(); // 使用现线程池提交任务 ExecutorService executorService = Executors.newFixedThreadPool(5); //引入countDownLatch进行线程同步，使主线程等待线程池的所有任务结束，便于计时 CountDownLatch countDownLatch = new CountDownLatch(100); for(int i = 1; i &lt; 201; i += 2) &#123; Map&lt;String, String&gt; params = Maps.newHashMap(); params.put("keyword", "零食"); params.put("enc", "utf-8"); params.put("wc", "零食"); params.put("page", i + ""); executorService.submit(() -&gt; &#123; spiderService.spiderData(SysConstant.BASE_URL, params); countDownLatch.countDown(); &#125;); &#125; try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; executorService.shutdown(); Date endDate = new Date(); FastDateFormat fdf = FastDateFormat.getInstance(SysConstant.DEFAULT_DATE_FORMAT); logger.info("爬虫结束...."); logger.info("[开始时间:" + fdf.format(startDate) + ",结束时间:" + fdf.format(endDate) + ",耗时:" + (endDate.getTime() - startDate.getTime()) + "ms]"); &#125;&#125; SpiderHandelr作为一个爬虫服务调度处理器，这里采用了ExecutorService线程池创建了5个线程进行多线程爬取，我们通过翻页发现，翻页过后地址URL多了一个page参数，而且这个参数还只能是奇数才有效，也就是page为1,3，5,7……代表第1,2,3,4……页。这里就只爬了100页，每页10条数据，将page作为不同的参数传给不同的任务。这里我想统计一下整个爬取任务所用的时间，假如不使用同步工具类的话，因为任务是分到线程池中去运行的，而主线程会继续执行下去，主线程和线程池中的线程是独立运行的，主线程会提前结束，所以就无法统计时间。这里我们使用CountDownLatch同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完后再执行。也就是说可以让主线程等待线程池内的线程执行结束再继续执行，里面维护了一个计数器，开始的时候构造计数器的初始数量，每个线程执行结束的时候调用countdown()方法，计数器就减1，调用await()方法，假如计数器不为0就会阻塞，假如计数器为0了就可以继续往下执行1234executorService.submit(() -&gt; &#123; spiderService.spiderData(SysConstant.BASE_URL, params); countDownLatch.countDown();&#125;); 这里使用了Java8中的lambda表达式替代了匿名内部类，详细的可以自行去了解这里还可以根据自己的业务需求做一些代码的调整和优化，比如实现定时任务爬取等等 App.java Spring Boot启动类App.java123456789101112131415161718192021222324package com.exmaple.spider;import javax.annotation.PostConstruct;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import com.exmaple.spider.handler.SpiderHandler;@SpringBootApplicationpublic class App &#123; @Autowired private SpiderHandler spiderHandler; public static void main(String[] args) throws Exception &#123; SpringApplication.run(App.class, args); &#125; @PostConstruct public void task() &#123; spiderHandler.spiderData(); &#125;&#125; 使用@PostConstruct注解会在spring容器实例化bean之前执行这个方法 运行结果我们以Spring Boot App的方式运行App.java文件，得到的结果如下：我们在看一下数据库内的信息发现数据库也有信息了，大功告成 总结写一个简单的爬虫其实也不难，但是其中也有不少的知识点需要梳理和记忆，发现问题或者是错误，查google，查文档，一点点debug去调试，最终把问题一点点的解决，编程其实需要是解决问题的能力，这种的能力的锻炼需要我们去多写代码，写完了代码之后还要多思考，思考为什么要这样写？还有没有更好的实现方式？为什么会出问题？需要怎么解决？这才是一名优秀的程序员应该养成的习惯，共勉！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis集群的原理的搭建]]></title>
    <url>%2F2017%2F07%2F08%2FRedis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[前言Redis 是我们目前大规模使用的缓存中间件，由于它强大高效而又便捷的功能，得到了广泛的使用。单节点的Redis已经就达到了很高的性能，为了提高可用性我们可以使用Redis集群。本文参考了Rdis的官方文档和使用Redis官方提供的Redis Cluster工具搭建Rdis集群。 注意 ：Redis的版本要在3.0以上,截止今天，Redis的版本是3.2.9，本教程也使用3.2.9作为教程 Redis集群的概念介绍Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施（installation）。 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的错误。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis 集群提供了以下两个好处： 将数据自动切分（split）到多个节点的能力。 当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。 数据分片Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个哈希槽， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。 节点 B 负责处理 5501 号至 11000 号哈希槽。 节点 C 负责处理 11001 号至 16384 号哈希槽。 这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说：我现在想设置一个key,叫my_name: set my_name zhangguoji 按照Redis Cluster的哈希槽算法，CRC16(&#39;my_name&#39;)%16384 = 2412 那么这个key就被分配到了节点A上同样的，当我连接(A,B,C)的任意一个节点想获取my_name这个key,都会转到节点A上再比如如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。增加一个D节点的结果可能如下： 节点A覆盖1365-5460 节点B覆盖6827-10922 节点C覆盖12288-16383 节点D覆盖0-1364,5461-6826,10923-1228 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。所以,Redis Cluster的模型大概是这样的形状 主从复制模型为了使得集群在一部分节点下线或者无法与集群的大多数（majority）节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： 集群中的每个节点都有 1 个至 N 个复制品（replica）， 其中一个复制品为主节点（master）， 而其余的 N-1 个复制品为从节点（slave）。 在之前列举的节点 A 、B 、C 的例子中， 如果节点 B 下线了， 那么集群将无法正常运行， 因为集群找不到节点来处理 5501 号至 11000号的哈希槽。 另一方面， 假如在创建集群的时候（或者至少在节点 B 下线之前）， 我们为主节点 B 添加了从节点 B1 ， 那么当主节点 B 下线的时候， 集群就会将 B1 设置为新的主节点， 并让它代替下线的主节点 B ， 继续处理 5501 号至 11000 号的哈希槽， 这样集群就不会因为主节点 B 的下线而无法正常运作了。 不过如果节点 B 和 B1 都下线的话， Redis 集群还是会停止运作。 Redis一致性保证Redis 并不能保证数据的强一致性. 这意味这在实际中集群在特定的条件下可能会丢失写操作：第一个原因是因为集群是用了异步复制. 写操作过程: 客户端向主节点B写入一条命令. 主节点B向客户端回复命令状态. 主节点将写操作复制给他得从节点 B1, B2 和 B3 主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 注意：Redis 集群可能会在将来提供同步写的方法。 Redis 集群另外一种可能会丢失命令的情况是集群出现了网络分区， 并且一个客户端与至少包括一个主节点在内的少数实例被孤立。举个例子 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， A1 、B1 、C1 为A，B，C的从节点， 还有一个客户端 Z1 假设集群中发生网络分区，那么集群可能会分为两方，大部分的一方包含节点 A 、C 、A1 、B1 和 C1 ，小部分的一方则包含节点 B 和客户端 Z1 .Z1仍然能够向主节点B中写入, 如果网络分区发生时间较短,那么集群将会继续正常运作,如果分区的时间足够让大部分的一方将B1选举为新的master，那么Z1写入B中得数据便丢失了.注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项 搭建Redis集群要让集群正常工作至少需要3个主节点，在这里我们要创建6个redis节点，其中三个为主节点，三个为从节点，对应的redis节点的ip和端口对应关系如下（为了简单演示都在同一台机器上面）1234567891011127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002127.0.0.1:7003127.0.0.1:7004127.0.0.1:7005 安装和启动Redis下载安装包1wget http://download.redis.io/releases/redis-3.2.9.tar.gz 解压安装123tar zxvf redis-3.2.9.tar.gzcd redis-3.2.9make &amp;&amp; make PREFIX=/usr/local/redis install 这里如果失败的自行yum安装gcc和tcl 12yum install gccyum install tcl 创建目录1234cd /usr/local/redismkdir clustercd clustermkdir 7000 7001 7002 7003 7004 7005 复制和修改配置文件将redis目录下的配置文件复制到对应端口文件夹下,6个文件夹都要复制一份1cp redis-3.2.9/redis.conf /usr/local/redis/cluster/7000 修改配置文件redis.conf，将下面的选项修改123456789101112131415161718# 端口号port 7000# 后台启动daemonize yes# 开启集群cluster-enabled yes#集群节点配置文件cluster-config-file nodes-7000.conf# 集群连接超时时间cluster-node-timeout 5000# 进程pid的文件位置pidfile /var/run/redis-7000.pid# 开启aofappendonly yes# aof文件路径appendfilename &quot;appendonly-7005.aof&quot;# rdb文件路径dbfilename dump-7000.rdb 6个配置文件安装对应的端口分别修改配置文件 创建启动脚本在/usr/local/redis目录下创建一个start.sh1234567#!/bin/bashbin/redis-server cluster/7000/redis.confbin/redis-server cluster/7001/redis.confbin/redis-server cluster/7002/redis.confbin/redis-server cluster/7003/redis.confbin/redis-server cluster/7004/redis.confbin/redis-server cluster/7005/redis.conf 这个时候我们查看一下进程看启动情况1ps -ef | grep redis 进程状态如下：123456root 1731 1 1 18:21 ? 00:00:49 bin/redis-server *:7000 [cluster] root 1733 1 0 18:21 ? 00:00:29 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:08 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:26 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:13 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:08 bin/redis-server *:7005 [cluster] 有6个redis进程在开启，说明我们的redis就启动成功了 开启集群这里我们只是开启了6个redis进程而已，它们都还只是独立的状态，还么有组成集群这里我们使用官方提供的工具redis-trib，不过这个工具是用ruby写的，要先安装ruby的环境1yum install ruby rubygems -y 执行，报错12345[root@centos]# redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005/usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31:in `gem_original_require&apos;: no such file to load -- redis (LoadError) from /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31:in `require&apos; from /usr/local/bin/redis-trib.rb:25[root@centos]# 原来是ruby和redis的连接没安装好安装gem-redis1gem install redis 安装到这里的时候突然卡住很久不动，网上搜了下，这里需要翻墙或者换镜像1gem source -a https://gems.ruby-china.org 这里可以将镜像换成ruby-china的镜像，不过我好像更换失败，最终还是翻墙下载了12345[root@centos]# gem install redisSuccessfully installed redis-3.2.11 gem installedInstalling ri documentation for redis-3.2.1...Installing RDoc documentation for redis-3.2.1... 等下载好后我们就可以使用了12345[root@centos]# gem install redisSuccessfully installed redis-3.2.11 gem installedInstalling ri documentation for redis-3.2.1...Installing RDoc documentation for redis-3.2.1... 将redis-3.2.9的src目录下的trib复制到相应文件夹1cp redis-3.2.9/src/redis-trib.rb /usr/local/redis/bin/redis-trib 创建集群：1redis-trib create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 命令的意义如下： 给定 redis-trib.rb 程序的命令是 create ， 这表示我们希望创建一个新的集群。 选项 –replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。之后跟着的其他参数则是实例的地址列表， 我们希望程序使用这些地址所指示的实例来创建新集群。简单来说，以上的命令的意思就是让redis-trib程序帮我们创建三个主节点和三个从节点的集群接着， redis-trib 会打印出一份预想中的配置给你看， 如果你觉得没问题的话， 就可以输入 yes ， redis-trib 就会将这份配置应用到集群当中： 12345678910111213141516171819202122&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002Adding replica 127.0.0.1:7003 to 127.0.0.1:7000Adding replica 127.0.0.1:7004 to 127.0.0.1:7001Adding replica 127.0.0.1:7005 to 127.0.0.1:7002M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) masterM: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) masterM: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) masterS: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6Can I set the above configuration? (type &apos;yes&apos; to accept): 按下yes，集群就会将配置应用到各个节点，并连接起（join)各个节点，也即是，让各个节点开始通讯123456789101112131415161718192021222324252627&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. Redis集群的使用连接集群这里我们使用reids-cli连接集群，使用时加上-c参数，就可以连接到集群连接7000端口的节点123456[root@centos1 redis]# ./redis-cli -c -p 7000127.0.0.1:7000&gt; set name zgj-&gt; Redirected to slot [5798] located at 127.0.0.1:7001OK127.0.0.1:7001&gt; get name&quot;zgj&quot; 前面的理论知识我们知道了，分配key的时候，它会使用CRC16算法，这里将keyname分配到了7001节点上 1Redirected to slot [5798] located at 127.0.0.1:7001 redis cluster 采用的方式很直接，它直接跳转到7001 节点了，而不是还在自身的7000节点。 好，现在我们连接7003这个从节点进入1234[root@centos1 redis]# ./redis-cli -c -p 7003127.0.0.1:7003&gt; get name-&gt; Redirected to slot [5798] located at 127.0.0.1:7001&quot;zgj&quot; 这里获取name的值，也同样跳转到了7001上我们再测试一下其他数据123456789127.0.0.1:7001&gt; set age 20-&gt; Redirected to slot [741] located at 127.0.0.1:7000OK127.0.0.1:7000&gt; set message helloworld-&gt; Redirected to slot [11537] located at 127.0.0.1:7002OK127.0.0.1:7002&gt; set height 175-&gt; Redirected to slot [8223] located at 127.0.0.1:7001OK 我们发现数据会在7000-7002这3个节点之间来回跳转 测试集群中的节点挂掉上面我们建立了一个集群，3个主节点和3个从节点，7000-7002负责存取数据，7003-7005负责把7000-7005的数据同步到自己的节点上来。我们现在来模拟一下一台matser服务器宕机的情况 12345678910111213141516171819202122232425262728293031[root@centos1 redis]# ps -ef | grep redisroot 1731 1 0 18:21 ? 00:01:02 bin/redis-server *:7000 [cluster] root 1733 1 0 18:21 ? 00:00:43 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:22 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:40 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:27 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:22 bin/redis-server *:7005 [cluster] root 23988 1 0 18:30 ? 00:00:42 ./redis-server *:6379 root 24491 1635 0 21:55 pts/1 00:00:00 grep redis[root@centos1 redis]# kill 1731[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7001&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7001)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots:0-5460 (5461 slots) master 0 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里看得出来，现在已经有了3个节点了，7003被选取成了替代7000成为主节点了。我们再来模拟 7000节点重新启动了的情况，那么它还会自动加入到集群中吗？那么，7000这个节点上充当什么角色呢？ 我们试一下：12345678910111213141516171819202122232425[root@centos1 redis]# bin/redis-server cluster/7000/redis.conf[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7000&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)S: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots: (0 slots) slave replicates d403713ab9db48aeac5b5393b69e1201026ef479S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots:0-5460 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里我们可以看到7000节点变成了7003节点的从节点我们试着将7000和7003两个节点都关掉 123456789101112131415161718192021222324252627282930[root@centos1 redis]# ps -ef | grep redisroot 1733 1 0 18:21 ? 00:00:45 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:24 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:42 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:29 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:24 bin/redis-server *:7005 [cluster] root 23988 1 0 18:30 ? 00:00:43 ./redis-server *:6379 root 24527 1 0 22:04 ? 00:00:00 bin/redis-server *:7000 [cluster] root 24541 1635 0 22:07 pts/1 00:00:00 grep redis[root@centos1 redis] kill 1743[root@centos1 redis] kill 24527[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7001&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7001)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[ERR] Not all 16384 slots are covered by nodes. 这里我们的集群就不能工作了，因为两个节点主节点和从节点都挂掉了，原来7001分配的slot现在无节点接管，需要人工介入重新分配slots。 集群中加入新的主节点这里在cluster目录下再新建一个7006并修改对应的配置文件，然后启动这个这个redis进程然后再使用redis-trib的add node指令加入节点1bin/redis-trib add-node 127.0.0.1:7006 127.0.0.1:7000 这里前面的节点表示要加入的节点，第二个节点表示要加入的集群中的任意一个节点，用来标识这个集群123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@centos1 redis]# bin/redis-trib add-node 127.0.0.1:7006 127.0.0.1:7000&gt;&gt;&gt; Adding node 127.0.0.1:7006 to cluster 127.0.0.1:7000&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 127.0.0.1:7006 to make it join the cluster.[OK] New node added correctly.[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7006&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots: (0 slots) master 0 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里我们可以看到7006节点已经变成了一个主节点，然鹅，等等，好像发现了有什么地方不对12M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots: (0 slots) master 里面0 slots,也就是说节点6没有分配哈希槽，即不能进行数据的存取，拿我加上去干嘛。。原来redis cluster 不是在新加节点的时候帮我们做好了迁移工作，需要我们手动对集群进行重新分片迁移，也是这个命令：1/bin/redis-trib reshard 127.0.0.1:7000 这个命令是用来迁移slot节点的，后面的127.0.0.1:7000是表示哪个集群的，7000-7006都是可以的1234567891011121314151617181920212223242526272829303132333435[root@centos1]# redis-trib.rb reshard 127.0.0.1:7000Connecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7003: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7006)M: efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 slots: (0 slots) master 0 additional replica(s)M: cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 4b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slots: (0 slots) slave replicates cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6cS: 3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slots: (0 slots) slave replicates d2237fdcfbba672de766b913d1186cebcb6e1761M: dfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 30858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slots: (0 slots) slave replicates dfa0754c7854a874a6ebd2613b86140ad97701fcM: d2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 slots:0-5460 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 它提示我们需要迁移多少slot到7006上，我们可以算一下：16384/4 = 4096，也就是说，为了平衡分配起见，我们需要移动4096个槽点到7006上。 好，那输入4096:它又提示我们，接受的node ID是多少，7006的id 我们通过上面就可以看到是efc3131fbdc6cf929720e0e0f7136cae85657481:12345What is the receiving node ID? efc3131fbdc6cf929720e0e0f7136cae85657481Please enter all the source node IDs. Type &apos;all&apos; to use all the nodes as source nodes for the hash slots. Type &apos;done&apos; once you entered all the source nodes IDs.Source node #1: 接着， redis-trib 会向你询问重新分片的源节点（source node）， 也即是， 要从哪个节点中取出 4096 个哈希槽， 并将这些槽移动到7006节点上面。 如果我们不打算从特定的节点上取出指定数量的哈希槽， 那么可以向 redis-trib 输入 all ， 这样的话， 集群中的所有主节点都会成为源节点， redis-trib 将从各个源节点中各取出一部分哈希槽， 凑够 4096 个， 然后移动到7006节点上：1Source node #1:all 接下来就开始迁移了，并且会询问你是否确认：1234567Moving slot 1359 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1360 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1361 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1362 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1363 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1364 from d2237fdcfbba672de766b913d1186cebcb6e1761Do you want to proceed with the proposed reshard plan (yes/no)? 输入yes并回车后，redis-trib就会正式执行重新分片操作，将制定的哈希槽从源节点一个个移动到7006节点上迁移结束之后，我们来检查一下12345678910111213141516171819202122232425M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:1365-5460 (4096 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 0 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:6827-10922 (4096 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:12288-16383 (4096 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 我们可以看到slots:0-1364,5461-6826,10923-12287 (4096 slots)这些原来在其他节点上的哈希槽都迁移到了7006上 增加一个从节点新建一个 7007从节点，作为7006的从节点 我们再新建一个节点7007，步骤类似，就先省略了。建好后，启动起来，我们看如何把它加入到集群中的从节点中：1[root@centos1]# redis-trib.rb add-node --slave 127.0.0.1:7007 127.0.0.1:7000 add-node的时候加上--slave表示是加入到从节点中，但是这样加，是随机的。这里的命令行完全像我们在添加一个新主服务器时使用的一样，所以我们没有指定要给哪个主服 务器添加副本。这种情况下，redis-trib会将7007作为一个具有较少副本的随机的主服务器的副本。 那么，你猜，它会作为谁的从节点，应该是7006，因为7006还没有从节点。我们运行下。12345678910[root@web3 7007]# redis-trib.rb add-node --slave 127.0.0.1:7007 127.0.0.1:7000......[OK] All 16384 slots covered.Automatically selected master 127.0.0.1:7006Connecting to node 127.0.0.1:7007: OK&gt;&gt;&gt; Send CLUSTER MEET to node 127.0.0.1:7007 to make it join the cluster.Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 127.0.0.1:7006.[OK] New node added correctly. 上面提示说，自动选择了7006作为master节点。并且成功了。我们检查下：1234567891011121314151617181920212223242526272829303132333435363738[root@centos1]# redis-trib.rb check 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7002: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)S: 3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slots: (0 slots) slave replicates d2237fdcfbba672de766b913d1186cebcb6e1761M: efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 1 additional replica(s)S: 4b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slots: (0 slots) slave replicates cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6cS: 30858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slots: (0 slots) slave replicates dfa0754c7854a874a6ebd2613b86140ad97701fcM: d2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 slots:1365-5460 (4096 slots) master 1 additional replica(s)M: cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 slots:6827-10922 (4096 slots) master 1 additional replica(s)S: 86d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 slots: (0 slots) slave replicates efc3131fbdc6cf929720e0e0f7136cae85657481M: dfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 slots:12288-16383 (4096 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 果然，7007加入到了7006的从节点当中。 你说，我如果想指定一个主节点行不行？当然可以。我们再建一个7008节点。1bin/redis-trib.rb add-node --slave --master-id efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7008 127.0.0.1:7000 --master-id 表示指定的主节点node id。这里指定的是 7006 这个主节点。123Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 127.0.0.1:7006.[OK] New node added correctly. 提示我们已经作为7006的从节点了，也就是加入到7006的从节点来了，照这么说，7006就有2个从节点了，我们看一下：1234bin/redis-cli -c -p 7008 cluster nodes |grep efc3131fbdc6cf929720e0e0f7136cae8565748186d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 slave efc3131fbdc6cf929720e0e0f7136cae85657481 0 1445089507786 8 connectedefc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 master - 0 1445089508289 8 connected 0-1364 5461-6826 10923-1228744321e7d619410dc4e0a8745366610a0d06d2395 127.0.0.1:7008 myself,slave efc3131fbdc6cf929720e0e0f7136cae85657481 0 0 0 connected 我们过滤了下看结果，果真，7007和7008是7006的从节点了。 刚好，我们再做一个实验，我把7006的进程杀掉，看7007和7008谁会变成主节点：123456789101112131415161718192021222324252627[root@centos1]# ps -ef|grep redisroot 11384 1 0 09:56 ? 00:00:16 redis-server *:7001 [cluster]root 11388 1 0 09:56 ? 00:00:16 redis-server *:7002 [cluster]root 11392 1 0 09:56 ? 00:00:16 redis-server *:7003 [cluster]root 11396 1 0 09:56 ? 00:00:15 redis-server *:7004 [cluster]root 11400 1 0 09:56 ? 00:00:15 redis-server *:7005 [cluster]root 12100 1 0 11:01 ? 00:00:11 redis-server *:7000 [cluster]root 12132 1 0 11:28 ? 00:00:11 redis-server *:7006 [cluster]root 12202 1 0 13:14 ? 00:00:02 redis-server *:7007 [cluster]root 12219 1 0 13:39 ? 00:00:00 redis-server *:7008 [cluster]root 12239 8259 0 13:49 pts/0 00:00:00 grep redis[root@centos1]# kill 12132[root@centos1]# redis-cli -c -p 7008127.0.0.1:7008&gt; get ss5rtr-&gt; Redirected to slot [1188] located at 127.0.0.1:7007&quot;66&quot;127.0.0.1:7007&gt; cluster nodesefc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 master,fail - 1445089780668 1445089779963 8 disconnectedd2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 master - 0 1445089812195 7 connected 1365-546030858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slave dfa0754c7854a874a6ebd2613b86140ad97701fc 0 1445089813710 3 connected86d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 myself,master - 0 0 10 connected 0-1364 5461-6826 10923-12287cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 master - 0 1445089814214 2 connected 6827-109224b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slave cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 0 1445089812701 2 connected44321e7d619410dc4e0a8745366610a0d06d2395 127.0.0.1:7008 slave 86d05e7c2b197dc182b5e71069e791d033cf899e 0 1445089814214 10 connected3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slave d2237fdcfbba672de766b913d1186cebcb6e1761 0 1445089813204 7 connecteddfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 master - 0 1445089813204 3 connected 12288-16383127.0.0.1:7007&gt; 这里7007获得了成为主节点的机会，7008就变成了7007的从节点。 那么这个时候，重启7006节点，那么他就会变成了一个7007的从节点了。 移除一个节点上面是增加一个节点，接下来就是移除一个节点了，移除节点的命令是1bin/redis-trib del-node 127.0.0.1:7000 `&lt;node-id&gt;` 没我们尝试下输入以下命令123456789101112[root@centos]# bin/redis-trib.rb del-node 127.0.0.1:7000 86d05e7c2b197dc182b5e71069e791d033cf899e&gt;&gt;&gt; Removing node 86d05e7c2b197dc182b5e71069e791d033cf899e from cluster 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7002: OK[ERR] Node 127.0.0.1:7007 is not empty! Reshard data away and try again. 这里报错了，提示我们7007节点里面有数据，让我们把7007节点里的数据移除出去，也就是说需要重新分片，这个和上面增加节点的方式一样，我们再来一遍1bin/redis-trib.rb reshard 127.0.0.1:7000 省去中间内容，原来7007节点上已经有了4096个哈希槽，这里我们也移动4096个哈希槽然后将这些哈希槽移动到7001节点上123Source node #1:86d05e7c2b197dc182b5e71069e791d033cf899eSource node #2:doneDo you want to proceed with the proposed reshard plan (yes/no)? yes 然后我们再继续执行移除命令，结果如下123456789101112131415[root@centos1]# redis-trib.rb del-node 127.0.0.1:7000 86d05e7c2b197dc182b5e71069e791d033cf899e&gt;&gt;&gt; Removing node 86d05e7c2b197dc182b5e71069e791d033cf899e from cluster 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7002: OK&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; 127.0.0.1:7006 as replica of 127.0.0.1:7001&gt;&gt;&gt; 127.0.0.1:7008 as replica of 127.0.0.1:7001&gt;&gt;&gt; SHUTDOWN the node. 删除成功，而且还很人性化的将7006和7008这2个原来7007的附属节点送给了7001。考虑的真周到~ 移除一个从节点移除一个从节点就比较简单了，因为从节点没有哈希槽，也不需要考虑数据迁移，直接移除就行1234567891011121314[root@centos1]# redis-trib.rb del-node 127.0.0.1:7005 44321e7d619410dc4e0a8745366610a0d06d2395&gt;&gt;&gt; Removing node 44321e7d619410dc4e0a8745366610a0d06d2395 from cluster 127.0.0.1:7005Connecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7003: OK&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node.[root@centos1]# redis-trib.rb check 127.0.0.1:7008Connecting to node 127.0.0.1:7008: [ERR] Sorry, can&apos;t connect to node 127.0.0.1:7008 表示移除成功 Redis性能测试Redis自带了性能测试工具redis-benchmark使用说明如下：1234567891011121314151617181920212223Usage: redis-benchmark [-h &lt;host&gt;] [-p &lt;port&gt;] [-c &lt;clients&gt;] [-n &lt;requests]&gt; [-k &lt;boolean&gt;] -h &lt;hostname&gt; Server hostname (default 127.0.0.1) -p &lt;port&gt; Server port (default 6379) -s &lt;socket&gt; Server socket (overrides host and port) -c &lt;clients&gt; Number of parallel connections (default 50) -n &lt;requests&gt; Total number of requests (default 10000) -d &lt;size&gt; Data size of SET/GET value in bytes (default 2) -k &lt;boolean&gt; 1=keep alive 0=reconnect (default 1) -r &lt;keyspacelen&gt; Use random keys for SET/GET/INCR, random values for SADD Using this option the benchmark will get/set keys in the form mykey_rand:000000012456 instead of constant keys, the &lt;keyspacelen&gt; argument determines the max number of values for the random number. For instance if set to 10 only rand:000000000000 - rand:000000000009 range will be allowed. -P &lt;numreq&gt; Pipeline &lt;numreq&gt; requests. Default 1 (no pipeline). -q Quiet. Just show query/sec values --csv Output in CSV format -l Loop. Run the tests forever -t &lt;tests&gt; Only run the comma-separated list of tests. The test names are the same as the ones produced as output. -I Idle mode. Just open N idle connections and wait. 基准测试基准的测试命令：redis-benchmark -q -n 100000结果入下：1234567891011121314151617181920root@centos1 bin]# redis-benchmark -q -n 100000-bash: redis-benchmark: command not found[root@centos1 bin]# ./redis-benchmark -q -n 100000PING_INLINE: 61576.36 requests per secondPING_BULK: 60277.28 requests per secondSET: 61349.69 requests per secondGET: 60459.49 requests per secondINCR: 58858.15 requests per secondLPUSH: 59066.75 requests per secondRPUSH: 57339.45 requests per secondLPOP: 55586.44 requests per secondRPOP: 56465.27 requests per secondSADD: 57045.07 requests per secondSPOP: 53734.55 requests per secondLPUSH (needed to benchmark LRANGE): 57012.54 requests per secondLRANGE_100 (first 100 elements): 55803.57 requests per secondLRANGE_300 (first 300 elements): 54914.88 requests per secondLRANGE_500 (first 450 elements): 53333.33 requests per secondLRANGE_600 (first 600 elements): 56529.11 requests per secondMSET (10 keys): 59276.82 requests per second 这里可以看出，单机版的redis每秒可以处理6万个请求，这已经是一个非常高的并发量了我们再来看下集群情况下是是什么情况123456789101112131415161718[root@centos1 bin]# ./redis-benchmark -q -n 100000 -p 7000PING_INLINE: 64599.48 requests per secondPING_BULK: 64184.85 requests per secondSET: 66800.27 requests per secondGET: 65616.80 requests per secondINCR: 66269.05 requests per secondLPUSH: 40273.86 requests per secondRPUSH: 40355.12 requests per secondLPOP: 43421.62 requests per secondRPOP: 45187.53 requests per secondSADD: 62539.09 requests per secondSPOP: 61538.46 requests per secondLPUSH (needed to benchmark LRANGE): 38182.51 requests per secondLRANGE_100 (first 100 elements): 25555.84 requests per secondLRANGE_300 (first 300 elements): 9571.21 requests per secondLRANGE_500 (first 450 elements): 7214.49 requests per secondLRANGE_600 (first 600 elements): 5478.85 requests per secondMSET (10 keys): 41893.59 requests per second 这里看出大部分和单机版的性能查不多，主要是lrange命令的差别是很大的 流水线测试使用流水线默认情况下，每个客户端都是在一个请求完成之后才发送下一个请求（基准会模拟50个客户端除非使用-c指定特别的数量），这意味着服务器几乎是按顺序读取每个客户端的命令。RTT也加入了其中。真实世界会更复杂，Redis支持/topics/pipelining，使得可以一次性执行多条命令成为可能。Redis流水线可以提高服务器的TPSredis-benchmark -n 1000000 -t set,get -P 16 -q 加入-P选项使用管道技术，一次执行多条命令123./redis-benchmark -n 1000000 -t set,get -P 16 -qSET: 515198.34 requests per secondGET: 613873.56 requests per second 每秒处理get/sret请求达到了60/50W,真的厉害！ 遇到的问题 安装redis集群的时候遇到了挺多问题，踩了很多坑，单单是修改配置文件就出了不少问题，那些配置文件的内容都要一一修改，有些配置不修改就会出现无法创建进程的错误 注意配置集群的时候不要加密码，否则会出现无法连接的情况 gem install的时候需要修改镜像或者翻墙 昨天启动成功，今天启动的时候报错1[ERR] Node 172.168.63.202:7001 is not empty. Either the nodealready knows other nodes (check with CLUSTER NODES) or contains some key in database 0 解决方法：1). 将需要新增的节点下aof、rdb等本地备份文件删除；2). 同时将新Node的集群配置文件删除,即：删除你redis.conf里面cluster-config-file所在的文件；3). 再次添加新节点如果还是报错，则登录新Node,执行bin/redis-cli–h x –p对数据库进行清除：1173.172.168.63.201:7001&gt; flushdb #清空当前数据库 总结之间对Redis的了解并不是说非常多，只是简单的会用，因为现在企业里也很多都在用，刚好老大说接下来的项目可能会用到Redis集群，让我先去了解下，所以最近就在回头看，一边看文档，博客，一边实践，踩了很多的坑，出问题的时候的确是让人感到很痛苦很郁闷的，可是当运行成功的那一刻心情却是无比激动和开心的，可能这就是编程的魅力吧。]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产者消费者的五种实现方式]]></title>
    <url>%2F2017%2F06%2F30%2FJava%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BA%94%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言生产者和消费者问题是线程模型中的经典问题：生产者和消费者在同一时间段内共用同一个存储空间，生产者往存储空间中添加产品，消费者从存储空间中取走产品，当存储空间为空时，消费者阻塞，当存储空间满时，生产者阻塞。现在用五种方式来实现生产者消费者模型 wait()和notify()方法的实现这也是最简单最基础的实现，缓冲区满和为空时都调用wait()方法等待，当生产者生产了一个产品或者消费者消费了一个产品之后会唤醒所有线程。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 生产者和消费者，wait()和notify()的实现 * @author ZGJ * @date 2017年6月22日 */public class Test1 &#123; private static Integer count = 0; private static final Integer FULL = 10; private static String LOCK = "lock"; public static void main(String[] args) &#123; Test1 test1 = new Test1(); new Thread(test1.new Producer()).start(); new Thread(test1.new Consumer()).start(); new Thread(test1.new Producer()).start(); new Thread(test1.new Consumer()).start(); new Thread(test1.new Producer()).start(); new Thread(test1.new Consumer()).start(); new Thread(test1.new Producer()).start(); new Thread(test1.new Consumer()).start(); &#125; class Producer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; synchronized (LOCK) &#123; while (count == FULL) &#123; try &#123; LOCK.wait(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; count++; System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count); LOCK.notifyAll(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (LOCK) &#123; while (count == 0) &#123; try &#123; LOCK.wait(); &#125; catch (Exception e) &#123; &#125; &#125; count--; System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count); LOCK.notifyAll(); &#125; &#125; &#125; &#125;&#125; 结果:12345678910111213141516171819202122232425262728293031Thread-0生产者生产，目前总共有1Thread-4生产者生产，目前总共有2Thread-3消费者消费，目前总共有1Thread-1消费者消费，目前总共有0Thread-2生产者生产，目前总共有1Thread-6生产者生产，目前总共有2Thread-7消费者消费，目前总共有1Thread-5消费者消费，目前总共有0Thread-0生产者生产，目前总共有1Thread-4生产者生产，目前总共有2Thread-3消费者消费，目前总共有1Thread-6生产者生产，目前总共有2Thread-1消费者消费，目前总共有1Thread-7消费者消费，目前总共有0Thread-2生产者生产，目前总共有1Thread-5消费者消费，目前总共有0Thread-0生产者生产，目前总共有1Thread-4生产者生产，目前总共有2Thread-3消费者消费，目前总共有1Thread-7消费者消费，目前总共有0Thread-6生产者生产，目前总共有1Thread-2生产者生产，目前总共有2Thread-1消费者消费，目前总共有1Thread-5消费者消费，目前总共有0Thread-0生产者生产，目前总共有1Thread-4生产者生产，目前总共有2Thread-3消费者消费，目前总共有1Thread-1消费者消费，目前总共有0Thread-6生产者生产，目前总共有1Thread-7消费者消费，目前总共有0Thread-2生产者生产，目前总共有1 可重入锁ReentrantLock的实现java.util.concurrent.lock 中的 Lock 框架是锁定的一个抽象，通过对lock的lock()方法和unlock()方法实现了对锁的显示控制，而synchronize()则是对锁的隐性控制。可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响，简单来说，该锁维护这一个与获取锁相关的计数器，如果拥有锁的某个线程再次得到锁，那么获取计数器就加1，函数调用结束计数器就减1，然后锁需要被释放两次才能获得真正释放。已经获取锁的线程进入其他需要相同锁的同步代码块不会被阻塞。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 生产者和消费者，ReentrantLock的实现 * * @author ZGJ * @date 2017年6月22日 */public class Test2 &#123; private static Integer count = 0; private static final Integer FULL = 10; //创建一个锁对象 private Lock lock = new ReentrantLock(); //创建两个条件变量，一个为缓冲区非满，一个为缓冲区非空 private final Condition notFull = lock.newCondition(); private final Condition notEmpty = lock.newCondition(); public static void main(String[] args) &#123; Test2 test2 = new Test2(); new Thread(test2.new Producer()).start(); new Thread(test2.new Consumer()).start(); new Thread(test2.new Producer()).start(); new Thread(test2.new Consumer()).start(); new Thread(test2.new Producer()).start(); new Thread(test2.new Consumer()).start(); new Thread(test2.new Producer()).start(); new Thread(test2.new Consumer()).start(); &#125; class Producer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; //获取锁 lock.lock(); try &#123; while (count == FULL) &#123; try &#123; notFull.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; count++; System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count); //唤醒消费者 notEmpty.signal(); &#125; finally &#123; //释放锁 lock.unlock(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; lock.lock(); try &#123; while (count == 0) &#123; try &#123; notEmpty.await(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; count--; System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count); notFull.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; &#125;&#125; 阻塞队列BlockingQueue的实现BlockingQueue即阻塞队列，从阻塞这个词可以看出，在某些情况下对阻塞队列的访问可能会造成阻塞。被阻塞的情况主要有如下两种: 当队列满了的时候进行入队列操作 当队列空了的时候进行出队列操作 因此，当一个线程对已经满了的阻塞队列进行入队操作时会阻塞，除非有另外一个线程进行了出队操作，当一个线程对一个空的阻塞队列进行出队操作时也会阻塞，除非有另外一个线程进行了入队操作。从上可知，阻塞队列是线程安全的。下面是BlockingQueue接口的一些方法: 操作 抛异常 特定值 阻塞 超时 插入 add(o) offer(o) put(o) offer(o, timeout, timeunit) 移除 remove(o) poll(o) take(o) poll(timeout, timeunit) 检查 element(o) peek(o) 这四类方法分别对应的是：1 . ThrowsException：如果操作不能马上进行，则抛出异常2 . SpecialValue：如果操作不能马上进行，将会返回一个特殊的值，一般是true或者false3 . Blocks:如果操作不能马上进行，操作会被阻塞4 . TimesOut:如果操作不能马上进行，操作会被阻塞指定的时间，如果指定时间没执行，则返回一个特殊值，一般是true或者false下面来看由阻塞队列实现的生产者消费者模型,这里我们使用take()和put()方法，这里生产者和生产者，消费者和消费者之间不存在同步，所以会出现连续生成和连续消费的现象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;/** * 使用BlockingQueue实现生产者消费者模型 * @author ZGJ * @date 2017年6月29日 */public class Test3 &#123; private static Integer count = 0; //创建一个阻塞队列 final BlockingQueue&lt;Integer&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(10); public static void main(String[] args) &#123; Test3 test3 = new Test3(); new Thread(test3.new Producer()).start(); new Thread(test3.new Consumer()).start(); new Thread(test3.new Producer()).start(); new Thread(test3.new Consumer()).start(); new Thread(test3.new Producer()).start(); new Thread(test3.new Consumer()).start(); new Thread(test3.new Producer()).start(); new Thread(test3.new Consumer()).start(); &#125; class Producer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; try &#123; blockingQueue.put(1); count++; System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; try &#123; blockingQueue.take(); count--; System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 信号量Semaphore的实现Semaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源，在操作系统中是一个非常重要的问题，可以用来解决哲学家就餐问题。Java中的Semaphore维护了一个许可集，一开始先设定这个许可集的数量，可以使用acquire()方法获得一个许可，当许可不足时会被阻塞，release()添加一个许可。在下列代码中，还加入了另外一个mutex信号量，维护生产者消费者之间的同步关系，保证生产者和消费者之间的交替进行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.concurrent.Semaphore;/** * 使用semaphore信号量实现 * @author ZGJ * @date 2017年6月29日 */public class Test4 &#123; private static Integer count = 0; //创建三个信号量 final Semaphore notFull = new Semaphore(10); final Semaphore notEmpty = new Semaphore(0); final Semaphore mutex = new Semaphore(1); public static void main(String[] args) &#123; Test4 test4 = new Test4(); new Thread(test4.new Producer()).start(); new Thread(test4.new Consumer()).start(); new Thread(test4.new Producer()).start(); new Thread(test4.new Consumer()).start(); new Thread(test4.new Producer()).start(); new Thread(test4.new Consumer()).start(); new Thread(test4.new Producer()).start(); new Thread(test4.new Consumer()).start(); &#125; class Producer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; notFull.acquire(); mutex.acquire(); count++; System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; mutex.release(); notEmpty.release(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; try &#123; notEmpty.acquire(); mutex.acquire(); count--; System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; mutex.release(); notFull.release(); &#125; &#125; &#125; &#125;&#125; 管道输入输出流PipedInputStream和PipedOutputStream实现在java的io包下，PipedOutputStream和PipedInputStream分别是管道输出流和管道输入流。它们的作用是让多线程可以通过管道进行线程间的通讯。在使用管道通信时，必须将PipedOutputStream和PipedInputStream配套使用。使用方法：先创建一个管道输入流和管道输出流，然后将输入流和输出流进行连接，用生产者线程往管道输出流中写入数据，消费者在管道输入流中读取数据，这样就可以实现了不同线程间的相互通讯，但是这种方式在生产者和生产者、消费者和消费者之间不能保证同步，也就是说在一个生产者和一个消费者的情况下是可以生产者和消费者之间交替运行的，多个生成者和多个消费者者之间则不行12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * 使用管道实现生产者消费者模型 * @author ZGJ * @date 2017年6月30日 */public class Test5 &#123; final PipedInputStream pis = new PipedInputStream(); final PipedOutputStream pos = new PipedOutputStream(); &#123; try &#123; pis.connect(pos); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; class Producer implements Runnable &#123; @Override public void run() &#123; try &#123; while(true) &#123; Thread.sleep(1000); int num = (int) (Math.random() * 255); System.out.println(Thread.currentThread().getName() + "生产者生产了一个数字，该数字为： " + num); pos.write(num); pos.flush(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; pos.close(); pis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; try &#123; while(true) &#123; Thread.sleep(1000); int num = pis.read(); System.out.println("消费者消费了一个数字，该数字为：" + num); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; pos.close(); pis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; Test5 test5 = new Test5(); new Thread(test5.new Producer()).start(); new Thread(test5.new Consumer()).start(); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之——单例模式]]></title>
    <url>%2F2017%2F05%2F19%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E2%80%94%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言 单例模式是一种常用的软件设计模式。在它的核心结构中只包含一个被称为单例的特殊类。通过单例模式可以保证系统中一个类只有一个实例。即一个类只有一个对象实例。我们会理所当然的认为单例模式很简单，其实单例模式是事先方式有很多种，现在就来一一介绍一下。 懒汉式（线程不安全）1234567891011121314public class Singleton &#123; private static Singleton instance; /** * 构造函数私有化 */ private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这种写法呢，有懒加载的作用，但是在多线程环境下会线程不安全为什么会线程不安全呢？我们假设一下，假如线程A和线程B同时调用getInstance()方法，线程A先执行判断if(instance == null)，判断instance对象是空的，这时候线程B获得了CPU执行权，它也判断instance对象是控制，这个时候执行了instance = new Singleton();这段代码，创建了一个对象并把这个对象的引用赋值给了instance，这个时候线程A又获得了执行权，之前已经判断过对象为空了，所以线程A又new了一个新的对象，这个时候就不符合单例模式的要求了，所以这种懒汉式是线程不安全的 懒汉式（线程安全）1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123; &#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这个时候，我们加上了synchronized关键字之后，相当于给这个对象加上了锁，就起到了同步的作用，同时只能有一个线程访问这个方法，不过这样的话也降低了效率 饿汉式12345678public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123; &#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 这种方式基于classloder机制避免了多线程的同步问题，在类初始化的时候就实例化这个对象 静态方法块12345678910public class Singleton &#123; private Singleton instance = null; static &#123; instance = new Singleton(); &#125; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return this.instance; &#125;&#125; 这种方式和上一种区别不大，都是在类初始化即实例化instance对象 静态内部类12345678910public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123; &#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 这种方式同样利用了classloder的类加载机制来保证初始化instance时只有一个线程，它和第三种饿汉式，第四种静态方法块不同的是：第三种和第四种方式是只要Singleton类被装载了，那么instance就会被实例化（没有达到lazy loading效果），而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance 双重检查锁定1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 双重检查锁定在C语言或者其他语言中已被广泛当做多线程环境下延迟初始化的一种高效手段，但是在Java中并不能很好的实现，这个涉及到Java的内存模型，所以需要加上volatile关键字，这个关键字的作用是保证内存可见性和禁止指令重排序 枚举12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125;&#125; 枚举是JDK1.5之后才有的语法糖，实际上我们创建enum时，编译器会自动为我们生成一个继承自Java.lang.Enum的类首先，在枚举中明确了构造方法限制为私有，在我们访问枚举实例时会执行构造方法，同时每个枚举实例都是static final类型的，也就表明只能被实例化一次。在调用构造方法时，我们的单例被实例化。也就是说，因为enum中的实例被保证只会被实例化一次，所以我们的INSTANCE也被保证实例化一次。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat配置]]></title>
    <url>%2F2017%2F05%2F15%2FTomcat%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言我们都知道Tomcat服务器是我们在学习JavaWeb中最常使用的服务器，所以了解Tomcat的配置文件显得很重要，昨天去面试的时候，被面试官问了几个关于Tomcat配置文件的几个问题， 以前配过，但是后来都忘记了，现在来回顾一下Tomcat中比较常用得到的配置文件吧 配置文件的位置Tomcat的配置文件在conf目录下，有context.xml、server.xml、tomcat-users.xml、web.xml这些配置文件 server.xml修改端口如果要修改连接的端口，在server.xml文件的Server标签和Connectot标签下修改如下1&lt;Server port="8005" shutdown="SHUTDOWN"&gt; 1234&lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt;&lt;Connector port="8009" protocol="AJP/1.3" redirectPort="8443" /&gt; 将port=xxxx改成其他端口即可 修改虚拟目录在server.xml中找到Host结点1234567891011121314151617&lt;Host name="localhost" appBase="webapps" unpackWARs="true" autoDeploy="true"&gt; &lt;!-- SingleSignOn valve, share authentication between web applications Documentation at: /docs/config/valve.html --&gt; &lt;!-- &lt;Valve className="org.apache.catalina.authenticator.SingleSignOn" /&gt; --&gt; &lt;!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern="common" --&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log." suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt;&lt;/Host&gt; Host结点代表一个主机，name对应的是其域名，我们可以通过修改name的属性来改变Tomcat资源的访问路径，方便我们管理 tomcat-userstoncat-users.xml是配置用户登录Tomcat对app进行管理的配置文件，如果我们需要登录Tomcat，这需要在改配置文件的tomcat-users结点下加上如下代码123 &lt;role rolename="manager-gui"/&gt;&lt;role rolename="manager-script"/&gt;&lt;user username="tomcat" password="123456" roles="manager-gui,manager-script"/&gt; 其中，我这里配置的登录名是utomcat,密码是123456我们打开Tomcat主页，点击Manager App 输入用户名和密码，进入到管理界面 这个时候我们就可以对我们的app进行管理了]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Socket 编程那些事（1）]]></title>
    <url>%2F2017%2F05%2F02%2FJava%20Socket%20%E7%BC%96%E7%A8%8B%E9%82%A3%E4%BA%9B%E4%BA%8B(1)%2F</url>
    <content type="text"><![CDATA[前言 最近在准备面试和笔试的一些东西，回去翻看了Java关于IO的基础，发现很多基础还是没有记牢固，现在回头重新学习，就从socket通讯开始吧，虽然说现在企业很少直接编写socket，都是使用一些封装好的框架，netty,mina等。不过对于这些基础的知识，还是需要掌握牢固的，对于以后学习更深的框架和笔试面试都很有裨益。 BIO、NIO、AIO的区别Java中的IO的方式有三种Java BIO ： 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。在JDK1.4以前采用这种方式Java NIO ： 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。Java AIO(NIO.2) ： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理， 曾经在一篇文章中看到这么很好的比喻 如果你想吃一份宫保鸡丁盖饭： 同步阻塞：你到饭馆点餐，然后在那等着，还要一边喊：好了没啊！ 同步非阻塞：在饭馆点完餐，就去遛狗了。不过溜一会儿，就回饭馆喊一声：好了没啊！ 异步阻塞：遛狗的时候，接到饭馆电话，说饭做好了，让您亲自去拿。 异步非阻塞：饭馆打电话说，我们知道您的位置，一会给你送过来，安心遛狗就可以了。 这里我们从最简单的BIO开始学习吧 单线程通讯首先我们先新建一个Server类，这个类将监听12345这个端口，等待客户端的连接,客户端与服务器连接后，客户端可以像服务器发送信息，如果发送bye，则结束通讯1234567891011121314151617181920212223242526272829303132333435363738package edu.gduf.bio.socket.demo1;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.PrintWriter;import java.net.ServerSocket;import java.net.Socket;/** * 基于bio的socket服务器端 * * @author ZGJ * @date 2017年5月4日 */public class Server &#123; public static void main(String[] args) &#123; //创建一个serversocket在10000号端口监听 try (ServerSocket server = new ServerSocket(12345)) &#123; //等待客户端连接，将一直阻塞直到有客户端连接 Socket socket = server.accept(); BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); PrintWriter out = new PrintWriter(socket.getOutputStream()); while (true) &#123; String msg = in.readLine(); System.out.println(msg); out.println("Server received " + msg); out.flush(); if (msg.equals("bye")) &#123; break; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 这里try (ServerSocket server = new ServerSocket(12345))采用了java7的try-with-resources语法，这样写的好处是try里面声明的资源在最后都会自动关闭，而不需要我们手动关闭，简化了语法 Client类1234567891011121314151617181920212223242526272829303132333435363738package edu.gduf.bio.socket.demo1;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.PrintWriter;import java.net.Socket;/** * 基于bio的socket客户端 * * @author ZGJ * @date 2017年5月4日 */public class Client &#123; public static void main(String[] args) &#123; try (Socket socket = new Socket("127.0.0.1", 12345)) &#123; //获取socket输入流 BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); //socket输出流 PrintWriter out = new PrintWriter(socket.getOutputStream()); //标准输入流 BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); while (true) &#123; String msg = reader.readLine(); out.println(msg); out.flush(); if (msg.equals("bye")) &#123; break; &#125; System.out.println(in.readLine()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果：Client: helloServer received hellohahaServer received hahabye Server:hellohahabye 多线程通讯我们发现，服务器只能处理一个客户端的请求，服务器处理了第一个客户端的请求之后，后续的Client就不能再连接，这个时候我们需要做一些改动，当服务器收到客户端的连接请求后，客户端的请求放在一个新的线程中去处理，而主线程仍然继续等待其他客户端的连接，这样就不会阻塞服务器处理其他客户端的请求了，只需要修改服务器的代码，客户端代码和原来的一样12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package edu.gduf.bio.socket.demo2;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.PrintWriter;import java.net.ServerSocket;import java.net.Socket;/** * 处理多个client的server * @author ZGJ * @date 2017年5月4日 */public class MulitClientServer &#123; public static void main(String[] args) &#123; try(ServerSocket server = new ServerSocket(12345)) &#123; //循环接受客户端请求 while(true) &#123; Socket socket = server.accept(); handle(socket); &#125; &#125; catch(IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 处理客户请求 * @param socket */ private static void handle(Socket socket) &#123; //开启一个线程,lambda表达式 new Thread(() -&gt; &#123; try(BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); PrintWriter writer = new PrintWriter(socket.getOutputStream(), true)) &#123; while(true) &#123; String message = in.readLine(); System.out.println(message); writer.println("Server received: " + message); if("bye".equals(message)) &#123; break; &#125; &#125; &#125; catch(IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 我们可以看到，当服务器接受到一个客户单的请求之后，采用handle()方法去处理客户端的请求，这里使用了Java8中lambda表达式来替代匿名内部类的语法，来简化我们的编程，使我们的语法更加简洁。对于每一个客户端的请求都开启一个新的线程去处理，当然，如果还想做到更加优化，可以采用Executor线程池去处理，节省创建和关闭线程的开销。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>io</tag>
        <tag>socket</tag>
      </tags>
  </entry>
</search>