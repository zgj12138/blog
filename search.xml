<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hadoop初探]]></title>
    <url>%2F2017%2F07%2F30%2Fhadoop%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[前言近几年，大数据，云计算，机器学习成为了非常热门的话题，这些技术运用在了很多的领域，也是在未来很有发展前景的技术。自己最近在接触一些大数据的东西，学习大数据的话自然很有必要先学习Hadoop和Spark。这里我们就来一探Hadoop的究竟吧。 Hadoop是什么Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。核心设计： HDFS: HDFS为海量的数据提供了存储 MapReduce: MapReduce为海量的数据提供了计算 Hadoop的作者是Doug Cutting，他受到谷歌的三篇论文的启发GFS(分布式存储系统)，MapReduce(分布式运行模型), BigTable(大表)，然后用Java去实现了这三个功能，然后就有了Hadoop，不得不感叹，牛人真的是牛人啊 Hadoop是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式,数据来源可以来自任何的形式，无论数据采用什么形式，最终都会转换成key-value的形式，key/value是基本数据单元简单总结来说，Hadoop是一种分布式计算的解决方案 解决了什么问题Hadoop就是解决了大数据（大到一台计算机无法进行存储，一台计算机无法在要求的时间内进行处理）的可靠存储和处理的问题。也就是两个核心的设计，HDFS和MapReduce HDFS:设计特点：1、大数据文件，非常适合上T级别的大文件或者一堆大数据文件的存储，现在互联网上的数据量非常庞大，动不动上T的数据，所以非常适合Hadoop。2、文件分块存储，HDFS会将一个完整的大文件平均分块存储到不同计算器上，它的意义在于读取文件时可以同时从多个主机取不同区块的文件，多主机读取比单主机读取效率要高得多。3、流式数据访问，一次写入多次读写，这种模式跟传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化也只能在文件末添加内容。4、廉价硬件，HDFS可以应用在普通PC机上，这种机制能够让给一些公司用几十台廉价的计算机就可以撑起一个大数据集群。5、硬件故障，HDFS认为所有计算机都可能会出问题，为了防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其它某几个主机上，如果其中一台主机失效，可以迅速找另一块副本取文件。 关键元素 Block：将一个文件进行分块，通常是128M。 NameNode：保存整个文件系统的目录信息、文件信息及分块信息，这是由唯一一台主机专门保存，当然这台主机如果出错，NameNode就失效了。在Hadoop2.*开始支持activity-standy模式—-如果主NameNode失效，启动备用主机运行NameNode。 DataNode：分布在廉价的计算机上，用于存储Block块文件。 MapReduceMapReduce是一套从海量源数据提取分析元素最后返回结果集的编程模型，将文件分布式存储到硬盘是第一步，而从海量数据中提取分析我们需要的内容就是MapReduce做的事了。 举个例子吧，假如说你想统计一个巨大的文本文件存储在HDFS上，你想要知道这个文本里各个词的出现频率。我们把我们要运算的逻辑分发到各个节点上，在每个节点上进行运算和统计，假如在各个节点上对这些单词进行统计，我们输入的格式是一行一行的文本，而统计的结果像key-value的形式，比如在第一个节点上(hello, 30), (world, 22), (hadoop, 60),第二个节点上(hello, 20), (world, 32), (spark, 70)，也就是说将任何形式的数据转换成key-value的形式，这个过程就是Map。然后我们要统计整个文本的单词出现的次数，就要对这些节点上的数据进行汇总，将这些节点上的数据按照key分组，合并，也就是(a, num1),(a, num2), (b, num3),(b, num4(合并后就变成(a, num1 + num2), (b, num3 + num4),按照上面的结果合并就是(hello, 50), (world, 54), (hadoop, 60), spark(70)，这个过程就是Reduce 适用场景hadoop擅长离线日志分析，facebook就用Hive来进行日志分析，2009年时facebook就有非编程人员的30%的人使用HiveQL进行数据分析；淘宝搜索中的自定义筛选也使用的Hive；利用Pig还可以做高级的数据处理，包括Twitter、LinkedIn上用于发现您可能认识的人，可以实现类似Amazon.com的协同过滤的推荐效果。淘宝的商品推荐也是！在Yahoo！的40%的Hadoop作业是用pig运行的，包括垃圾邮件的识别和过滤，还有用户特征建模。（2012年8月25新更新，天猫的推荐系统是hive，少量尝试mahout！）不过从现在企业的使用趋势来看,Pig慢慢有点从企业的视野中淡化了。 Hadoop伪分布式的安装好了，我们了解和学习了Hadoop的概念之后就来学习一下如何安装Hadoop吧，这里我们先来学习伪分布式的安装，也就是NameNode和DataNoe都在同一台服务器上而且salve也是自己 环境准备 虚拟机Vmware Centos 6.9 Hadoop 2.7.3 JDK 1.8 配置网络IP我们首先先安装好Centos，然后配置好网络，虚拟机与主机的连接方式选择NAT,然后cmd命令输入ipconfig，记录下VMware Network Adapter VMnet8 下的IP,在虚拟机中输入1vim /etc/sysconfig/network-scripts/ifcfg-eth0 IP地址要和VMware Network Adapter VMnet8 下的IP在同一个网段,我的IP是192.168.109.1，贴一个自己的配置123456789DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=staticIPADDR=192.168.109.3NETMASK=255.255.255.0GATEWAY=192.168.109.2DNS1=192.168.109.2 然后使用命令service network restart重启网络 修改主机名1vim /etc/sysconfig/network NETWORKING=yesHOSTNAME=hadoop1 修改主机名和IP的映射关系1vim /etc/hosts 192.168.109.3 hadoop1 关闭防火墙这里我们需要关闭我们的防火墙，开启防火墙会有访问限制，在虚拟机局域网内我们也不需要做访问限制，索性就把防火墙关了 12345678#查看防火墙状态service iptables status#关闭防火墙service iptables stop#查看防火墙开机启动状态chkconfig iptables --list#关闭防火墙开机启动chkconfig iptables off 创建用户我们一般不直接使用root用户，会创建一个新的用户来完成我们的实验，这里我们新建一个hadoop用户user add hadoop接下来为hadoop用户设置密码passwd hadoop然后我们为hadoop用户授予root权限vim /etc/sudoer找到root ALL=(ALL) ALL 并下面加入以下123## Allow root to run any commands anywhere root ALL=(ALL) ALLhadoop ALL=(ALL) ALL 切换用户现在我们切换到hadoop用户下进行操作 su hadoop 安装软件我们在根目录下创建一个app文件夹, mkdir app,然后我们将需要弄的文件都解压到app文件夹里面用winscp上传JDK,Hadoop的文件，解压JDK, 执行命令 tar -zxvf jdk-8u131-linux-x64.tar.gz -C app 解压Hadoop tar -zxvf hadoop-2.7.3.tar.gz -C app 这个时候我们将jdk和hadoop都解压到app目录下，接下来我们就开始配置环境了 配置环境配置JDK1vim /etc/profile 在文件最后添加如下： 12export JAVA_HOME=/home/hadoop/app/jdk1.8.0_131export PATH=$PATH:$JAVA_HOME/bin 刷新配置 1source /etc/profile 配置Hadoop注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop,这里我们的目录就是在/home/hadoop/app/hadoop-2.7.3/etc/下伪分布式需要修改5个配置文件 hadoop-env.sh这个文件表示hadoop运行环境的文件，找到25号，改成 12export JAVA_HOME=/home/hadoop/app/jdk1.8.0_131` 这个值原来是${JAVA_HOME}，但是有点问题，老是获取不到正确的值，所以这里我们就直接将它写死 core-site.xml配置如下： 123456789101112&lt;configuration&gt; &lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.7.3/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml配置如下： 1234567&lt;configuration&gt; &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml这里先执行一个命令mv mapred-site.xml.template mapred-site.xml然后再修改vim mapred-site.xml配置如下： 1234567&lt;configuration&gt; &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml配置如下： 123456789101112131415&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5个文件都修改完成后，我们需要将hadoop添加到环境中vim /etc/proflie加上HADOOP_HOME, 修改文件内容如下：1234export JAVA_HOME=/home/hadoop/app/jdk1.8.0_131export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/home/hadoop/app/hadoop-2.7.3export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 格式化hdfs第一次启动的时候我们需要格式化namenode，对namenode进行初始化，执行以下命令 1hdfs namenode -format (hadoop namenode -format) 成功的话，会看到 “successfully formatted” 和 “Exitting with status 0″ 的提示，若为 “Exitting with status 1″ 则是出错。 启动hadoop先启动HDFS: start-dfs.sh再启动YARN: start-yarn.sh期间会让你多次输入密码，我们在后面配置SSH免密登录之后就不用输入密码了验证是否启动成功，使用jps命令验证12345627408 NameNode28218 Jps27643 SecondaryNameNode28066 NodeManager27803 ResourceManager27512 DataNode 看到以上进程的时候，就说明我们启动成功了 配置SSH免登录生成ssh免登陆密钥，进入到我的home目录cd ~/.ssh,执行 1ssh-keygen -t rsa （四个回车） 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）将公钥拷贝到要免登陆的机器上ssh-copy-id localhost然后我们执行ssh localhost 就可以不用输入密码登录这台机器了 进入Web管理界面我们在浏览器里面输入http://192.168.109.3:50070/可以看到如下图片 说明我们的hadooop已经开启成功了 运行mapreduce程序好了，我们的环境也搭建成功了，现在来试着跑一下mapreduce程序，进入hadoop的share目录下 1cd /home/hadoop/app/hadoop-2.7.3/share/hadoop/mapreduce 看到有以下文件：123456789101112hadoop-mapreduce-client-app-2.7.3.jarhadoop-mapreduce-client-common-2.7.3.jarhadoop-mapreduce-client-core-2.7.3.jarhadoop-mapreduce-client-hs-2.7.3.jarhadoop-mapreduce-client-hs-plugins-2.7.3.jarhadoop-mapreduce-client-jobclient-2.7.3.jarhadoop-mapreduce-client-jobclient-2.7.3-tests.jarhadoop-mapreduce-client-shuffle-2.7.3.jarhadoop-mapreduce-examples-2.7.3.jarliblib-examplessources 运算PI圆周率这里我们用hadoop-mapreduce-examples-2.7.3.jar的例子跑一下，执行的命令为hadoop jar hadoop-mapreduce-examples-2.7.3.jar pi 5 6pi是运算圆周率，后面的两个参数代表map的任务数量和map的取样数，取样数越大，运算的结果越精确，这里我们取了5和10作为参数，结果如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126Number of Maps = 5Samples per Map = 10Wrote input for Map #0Wrote input for Map #117/07/20 22:34:51 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)Wrote input for Map #217/07/20 22:34:51 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)Wrote input for Map #317/07/20 22:34:51 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)Wrote input for Map #4Starting Job17/07/20 22:34:51 INFO client.RMProxy: Connecting to ResourceManager at hadoop1/192.168.109.3:803217/07/20 22:34:52 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:577) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:573)17/07/20 22:34:52 INFO input.FileInputFormat: Total input paths to process : 517/07/20 22:34:52 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)17/07/20 22:34:52 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)17/07/20 22:34:52 INFO mapreduce.JobSubmitter: number of splits:517/07/20 22:34:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1500560388081_000317/07/20 22:34:53 INFO impl.YarnClientImpl: Submitted application application_1500560388081_000317/07/20 22:34:53 INFO mapreduce.Job: The url to track the job: http://hadoop1:8088/proxy/application_1500560388081_0003/17/07/20 22:34:53 INFO mapreduce.Job: Running job: job_1500560388081_000317/07/20 22:35:08 INFO mapreduce.Job: Job job_1500560388081_0003 running in uber mode : false17/07/20 22:35:08 INFO mapreduce.Job: map 0% reduce 0%17/07/20 22:36:14 INFO mapreduce.Job: map 100% reduce 0%17/07/20 22:36:28 INFO mapreduce.Job: map 100% reduce 100%17/07/20 22:36:29 INFO mapreduce.Job: Job job_1500560388081_0003 completed successfully17/07/20 22:36:29 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=116 FILE: Number of bytes written=714243 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=1320 HDFS: Number of bytes written=215 HDFS: Number of read operations=23 HDFS: Number of large read operations=0 HDFS: Number of write operations=3 Job Counters Launched map tasks=5 Launched reduce tasks=1 Data-local map tasks=5 Total time spent by all maps in occupied slots (ms)=325021 Total time spent by all reduces in occupied slots (ms)=8256 Total time spent by all map tasks (ms)=325021 Total time spent by all reduce tasks (ms)=8256 Total vcore-milliseconds taken by all map tasks=325021 Total vcore-milliseconds taken by all reduce tasks=8256 Total megabyte-milliseconds taken by all map tasks=332821504 Total megabyte-milliseconds taken by all reduce tasks=8454144 Map-Reduce Framework Map input records=5 Map output records=10 Map output bytes=90 Map output materialized bytes=140 Input split bytes=730 Combine input records=0 Combine output records=0 Reduce input groups=2 Reduce shuffle bytes=140 Reduce input records=10 Reduce output records=0 Spilled Records=20 Shuffled Maps =5 Failed Shuffles=0 Merged Map outputs=5 GC time elapsed (ms)=8989 CPU time spent (ms)=9260 Physical memory (bytes) snapshot=458428416 Virtual memory (bytes) snapshot=12371886080 Total committed heap usage (bytes)=624766976 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=590 File Output Format Counters Bytes Written=97Job Finished in 98.692 secondsEstimated value of Pi is 3.28000000000000000000 我们看最后一行，得出结果为3.28,是我们的样本数量太少了，要是样本数量大一点，结果应该更接近3.14还发现个问题，运行中出现了多次警告1234567817/07/20 22:34:52 WARN hdfs.DFSClient: Caught exceptionjava.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370) at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546) 我google了一下，再hadoop issue里面找到了答案https://issues.apache.org/jira/browse/HDFS-10429，发现是bug,忽略就好了，换个hadoop版本也许就没事了，或者修改日志的输出级别 计算单词数量wordcount首先我们先新建一个words.txt文件，内容如下123456hello worldhello tomhello kevinhello jerryhello babytom and jerry 然后在hdfs里创建一个目录1hadoop fs -mkdir -p /wordcount/input 把文件上传到该目录下1hadoop fs -put words.txt /wordcount/input 查看文件是否上传上去了1hadoop fs -ls /wordcount/input 我们可以看到我们的文件已经成功上传上去了我们发现操作hdfs的命令和操作linux的命令大致都是一样的，大家可以自行去看官方的文档回到刚才的share目录下，继续执行刚才的那个示例文件 1hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /wordcount/input /wordcount/output 这里执行的方法是wordcount，第一个参数是输入的文件位置，第二个参数是输出的结果的文件位置执行结束后，我们来看一下输出目录hadoop fs -ls /wordcount/output,发下目录下生成了两个文件123Found 2 items-rw-r--r-- 1 hadoop supergroup 0 2017-07-20 23:07 /wordcount/output/_SUCCESS-rw-r--r-- 1 hadoop supergroup 51 2017-07-20 23:07 /wordcount/output/part-r-00000 查看一下part-r-00000这个文件 1hadoop fs -cat /wordcount/output/part-r-00000 结果如下：1234567and 1baby 1hello 5jerry 2kevin 1tom 2world 1 可以看到，结果是正确的，大功告成 总结接触一个新的技术，装环境什么的都是一件比较麻烦的事，可能你第一天就要花费很多时间在搭建环境上面了，可能期间你会遇到各种问题，不过也是锻炼耐心的一个过程，有一个不错的方法可以解决，那就是使用docker容器技术，使用别人搭建好的环境镜像，直接拿来用就可以，这样我们就可以不必花费太多时间在环境问题上，专心学我们的技术，有兴趣的同学可以自行了解下。还有这次的演示例子也只是拿官方的例子来做演示，后面需要自己写程序实现map-reduce,官网上也有很多的例子，所以我觉得看官方其实是最快了解一门技术的方法了，而且一些比较著名的开源项目的文档都是写的比较好的，基本上你看，然后照着demo敲一遍就可以上手了，而且那些资料还是最新的。还有这里也只是演示了伪分布的安装，其实hadoop有三种安装模式：1.独立式:Hadoop运行所有的东西在无后台的单独的JVM中，这种模式适合在开发阶段测试与Debug MapReduce程序。2.伪分布式:Hadoop做为后台应用运行在本地机器，模拟小集群。3.全分布式:Hadoop做为后台应用运行真实的集群电脑中。剩下的就留给读者自己探索吧！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习总结 2017-07]]></title>
    <url>%2F2017%2F07%2F24%2F%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%932017-07%2F</url>
    <content type="text"><![CDATA[前言记录个人在平时的学习和总结，不定期更新 2017-07-23调停者模式是什么调停者模式是对象的行为模式。调停者模式包装了一系列对象相互作用的方式，使得这些对象不必相互明显引用。从而使它们可以较松散地耦合。当这些对象中的某些对象之间的相互作用发生改变时，不会立即影响到其他的一些对象之间的相互作用。从而保证这些相互作用可以彼此独立地变化。这个示意图中有大量的对象，这些对象既会影响别的对象，又会被别的对象所影响，因此常常叫做同事(Colleague)对象。这些同事对象通过彼此的相互作用形成系统的行为。从图中可以看出，几乎每一个对象都需要与其他的对象发生相互作用，而这种相互作用表现为一个对象与另一个对象的直接耦合。这就是过度耦合的系统。如下图所示：通过引入调停者对象(Mediator)，可以将系统的网状结构变成以中介者为中心的星形结构，如下图所示。在这个星形结构中，同事对象不再通过直接的联系与另一个对象发生相互作用；相反的，它通过调停者对象与另一个对象发生相互作用。调停者对象的存在保证了对象结构上的稳定，也就是说，系统的结构不会因为新对象的引入造成大量的修改工作。 举例网络拓扑图的星型总线图星型拓扑结构是用一个节点作为中心节点和其他节点直接与中心节点相连构成的网络。中心节点可以是文件服务器，也可以是连接设备。常见的中心节点为集线器。计算机中的主板主板作为电脑里面各个配件之间的交互的桥梁，电脑各个配件的交互主要是通过主板来完成的，每个部件不需要知道其他部件的接口形式，只需要知道主板的接口形式即可，屏蔽了很多交互细节 总结设计模式是很多人在编程的道路上发现了问题，然后通过思考和实践将对这些问题的解决方式抽象出来，这就是形成了设计模式。我们发现设计模式在很多地方都可以看到影子，这是因为设计模式是一种思想，一种高度抽取用来解决问题的问题的思想，也可以用来解决生活中很多的问题。 2017-07-24select,poll,epoll共同点都是解决IO多路复用的问题，好处就在于单个process就可以同时处理多个网络连接的IO。基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 不同点 select和poll的本质是一样的，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket，不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。 epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll之会把哪个流发生了怎样的I/O事件通知我们。此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)） spark是什么Apache Spark 是个通用的集群计算框架，通过将大量数据集计算任务分配到多台计算机上，提供高效内存计算。Spark 正如其名，最大的特点就是快（Lightning-fast），可比 Hadoop MapReduce 的处理速度快 100 倍。如果你熟悉 Hadoop，那么你知道分布式计算框架要解决两个问题：如何分发数据和如何分发计算。Hadoop 使用 HDFS 来解决分布式数据问题，MapReduce 计算范式提供有效的分布式计算。类似的，Spark 拥有多种语言的函数式编程 API，提供了除 map 和 reduce 之外更多的运算符，这些操作是通过一个称作弹性分布式数据集(resilient distributed datasets, RDDs)的分布式数据框架进行的。 Spark核心组件 Spark Core：包含 Spark 的基本功能；尤其是定义 RDD 的 API、操作以及这两者上的动作。其他 Spark 的库都是构建在 RDD 和 Spark Core 之上的。 Spark SQL：提供通过 Apache Hive 的 SQL 变体 Hive 查询语言（HiveQL）与 Spark 进行交互的 API。每个数据库表被当做一个 RDD，Spark SQL 查询被转换为 Spark 操作。对熟悉 Hive 和 HiveQL 的人，Spar k可以拿来就用。 Spark Streaming：允许对实时数据流进行处理和控制。很多实时数据库（如Apache Store）可以处理实时数据。Spark Streaming 允许程序能够像普通 RDD 一样处理实时数据。 MLlib：一个常用机器学习算法库，算法被实现为对 RDD 的 Spark 操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作。之前可选的大数据机器学习库 Mahout，将会转到 Spark，并在未来实现。 GraphX：控制图、并行图操作和计算的一组算法和工具的集合。GraphX 扩展了 RDD API，包含控制图、创建子图、访问路径上所有顶点的操作。 由于这些组件满足了很多大数据需求，也满足了很多数据科学任务的算法和计算上的需要，Spark 快速流行起来。不仅如此，Spark 也提供了使用 Scala、Java 和Python 编写的 API；满足了不同团体的需求，允许更多数据科学家简便地采用 Spark 作为他们的大数据解决方案。 Spark 体系架构Spark体系架构包括如下三个主要组件： 数据存储 API 管理框架 数据存储：Spark 用 HDFS 文件系统存储数据。它可用于存储任何兼容于 Hadoop 的数据源，包括HDFS，Hbase，Cassandra等。 API：利用 API，应用开发者可以用标准的 API 接口创建基于 Spark 的应用。Spark 提供 Scala，Java 和 Python 三种程序设计语言的 API。Spark基本概念： Application： 用户自己写的 Spark 应用程序，批处理作业的集合。Application 的 main 方法为应用程序的入口，用户通过 Spark 的 API，定义了 RDD 和对 RDD 的操作。 SparkContext： Spark 最重要的 API，用户逻辑与 Spark 集群主要的交互接口，它会和 Cluster Master 交互，包括向它申请计算资源等。 Driver 和 Executor：Spark 在执行每个 Application 的过程中会启动 Driver 和 Executor 两种 JVM 进程。Driver 进程为主控进程，负责执行用户 Application 中的 main 方法，提交 Job，并将 Job 转化为 Task，在各个 Executor 进程间协调 Task 的调度。运行在Worker上 的 Executor 进程负责执行 Task，并将结果返回给 Driver，同时为需要缓存的 RDD 提供存储功能。资源管理： 一组计算机的集合，每个计算机节点作为独立的计算资源，又可以虚拟出多个具备计算能力的虚拟机，这些虚拟机是集群中的计算单元。Spark 的核心模块专注于调度和管理虚拟机之上分布式计算任务的执行，集群中的计算资源则交给 Cluster Manager 这个角色来管理，Cluster Manager 可以为自带的Standalone、或第三方的 Yarn和 Mesos。 Cluster Manager 一般采用 Master-Slave 结构。以 Yarn 为例，部署 ResourceManager 服务的节点为 Master，负责集群中所有计算资源的统一管理和分配；部署 NodeManager 服务的节点为Slave，负责在当前节点创建一个或多个具备独立计算能力的 JVM 实例，在 Spark 中，这些节点也叫做 Worker。 另外还有一个 Client 节点的概念，是指用户提交Spark Application 时所在的节点。 弹性分布式数据集(RDD)： 弹性分布式数据集(RDD)是 Spark 框架中的核心概念。可以将 RDD 视作数据库中的一张表。其中可以保存任何类型的数据。Spark 将数据存储在不同分区上的 RDD 之中。 RDD 可以帮助重新安排计算并优化数据处理过程。 此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。 RDD 是不可变的。你可以用变换（Transformation）修改 RDD，但是这个变换所返回的是一个全新的RDD，而原有的 RDD 仍然保持不变。 RDD 支持两种类型的操作： 变换（Transformation） 变换的返回值是一个新的 RDD 集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个 RDD 作为参数，然后返回一个新的 RDD。 变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce。 行动（Action） 行动操作计算并返回一个新的值。当在一个 RDD 对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。 行动操作包括：reduce，collect，count，first，take，countByKey 以及 foreach。 Java客户端在本地跑数据配置1234SparkConf sparkConf = new SparkConf() .setAppName("spark01") .setMaster("local[4]");sc = new JavaSparkContext(sparkConf); local代表的是在本地跑，[4]指的是创建4个节点 总结跟着官网首先在虚拟机上安装了spark，然后使用scala连接到spark-shell，试着写了几个demo,发现hadoop的map,reduce操作也在里面，还有Java8中对stream中的操作，基本上思想是一样的，map变换，延迟执行，reduce汇聚。]]></content>
      <categories>
        <category>学习总结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java多线程爬虫爬取京东商品信息]]></title>
    <url>%2F2017%2F07%2F15%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E4%BA%AC%E4%B8%9C%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言网络爬虫，是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。爬虫可以通过模拟浏览器访问网页，从而获取数据，一般网页里会有很多个URL,爬虫可以访问这些URL到达其他网页，相当于形成了一种数据结构——图，我们通过广度优先搜索和深度优先搜索的方式来遍历这个图，从而做到不断爬取数据的目的。最近准备做一个电商网站，商品的原型就打算从一些电商网站上爬取，这里使用了HttpClient和Jsoup实现了一个简答的爬取商品的demo,采用了多线程的方式，并将爬取的数据持久化到了数据库。 项目环境搭建整体使用技术我IDE使用了Spring Tool Suite(sts)，你也可以使用Eclipse或者是IDEA，安利使用IDEA，真的好用，谁用谁知道。整个项目使用Maven进行构建吗，使用Springboot进行自动装配，使用HttpClient对网页进行抓取，Jsoup对网页进行解析，数据库连接池使用Druild，还使用了工具类Guava和Commons.lang3。 项目结构在sts里面新建一个maven工程，创建如下的包 common 一些通用工具类 constant 系统常量 dao 数据库访问层 service 服务层 handler 调度控制层 entity 实体层这样分层的意义是使得项目结构层次清晰，每层都有着其对应的职责，便于扩展和维护 pom文件这里使用maven进行构建，还没有了解maven的童鞋自行去了解，使用maven的好处是不用自己导入jar包和完整的生命周期控制，注意，使用阿里云的镜像速度回加快很多。项目的pom.xml文件如下pom.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.exmaple&lt;/groupId&gt; &lt;artifactId&gt;spider-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spider-demo&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;jsoup.version&gt;1.10.3&lt;/jsoup.version&gt; &lt;guava.version&gt;22.0&lt;/guava.version&gt; &lt;lang3.version&gt;3.6&lt;/lang3.version&gt; &lt;mysql.version&gt;5.1.42&lt;/mysql.version&gt; &lt;druid.version&gt;1.1.0&lt;/druid.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.4.RELEASE&lt;/version&gt; &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- httpclient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- jsoup --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;/groupId&gt; &lt;artifactId&gt;jsoup&lt;/artifactId&gt; &lt;version&gt;$&#123;jsoup.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- guava --&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;$&#123;guava.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- commons-lang3 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;$&#123;lang3.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.34&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;spider-demo&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; application.yml文件spring boot的配置文件有两种形式，放在src/main/resources目录下，分别是application.yml和application.properties这里为了配置更加简洁，使用了application.yml作为我们的配置文件application.yml12345678# mysqlspring: datasource: type: com.alibaba.druid.pool.DruidDataSource driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/spider?useUnicode=true&amp;characterEncoding=UTF-8&amp;&amp;useSSL=true username: root password: 123 这里可以在url，username和pssword里换成自己环境对应的配置 sql文件这里我们创建了一个数据库和一张表，以便后面将商品信息持久化到数据库db.sql123456789USE spider;CREATE TABLE `goods_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT COMMENT 'ID', `goods_id` VARCHAR(255) NOT NULL COMMENT '商品ID', `goods_name` VARCHAR(255) NOT NULL COMMENT '商品名称', `img_url` VARCHAR(255) NOT NULL COMMENT '商品图片地址', `goods_price` VARCHAR(255) NOT NULL COMMENT '商品标价', PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT='商品信息表'; 网页的分析网址URL的分析我们要爬取的网页的URL的基本地址是https://search.jd.com/Search我们打开这个网页，在搜索框内搜索零食，我们看一下我们的浏览器的地址栏的URL的变化，发现浏览器的地址栏变成了https://search.jd.com/Search?keyword=零食&amp;enc=utf-8&amp;wq=零食&amp;pvid=2c636c9dc26c4e6e88e0dea0357b81a3我们就可以对参数进行分析，keyword和wq应该是代表要搜索的关键字，enc代表的编码，·pvid不知道是什么，我们吧这个参数去掉看能不能访问https://search.jd.com/Search?keyword=零食&amp;enc=utf-8&amp;wq=零食，发现这个URL也是可以正常访问到这个网址的，那么我们就可以暂时忽略这个参数，参数就设置就设置keyword,wq和enc这里我们要设置的参数就是 keyword 零食 wq 零食 enc utf-8 网页内容的分析我们打开我们要爬取数据的页面使用浏览器-检查元素通过查看源码，我们发现JD的商品列表放在id是J_goodsList的div下的的class是gl-warp clearfix的ul标签的gl-item的li标签下再分别审查各个元素，我们发现 li标签的data-sku的属性值就是商品的ID li标签下的class为p-name p-name-type-2的em的值就是商品的名称 li标签下的class为p-price的strong标签下的i标签的值是商品的价格 li标签下的class为p-img的img标签的src值就是商品的图片URL 对网页进行了分析以后，我们就可以通过对DOM结点的选择来筛选我们想要的数据了 代码的编写这里我们封装了HttpClientUtils作为我们的工具类，以便以后使用 HttpClientUtils工具类HttpClient.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212package com.exmaple.spider.common;import java.io.IOException;import java.io.UnsupportedEncodingException;import java.util.ArrayList;import java.util.List;import java.util.Map;import java.util.Map.Entry;import org.apache.http.HttpEntity;import org.apache.http.NameValuePair;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.client.methods.HttpPost;import org.apache.http.entity.ContentType;import org.apache.http.entity.StringEntity;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.message.BasicNameValuePair;import org.apache.http.util.EntityUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import com.exmaple.spider.constant.SysConstant;/** * HttpClient工具类 * * @author ZGJ * @date 2017年7月14日 */public class HttpClientUtils &#123; private final static Logger logger = LoggerFactory.getLogger(HttpClientUtils.class); private final static String GET_METHOD = "GET"; private final static String POST_METHOD = "POST"; /** * GET请求 * * @param url * 请求url * @param headers * 头部 * @param params * 参数 * @return */ public static String sendGet(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) &#123; // 创建HttpClient对象 CloseableHttpClient client = HttpClients.createDefault(); StringBuilder reqUrl = new StringBuilder(url); String result = ""; /* * 设置param参数 */ if (params != null &amp;&amp; params.size() &gt; 0) &#123; reqUrl.append("?"); for (Entry&lt;String, String&gt; param : params.entrySet()) &#123; reqUrl.append(param.getKey() + "=" + param.getValue() + "&amp;"); &#125; url = reqUrl.subSequence(0, reqUrl.length() - 1).toString(); &#125; logger.debug("[url:" + url + ",method:" + GET_METHOD + "]"); HttpGet httpGet = new HttpGet(url); /** * 设置头部 */ logger.debug("Header\n"); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; for (Entry&lt;String, String&gt; header : headers.entrySet()) &#123; httpGet.addHeader(header.getKey(), header.getValue()); logger.debug(header.getKey() + " : " + header.getValue()); &#125; &#125; CloseableHttpResponse response = null; try &#123; response = client.execute(httpGet); /** * 请求成功 */ if (response.getStatusLine().getStatusCode() == 200) &#123; HttpEntity entity = response.getEntity(); result = EntityUtils.toString(entity, SysConstant.DEFAULT_CHARSET); &#125; &#125; catch (IOException e) &#123; logger.error("网络请求出错，请检查原因"); &#125; finally &#123; // 关闭资源 try &#123; if (response != null) &#123; response.close(); &#125; client.close(); &#125; catch (IOException e) &#123; logger.error("网络关闭错误错，请检查原因"); &#125; &#125; return result; &#125; /** * POST请求 * * @param url * 请求url * @param headers * 头部 * @param params * 参数 * @return */ public static String sendPost(String url, Map&lt;String, String&gt; headers, Map&lt;String, String&gt; params) &#123; CloseableHttpClient client = HttpClients.createDefault(); String result = ""; HttpPost httpPost = new HttpPost(url); /** * 设置参数 */ if (params != null &amp;&amp; params.size() &gt; 0) &#123; List&lt;NameValuePair&gt; paramList = new ArrayList&lt;&gt;(); for (Entry&lt;String, String&gt; param : params.entrySet()) &#123; paramList.add(new BasicNameValuePair(param.getKey(), param.getValue())); &#125; logger.debug("[url: " + url + ",method: " + POST_METHOD + "]"); // 模拟表单提交 try &#123; UrlEncodedFormEntity entity = new UrlEncodedFormEntity(paramList, SysConstant.DEFAULT_CHARSET); httpPost.setEntity(entity); &#125; catch (UnsupportedEncodingException e) &#123; logger.error("不支持的编码"); &#125; /** * 设置头部 */ if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; logger.debug("Header\n"); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; for (Entry&lt;String, String&gt; header : headers.entrySet()) &#123; httpPost.addHeader(header.getKey(), header.getValue()); logger.debug(header.getKey() + " : " + header.getValue()); &#125; &#125; &#125; CloseableHttpResponse response = null; try &#123; response = client.execute(httpPost); HttpEntity entity = response.getEntity(); result = EntityUtils.toString(entity, SysConstant.DEFAULT_CHARSET); &#125; catch (IOException e) &#123; logger.error("网络请求出错，请检查原因"); &#125; finally &#123; try &#123; if (response != null) &#123; response.close(); &#125; client.close(); &#125; catch (IOException e) &#123; logger.error("网络关闭错误"); &#125; &#125; &#125; return result; &#125; /** * post请求发送json * @param url * @param json * @param headers * @return */ public static String senPostJson(String url, String json, Map&lt;String, String&gt; headers) &#123; CloseableHttpClient client = HttpClients.createDefault(); String result = ""; HttpPost httpPost = new HttpPost(url); StringEntity stringEntity = new StringEntity(json, ContentType.APPLICATION_JSON); httpPost.setEntity(stringEntity); logger.debug("[url: " + url + ",method: " + POST_METHOD + ", json: " + json + "]"); /** * 设置头部 */ if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; logger.debug("Header\n"); if (headers != null &amp;&amp; headers.size() &gt; 0) &#123; for (Entry&lt;String, String&gt; header : headers.entrySet()) &#123; httpPost.addHeader(header.getKey(), header.getValue()); logger.debug(header.getKey() + " : " + header.getValue()); &#125; &#125; &#125; CloseableHttpResponse response = null; try &#123; response = client.execute(httpPost); HttpEntity entity = response.getEntity(); result = EntityUtils.toString(entity, SysConstant.DEFAULT_CHARSET); &#125; catch (IOException e) &#123; logger.error("网络请求出错，请检查原因"); &#125; finally &#123; try &#123; if (response != null) &#123; response.close(); &#125; client.close(); &#125; catch (IOException e) &#123; logger.error("网络关闭错误"); &#125; &#125; return result; &#125;&#125; SyConstant.java 系统常量SysConstant.java1234567891011121314151617181920212223242526272829303132package com.exmaple.spider.constant;/** * 系统全局常量 * @author ZGJ * @date 2017年7月15日 */public interface SysConstant &#123; /** * 系统默认字符集 */ String DEFAULT_CHARSET = "utf-8"; /** * 需要爬取的网站 */ String BASE_URL = "https://search.jd.com/Search"; interface Header &#123; String ACCEPT = "Accept"; String ACCEPT_ENCODING = "Accept-Encoding"; String ACCEPT_LANGUAGE = "Accept-Language"; String CACHE_CONTROL = "Cache-Controle"; String COOKIE = "Cookie"; String HOST = "Host"; String PROXY_CONNECTION = "Proxy-Connection"; String REFERER = "Referer"; String USER_AGENT = "User-Agent"; &#125; /** * 默认日期格式 */ String DEFAULT_DATE_FORMAT = "yyy-MM-dd HH:mm:ss";&#125; GoodsInfo 商品信息GoodsInfo.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.exmaple.spider.entity;public class GoodsInfo &#123; private Integer id; private String goodsId; private String goodsName; private String imgUrl; private String goodsPrice; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getGoodsId() &#123; return goodsId; &#125; public void setGoodsId(String goodsId) &#123; this.goodsId = goodsId; &#125; public String getGoodsName() &#123; return goodsName; &#125; public void setGoodsName(String goodsName) &#123; this.goodsName = goodsName; &#125; public String getImgUrl() &#123; return imgUrl; &#125; public void setImgUrl(String imgUrl) &#123; this.imgUrl = imgUrl; &#125; public String getGoodsPrice() &#123; return goodsPrice; &#125; public void setGoodsPrice(String goodsPrice) &#123; this.goodsPrice = goodsPrice; &#125; public GoodsInfo(String goodsId, String goodsName, String imgUrl, String goodsPrice) &#123; super(); this.goodsId = goodsId; this.goodsName = goodsName; this.imgUrl = imgUrl; this.goodsPrice = goodsPrice; &#125; &#125; GoodsInfoDao 商品信息Dao层因为这里仅仅涉及到把商品信息写入到数据库比较简单的操作，并没有使用MyBatis或者Hibernate框架，只是使用了Spring的JdbcTemplate对数据进行插入操作GoodsInfoDao.java123456789101112131415161718package com.exmaple.spider.dao;import java.util.List;import com.exmaple.spider.entity.GoodsInfo;/** * 商品Dao层 * @author ZGJ * @date 2017年7月15日 */public interface GoodsInfoDao &#123; /** * 插入商品信息 * @param infos */ void saveBatch(List&lt;GoodsInfo&gt; infos);&#125; GoodsInfoDaoImpl.java1234567891011121314151617181920212223242526package com.exmaple.spider.dao.impl;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;import com.exmaple.spider.dao.GoodsInfoDao;import com.exmaple.spider.entity.GoodsInfo;@Repositorypublic class GoodsInfoDaoImpl implements GoodsInfoDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public void saveBatch(List&lt;GoodsInfo&gt; infos) &#123; String sql = "REPLACE INTO goods_info(" + "goods_id," + "goods_name," + "goods_price," + "img_url) " + "VALUES(?,?,?,?)"; for(GoodsInfo info : infos) &#123; jdbcTemplate.update(sql, info.getGoodsId(), info.getGoodsName(), info.getGoodsPrice(), info.getImgUrl()); &#125; &#125;&#125; 商品的Dao层实现了向数据库里插入商品信息，使用JdbcTemplate和占位符的方式设置sql语句 SpiderService 爬虫服务层SpiderService.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.exmaple.spider.service;import java.util.HashMap;import java.util.List;import java.util.Map;import org.apache.commons.lang3.StringUtils;import org.jsoup.Jsoup;import org.jsoup.nodes.Document;import org.jsoup.nodes.Element;import org.jsoup.select.Elements;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.alibaba.fastjson.JSON;import com.exmaple.spider.common.HttpClientUtils;import com.exmaple.spider.constant.SysConstant;import com.exmaple.spider.dao.GoodsInfoDao;import com.exmaple.spider.entity.GoodsInfo;import com.google.common.collect.Lists;@Servicepublic class SpiderService &#123; private static Logger logger = LoggerFactory.getLogger(SpiderService.class); @Autowired private GoodsInfoDao goodsInfoDao; private static String HTTPS_PROTOCOL = "https:"; public void spiderData(String url, Map&lt;String, String&gt; params) &#123; String html = HttpClientUtils.sendGet(url, null, params); if(!StringUtils.isBlank(html)) &#123; List&lt;GoodsInfo&gt; goodsInfos =parseHtml(html); goodsInfoDao.saveBatch(goodsInfos); &#125; &#125; /** * 解析html * @param html */ private List&lt;GoodsInfo&gt; parseHtml(String html) &#123; //商品集合 List&lt;GoodsInfo&gt; goods = Lists.newArrayList(); /** * 获取dom并解析 */ Document document = Jsoup.parse(html); Elements elements = document. select("ul[class=gl-warp clearfix]").select("li[class=gl-item]"); int index = 0; for(Element element : elements) &#123; String goodsId = element.attr("data-sku"); String goodsName = element.select("div[class=p-name p-name-type-2]").select("em").text(); String goodsPrice = element.select("div[class=p-price]").select("strong").select("i").text(); String imgUrl = HTTPS_PROTOCOL + element.select("div[class=p-img]").select("a").select("img").attr("src"); GoodsInfo goodsInfo = new GoodsInfo(goodsId, goodsName, imgUrl, goodsPrice); goods.add(goodsInfo); String jsonStr = JSON.toJSONString(goodsInfo); logger.info("成功爬取【" + goodsName + "】的基本信息 "); logger.info(jsonStr); if(index ++ == 9) &#123; break; &#125; &#125; return goods; &#125;&#125; Service层通过使用HttpClientUtils模拟浏览器访问页面，然后再使用Jsoup对页面进行解析，Jsoup的使用和Jquery的DOM结点选取基本相似，可以看作是java版的Jquery，如果写过Jquery的人基本上就可以看出是什么意思。每抓取一条信息就会打印一次记录，而且使用fastjson将对象转换成json字符串并输出在写测试代码的时候发现，发现爬取的数据只有前10条是完整的，后面的爬取的有些是不完整的，按道理来说是对于整个页面都是通用的，就是不知道为什么只有前面才是完整的，排查了很久没用发现原因，这里就只选择了前面的10条作为要爬取的数据我们了解到，我们要爬取数据前要分析我们要爬取的数据有哪些，再分析网友的结构，然后对网页进行解析，选取对应的DOM或者使用正则表达式筛选，思路首先要清晰，有了思路之后剩下的也只是把你的思路翻译成代码而已了。 SpiderHandler 爬虫调度处理器SpiderHandler.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.exmaple.spider.handler;import java.util.Date;import java.util.Map;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import org.apache.commons.lang3.time.FastDateFormat;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import com.exmaple.spider.constant.SysConstant;import com.exmaple.spider.service.SpiderService;import com.google.common.collect.Maps;/** * 爬虫调度处理器 * @author ZGJ * @date 2017年7月15日 */@Componentpublic class SpiderHandler &#123; @Autowired private SpiderService spiderService; private static final Logger logger = LoggerFactory.getLogger(SpiderHandler.class); public void spiderData() &#123; logger.info("爬虫开始...."); Date startDate = new Date(); // 使用现线程池提交任务 ExecutorService executorService = Executors.newFixedThreadPool(5); //引入countDownLatch进行线程同步，使主线程等待线程池的所有任务结束，便于计时 CountDownLatch countDownLatch = new CountDownLatch(100); for(int i = 1; i &lt; 201; i += 2) &#123; Map&lt;String, String&gt; params = Maps.newHashMap(); params.put("keyword", "零食"); params.put("enc", "utf-8"); params.put("wc", "零食"); params.put("page", i + ""); executorService.submit(() -&gt; &#123; spiderService.spiderData(SysConstant.BASE_URL, params); countDownLatch.countDown(); &#125;); &#125; try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; executorService.shutdown(); Date endDate = new Date(); FastDateFormat fdf = FastDateFormat.getInstance(SysConstant.DEFAULT_DATE_FORMAT); logger.info("爬虫结束...."); logger.info("[开始时间:" + fdf.format(startDate) + ",结束时间:" + fdf.format(endDate) + ",耗时:" + (endDate.getTime() - startDate.getTime()) + "ms]"); &#125;&#125; SpiderHandelr作为一个爬虫服务调度处理器，这里采用了ExecutorService线程池创建了5个线程进行多线程爬取，我们通过翻页发现，翻页过后地址URL多了一个page参数，而且这个参数还只能是奇数才有效，也就是page为1,3，5,7……代表第1,2,3,4……页。这里就只爬了100页，每页10条数据，将page作为不同的参数传给不同的任务。这里我想统计一下整个爬取任务所用的时间，假如不使用同步工具类的话，因为任务是分到线程池中去运行的，而主线程会继续执行下去，主线程和线程池中的线程是独立运行的，主线程会提前结束，所以就无法统计时间。这里我们使用CountDownLatch同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完后再执行。也就是说可以让主线程等待线程池内的线程执行结束再继续执行，里面维护了一个计数器，开始的时候构造计数器的初始数量，每个线程执行结束的时候调用countdown()方法，计数器就减1，调用await()方法，假如计数器不为0就会阻塞，假如计数器为0了就可以继续往下执行1234executorService.submit(() -&gt; &#123; spiderService.spiderData(SysConstant.BASE_URL, params); countDownLatch.countDown();&#125;); 这里使用了Java8中的lambda表达式替代了匿名内部类，详细的可以自行去了解这里还可以根据自己的业务需求做一些代码的调整和优化，比如实现定时任务爬取等等 App.java Spring Boot启动类App.java123456789101112131415161718192021222324package com.exmaple.spider;import javax.annotation.PostConstruct;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import com.exmaple.spider.handler.SpiderHandler;@SpringBootApplicationpublic class App &#123; @Autowired private SpiderHandler spiderHandler; public static void main(String[] args) throws Exception &#123; SpringApplication.run(App.class, args); &#125; @PostConstruct public void task() &#123; spiderHandler.spiderData(); &#125;&#125; 使用@PostConstruct注解会在spring容器实例化bean之前执行这个方法 运行结果我们以Spring Boot App的方式运行App.java文件，得到的结果如下：我们在看一下数据库内的信息发现数据库也有信息了，大功告成 总结写一个简单的爬虫其实也不难，但是其中也有不少的知识点需要梳理和记忆，发现问题或者是错误，查google，查文档，一点点debug去调试，最终把问题一点点的解决，编程其实需要是解决问题的能力，这种的能力的锻炼需要我们去多写代码，写完了代码之后还要多思考，思考为什么要这样写？还有没有更好的实现方式？为什么会出问题？需要怎么解决？这才是一名优秀的程序员应该养成的习惯，共勉！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis集群的原理的搭建]]></title>
    <url>%2F2017%2F07%2F08%2FRedis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[前言Redis 是我们目前大规模使用的缓存中间件，由于它强大高效而又便捷的功能，得到了广泛的使用。单节点的Redis已经就达到了很高的性能，为了提高可用性我们可以使用Redis集群。本文参考了Rdis的官方文档和使用Redis官方提供的Redis Cluster工具搭建Rdis集群。 注意 ：Redis的版本要在3.0以上,截止今天，Redis的版本是3.2.9，本教程也使用3.2.9作为教程 Redis集群的概念介绍Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施（installation）。 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的错误。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis 集群提供了以下两个好处： 将数据自动切分（split）到多个节点的能力。 当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。 数据分片Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个哈希槽， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。 节点 B 负责处理 5501 号至 11000 号哈希槽。 节点 C 负责处理 11001 号至 16384 号哈希槽。 这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说：我现在想设置一个key,叫my_name: set my_name zhangguoji 按照Redis Cluster的哈希槽算法，CRC16(&#39;my_name&#39;)%16384 = 2412 那么这个key就被分配到了节点A上同样的，当我连接(A,B,C)的任意一个节点想获取my_name这个key,都会转到节点A上再比如如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。增加一个D节点的结果可能如下： 节点A覆盖1365-5460 节点B覆盖6827-10922 节点C覆盖12288-16383 节点D覆盖0-1364,5461-6826,10923-1228 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。所以,Redis Cluster的模型大概是这样的形状 主从复制模型为了使得集群在一部分节点下线或者无法与集群的大多数（majority）节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： 集群中的每个节点都有 1 个至 N 个复制品（replica）， 其中一个复制品为主节点（master）， 而其余的 N-1 个复制品为从节点（slave）。 在之前列举的节点 A 、B 、C 的例子中， 如果节点 B 下线了， 那么集群将无法正常运行， 因为集群找不到节点来处理 5501 号至 11000号的哈希槽。 另一方面， 假如在创建集群的时候（或者至少在节点 B 下线之前）， 我们为主节点 B 添加了从节点 B1 ， 那么当主节点 B 下线的时候， 集群就会将 B1 设置为新的主节点， 并让它代替下线的主节点 B ， 继续处理 5501 号至 11000 号的哈希槽， 这样集群就不会因为主节点 B 的下线而无法正常运作了。 不过如果节点 B 和 B1 都下线的话， Redis 集群还是会停止运作。 Redis一致性保证Redis 并不能保证数据的强一致性. 这意味这在实际中集群在特定的条件下可能会丢失写操作：第一个原因是因为集群是用了异步复制. 写操作过程: 客户端向主节点B写入一条命令. 主节点B向客户端回复命令状态. 主节点将写操作复制给他得从节点 B1, B2 和 B3 主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 注意：Redis 集群可能会在将来提供同步写的方法。 Redis 集群另外一种可能会丢失命令的情况是集群出现了网络分区， 并且一个客户端与至少包括一个主节点在内的少数实例被孤立。举个例子 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， A1 、B1 、C1 为A，B，C的从节点， 还有一个客户端 Z1 假设集群中发生网络分区，那么集群可能会分为两方，大部分的一方包含节点 A 、C 、A1 、B1 和 C1 ，小部分的一方则包含节点 B 和客户端 Z1 .Z1仍然能够向主节点B中写入, 如果网络分区发生时间较短,那么集群将会继续正常运作,如果分区的时间足够让大部分的一方将B1选举为新的master，那么Z1写入B中得数据便丢失了.注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项 搭建Redis集群要让集群正常工作至少需要3个主节点，在这里我们要创建6个redis节点，其中三个为主节点，三个为从节点，对应的redis节点的ip和端口对应关系如下（为了简单演示都在同一台机器上面）1234567891011127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002127.0.0.1:7003127.0.0.1:7004127.0.0.1:7005 安装和启动Redis下载安装包1wget http://download.redis.io/releases/redis-3.2.9.tar.gz 解压安装123tar zxvf redis-3.2.9.tar.gzcd redis-3.2.9make &amp;&amp; make PREFIX=/usr/local/redis install 这里如果失败的自行yum安装gcc和tcl 12yum install gccyum install tcl 创建目录1234cd /usr/local/redismkdir clustercd clustermkdir 7000 7001 7002 7003 7004 7005 复制和修改配置文件将redis目录下的配置文件复制到对应端口文件夹下,6个文件夹都要复制一份1cp redis-3.2.9/redis.conf /usr/local/redis/cluster/7000 修改配置文件redis.conf，将下面的选项修改123456789101112131415161718# 端口号port 7000# 后台启动daemonize yes# 开启集群cluster-enabled yes#集群节点配置文件cluster-config-file nodes-7000.conf# 集群连接超时时间cluster-node-timeout 5000# 进程pid的文件位置pidfile /var/run/redis-7000.pid# 开启aofappendonly yes# aof文件路径appendfilename &quot;appendonly-7005.aof&quot;# rdb文件路径dbfilename dump-7000.rdb 6个配置文件安装对应的端口分别修改配置文件 创建启动脚本在/usr/local/redis目录下创建一个start.sh1234567#!/bin/bashbin/redis-server cluster/7000/redis.confbin/redis-server cluster/7001/redis.confbin/redis-server cluster/7002/redis.confbin/redis-server cluster/7003/redis.confbin/redis-server cluster/7004/redis.confbin/redis-server cluster/7005/redis.conf 这个时候我们查看一下进程看启动情况1ps -ef | grep redis 进程状态如下：123456root 1731 1 1 18:21 ? 00:00:49 bin/redis-server *:7000 [cluster] root 1733 1 0 18:21 ? 00:00:29 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:08 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:26 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:13 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:08 bin/redis-server *:7005 [cluster] 有6个redis进程在开启，说明我们的redis就启动成功了 开启集群这里我们只是开启了6个redis进程而已，它们都还只是独立的状态，还么有组成集群这里我们使用官方提供的工具redis-trib，不过这个工具是用ruby写的，要先安装ruby的环境1yum install ruby rubygems -y 执行，报错12345[root@centos]# redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005/usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31:in `gem_original_require&apos;: no such file to load -- redis (LoadError) from /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31:in `require&apos; from /usr/local/bin/redis-trib.rb:25[root@centos]# 原来是ruby和redis的连接没安装好安装gem-redis1gem install redis 安装到这里的时候突然卡住很久不动，网上搜了下，这里需要翻墙或者换镜像1gem source -a https://gems.ruby-china.org 这里可以将镜像换成ruby-china的镜像，不过我好像更换失败，最终还是翻墙下载了12345[root@centos]# gem install redisSuccessfully installed redis-3.2.11 gem installedInstalling ri documentation for redis-3.2.1...Installing RDoc documentation for redis-3.2.1... 等下载好后我们就可以使用了12345[root@centos]# gem install redisSuccessfully installed redis-3.2.11 gem installedInstalling ri documentation for redis-3.2.1...Installing RDoc documentation for redis-3.2.1... 将redis-3.2.9的src目录下的trib复制到相应文件夹1cp redis-3.2.9/src/redis-trib.rb /usr/local/redis/bin/redis-trib 创建集群：1redis-trib create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 命令的意义如下： 给定 redis-trib.rb 程序的命令是 create ， 这表示我们希望创建一个新的集群。 选项 –replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。之后跟着的其他参数则是实例的地址列表， 我们希望程序使用这些地址所指示的实例来创建新集群。简单来说，以上的命令的意思就是让redis-trib程序帮我们创建三个主节点和三个从节点的集群接着， redis-trib 会打印出一份预想中的配置给你看， 如果你觉得没问题的话， 就可以输入 yes ， redis-trib 就会将这份配置应用到集群当中： 12345678910111213141516171819202122&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002Adding replica 127.0.0.1:7003 to 127.0.0.1:7000Adding replica 127.0.0.1:7004 to 127.0.0.1:7001Adding replica 127.0.0.1:7005 to 127.0.0.1:7002M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) masterM: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) masterM: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) masterS: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6Can I set the above configuration? (type &apos;yes&apos; to accept): 按下yes，集群就会将配置应用到各个节点，并连接起（join)各个节点，也即是，让各个节点开始通讯123456789101112131415161718192021222324252627&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. Redis集群的使用连接集群这里我们使用reids-cli连接集群，使用时加上-c参数，就可以连接到集群连接7000端口的节点123456[root@centos1 redis]# ./redis-cli -c -p 7000127.0.0.1:7000&gt; set name zgj-&gt; Redirected to slot [5798] located at 127.0.0.1:7001OK127.0.0.1:7001&gt; get name&quot;zgj&quot; 前面的理论知识我们知道了，分配key的时候，它会使用CRC16算法，这里将keyname分配到了7001节点上 1Redirected to slot [5798] located at 127.0.0.1:7001 redis cluster 采用的方式很直接，它直接跳转到7001 节点了，而不是还在自身的7000节点。 好，现在我们连接7003这个从节点进入1234[root@centos1 redis]# ./redis-cli -c -p 7003127.0.0.1:7003&gt; get name-&gt; Redirected to slot [5798] located at 127.0.0.1:7001&quot;zgj&quot; 这里获取name的值，也同样跳转到了7001上我们再测试一下其他数据123456789127.0.0.1:7001&gt; set age 20-&gt; Redirected to slot [741] located at 127.0.0.1:7000OK127.0.0.1:7000&gt; set message helloworld-&gt; Redirected to slot [11537] located at 127.0.0.1:7002OK127.0.0.1:7002&gt; set height 175-&gt; Redirected to slot [8223] located at 127.0.0.1:7001OK 我们发现数据会在7000-7002这3个节点之间来回跳转 测试集群中的节点挂掉上面我们建立了一个集群，3个主节点和3个从节点，7000-7002负责存取数据，7003-7005负责把7000-7005的数据同步到自己的节点上来。我们现在来模拟一下一台matser服务器宕机的情况 12345678910111213141516171819202122232425262728293031[root@centos1 redis]# ps -ef | grep redisroot 1731 1 0 18:21 ? 00:01:02 bin/redis-server *:7000 [cluster] root 1733 1 0 18:21 ? 00:00:43 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:22 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:40 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:27 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:22 bin/redis-server *:7005 [cluster] root 23988 1 0 18:30 ? 00:00:42 ./redis-server *:6379 root 24491 1635 0 21:55 pts/1 00:00:00 grep redis[root@centos1 redis]# kill 1731[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7001&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7001)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots:0-5460 (5461 slots) master 0 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里看得出来，现在已经有了3个节点了，7003被选取成了替代7000成为主节点了。我们再来模拟 7000节点重新启动了的情况，那么它还会自动加入到集群中吗？那么，7000这个节点上充当什么角色呢？ 我们试一下：12345678910111213141516171819202122232425[root@centos1 redis]# bin/redis-server cluster/7000/redis.conf[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7000&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)S: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots: (0 slots) slave replicates d403713ab9db48aeac5b5393b69e1201026ef479S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots:0-5460 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里我们可以看到7000节点变成了7003节点的从节点我们试着将7000和7003两个节点都关掉 123456789101112131415161718192021222324252627282930[root@centos1 redis]# ps -ef | grep redisroot 1733 1 0 18:21 ? 00:00:45 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:24 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:42 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:29 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:24 bin/redis-server *:7005 [cluster] root 23988 1 0 18:30 ? 00:00:43 ./redis-server *:6379 root 24527 1 0 22:04 ? 00:00:00 bin/redis-server *:7000 [cluster] root 24541 1635 0 22:07 pts/1 00:00:00 grep redis[root@centos1 redis] kill 1743[root@centos1 redis] kill 24527[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7001&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7001)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[ERR] Not all 16384 slots are covered by nodes. 这里我们的集群就不能工作了，因为两个节点主节点和从节点都挂掉了，原来7001分配的slot现在无节点接管，需要人工介入重新分配slots。 集群中加入新的主节点这里在cluster目录下再新建一个7006并修改对应的配置文件，然后启动这个这个redis进程然后再使用redis-trib的add node指令加入节点1bin/redis-trib add-node 127.0.0.1:7006 127.0.0.1:7000 这里前面的节点表示要加入的节点，第二个节点表示要加入的集群中的任意一个节点，用来标识这个集群123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@centos1 redis]# bin/redis-trib add-node 127.0.0.1:7006 127.0.0.1:7000&gt;&gt;&gt; Adding node 127.0.0.1:7006 to cluster 127.0.0.1:7000&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 127.0.0.1:7006 to make it join the cluster.[OK] New node added correctly.[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7006&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots: (0 slots) master 0 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里我们可以看到7006节点已经变成了一个主节点，然鹅，等等，好像发现了有什么地方不对12M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots: (0 slots) master 里面0 slots,也就是说节点6没有分配哈希槽，即不能进行数据的存取，拿我加上去干嘛。。原来redis cluster 不是在新加节点的时候帮我们做好了迁移工作，需要我们手动对集群进行重新分片迁移，也是这个命令：1/bin/redis-trib reshard 127.0.0.1:7000 这个命令是用来迁移slot节点的，后面的127.0.0.1:7000是表示哪个集群的，7000-7006都是可以的1234567891011121314151617181920212223242526272829303132333435[root@centos1]# redis-trib.rb reshard 127.0.0.1:7000Connecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7003: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7006)M: efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 slots: (0 slots) master 0 additional replica(s)M: cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 4b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slots: (0 slots) slave replicates cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6cS: 3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slots: (0 slots) slave replicates d2237fdcfbba672de766b913d1186cebcb6e1761M: dfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 30858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slots: (0 slots) slave replicates dfa0754c7854a874a6ebd2613b86140ad97701fcM: d2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 slots:0-5460 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 它提示我们需要迁移多少slot到7006上，我们可以算一下：16384/4 = 4096，也就是说，为了平衡分配起见，我们需要移动4096个槽点到7006上。 好，那输入4096:它又提示我们，接受的node ID是多少，7006的id 我们通过上面就可以看到是efc3131fbdc6cf929720e0e0f7136cae85657481:12345What is the receiving node ID? efc3131fbdc6cf929720e0e0f7136cae85657481Please enter all the source node IDs. Type &apos;all&apos; to use all the nodes as source nodes for the hash slots. Type &apos;done&apos; once you entered all the source nodes IDs.Source node #1: 接着， redis-trib 会向你询问重新分片的源节点（source node）， 也即是， 要从哪个节点中取出 4096 个哈希槽， 并将这些槽移动到7006节点上面。 如果我们不打算从特定的节点上取出指定数量的哈希槽， 那么可以向 redis-trib 输入 all ， 这样的话， 集群中的所有主节点都会成为源节点， redis-trib 将从各个源节点中各取出一部分哈希槽， 凑够 4096 个， 然后移动到7006节点上：1Source node #1:all 接下来就开始迁移了，并且会询问你是否确认：1234567Moving slot 1359 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1360 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1361 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1362 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1363 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1364 from d2237fdcfbba672de766b913d1186cebcb6e1761Do you want to proceed with the proposed reshard plan (yes/no)? 输入yes并回车后，redis-trib就会正式执行重新分片操作，将制定的哈希槽从源节点一个个移动到7006节点上迁移结束之后，我们来检查一下12345678910111213141516171819202122232425M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:1365-5460 (4096 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 0 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:6827-10922 (4096 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:12288-16383 (4096 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 我们可以看到slots:0-1364,5461-6826,10923-12287 (4096 slots)这些原来在其他节点上的哈希槽都迁移到了7006上 增加一个从节点新建一个 7007从节点，作为7006的从节点 我们再新建一个节点7007，步骤类似，就先省略了。建好后，启动起来，我们看如何把它加入到集群中的从节点中：1[root@centos1]# redis-trib.rb add-node --slave 127.0.0.1:7007 127.0.0.1:7000 add-node的时候加上--slave表示是加入到从节点中，但是这样加，是随机的。这里的命令行完全像我们在添加一个新主服务器时使用的一样，所以我们没有指定要给哪个主服 务器添加副本。这种情况下，redis-trib会将7007作为一个具有较少副本的随机的主服务器的副本。 那么，你猜，它会作为谁的从节点，应该是7006，因为7006还没有从节点。我们运行下。12345678910[root@web3 7007]# redis-trib.rb add-node --slave 127.0.0.1:7007 127.0.0.1:7000......[OK] All 16384 slots covered.Automatically selected master 127.0.0.1:7006Connecting to node 127.0.0.1:7007: OK&gt;&gt;&gt; Send CLUSTER MEET to node 127.0.0.1:7007 to make it join the cluster.Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 127.0.0.1:7006.[OK] New node added correctly. 上面提示说，自动选择了7006作为master节点。并且成功了。我们检查下：1234567891011121314151617181920212223242526272829303132333435363738[root@centos1]# redis-trib.rb check 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7002: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)S: 3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slots: (0 slots) slave replicates d2237fdcfbba672de766b913d1186cebcb6e1761M: efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 1 additional replica(s)S: 4b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slots: (0 slots) slave replicates cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6cS: 30858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slots: (0 slots) slave replicates dfa0754c7854a874a6ebd2613b86140ad97701fcM: d2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 slots:1365-5460 (4096 slots) master 1 additional replica(s)M: cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 slots:6827-10922 (4096 slots) master 1 additional replica(s)S: 86d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 slots: (0 slots) slave replicates efc3131fbdc6cf929720e0e0f7136cae85657481M: dfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 slots:12288-16383 (4096 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 果然，7007加入到了7006的从节点当中。 你说，我如果想指定一个主节点行不行？当然可以。我们再建一个7008节点。1bin/redis-trib.rb add-node --slave --master-id efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7008 127.0.0.1:7000 --master-id 表示指定的主节点node id。这里指定的是 7006 这个主节点。123Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 127.0.0.1:7006.[OK] New node added correctly. 提示我们已经作为7006的从节点了，也就是加入到7006的从节点来了，照这么说，7006就有2个从节点了，我们看一下：1234bin/redis-cli -c -p 7008 cluster nodes |grep efc3131fbdc6cf929720e0e0f7136cae8565748186d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 slave efc3131fbdc6cf929720e0e0f7136cae85657481 0 1445089507786 8 connectedefc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 master - 0 1445089508289 8 connected 0-1364 5461-6826 10923-1228744321e7d619410dc4e0a8745366610a0d06d2395 127.0.0.1:7008 myself,slave efc3131fbdc6cf929720e0e0f7136cae85657481 0 0 0 connected 我们过滤了下看结果，果真，7007和7008是7006的从节点了。 刚好，我们再做一个实验，我把7006的进程杀掉，看7007和7008谁会变成主节点：123456789101112131415161718192021222324252627[root@centos1]# ps -ef|grep redisroot 11384 1 0 09:56 ? 00:00:16 redis-server *:7001 [cluster]root 11388 1 0 09:56 ? 00:00:16 redis-server *:7002 [cluster]root 11392 1 0 09:56 ? 00:00:16 redis-server *:7003 [cluster]root 11396 1 0 09:56 ? 00:00:15 redis-server *:7004 [cluster]root 11400 1 0 09:56 ? 00:00:15 redis-server *:7005 [cluster]root 12100 1 0 11:01 ? 00:00:11 redis-server *:7000 [cluster]root 12132 1 0 11:28 ? 00:00:11 redis-server *:7006 [cluster]root 12202 1 0 13:14 ? 00:00:02 redis-server *:7007 [cluster]root 12219 1 0 13:39 ? 00:00:00 redis-server *:7008 [cluster]root 12239 8259 0 13:49 pts/0 00:00:00 grep redis[root@centos1]# kill 12132[root@centos1]# redis-cli -c -p 7008127.0.0.1:7008&gt; get ss5rtr-&gt; Redirected to slot [1188] located at 127.0.0.1:7007&quot;66&quot;127.0.0.1:7007&gt; cluster nodesefc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 master,fail - 1445089780668 1445089779963 8 disconnectedd2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 master - 0 1445089812195 7 connected 1365-546030858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slave dfa0754c7854a874a6ebd2613b86140ad97701fc 0 1445089813710 3 connected86d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 myself,master - 0 0 10 connected 0-1364 5461-6826 10923-12287cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 master - 0 1445089814214 2 connected 6827-109224b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slave cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 0 1445089812701 2 connected44321e7d619410dc4e0a8745366610a0d06d2395 127.0.0.1:7008 slave 86d05e7c2b197dc182b5e71069e791d033cf899e 0 1445089814214 10 connected3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slave d2237fdcfbba672de766b913d1186cebcb6e1761 0 1445089813204 7 connecteddfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 master - 0 1445089813204 3 connected 12288-16383127.0.0.1:7007&gt; 这里7007获得了成为主节点的机会，7008就变成了7007的从节点。 那么这个时候，重启7006节点，那么他就会变成了一个7007的从节点了。 移除一个节点上面是增加一个节点，接下来就是移除一个节点了，移除节点的命令是1bin/redis-trib del-node 127.0.0.1:7000 `&lt;node-id&gt;` 没我们尝试下输入以下命令123456789101112[root@centos]# bin/redis-trib.rb del-node 127.0.0.1:7000 86d05e7c2b197dc182b5e71069e791d033cf899e&gt;&gt;&gt; Removing node 86d05e7c2b197dc182b5e71069e791d033cf899e from cluster 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7002: OK[ERR] Node 127.0.0.1:7007 is not empty! Reshard data away and try again. 这里报错了，提示我们7007节点里面有数据，让我们把7007节点里的数据移除出去，也就是说需要重新分片，这个和上面增加节点的方式一样，我们再来一遍1bin/redis-trib.rb reshard 127.0.0.1:7000 省去中间内容，原来7007节点上已经有了4096个哈希槽，这里我们也移动4096个哈希槽然后将这些哈希槽移动到7001节点上123Source node #1:86d05e7c2b197dc182b5e71069e791d033cf899eSource node #2:doneDo you want to proceed with the proposed reshard plan (yes/no)? yes 然后我们再继续执行移除命令，结果如下123456789101112131415[root@centos1]# redis-trib.rb del-node 127.0.0.1:7000 86d05e7c2b197dc182b5e71069e791d033cf899e&gt;&gt;&gt; Removing node 86d05e7c2b197dc182b5e71069e791d033cf899e from cluster 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7002: OK&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; 127.0.0.1:7006 as replica of 127.0.0.1:7001&gt;&gt;&gt; 127.0.0.1:7008 as replica of 127.0.0.1:7001&gt;&gt;&gt; SHUTDOWN the node. 删除成功，而且还很人性化的将7006和7008这2个原来7007的附属节点送给了7001。考虑的真周到~ 移除一个从节点移除一个从节点就比较简单了，因为从节点没有哈希槽，也不需要考虑数据迁移，直接移除就行1234567891011121314[root@centos1]# redis-trib.rb del-node 127.0.0.1:7005 44321e7d619410dc4e0a8745366610a0d06d2395&gt;&gt;&gt; Removing node 44321e7d619410dc4e0a8745366610a0d06d2395 from cluster 127.0.0.1:7005Connecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7003: OK&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node.[root@centos1]# redis-trib.rb check 127.0.0.1:7008Connecting to node 127.0.0.1:7008: [ERR] Sorry, can&apos;t connect to node 127.0.0.1:7008 表示移除成功 Redis性能测试Redis自带了性能测试工具redis-benchmark使用说明如下：1234567891011121314151617181920212223Usage: redis-benchmark [-h &lt;host&gt;] [-p &lt;port&gt;] [-c &lt;clients&gt;] [-n &lt;requests]&gt; [-k &lt;boolean&gt;] -h &lt;hostname&gt; Server hostname (default 127.0.0.1) -p &lt;port&gt; Server port (default 6379) -s &lt;socket&gt; Server socket (overrides host and port) -c &lt;clients&gt; Number of parallel connections (default 50) -n &lt;requests&gt; Total number of requests (default 10000) -d &lt;size&gt; Data size of SET/GET value in bytes (default 2) -k &lt;boolean&gt; 1=keep alive 0=reconnect (default 1) -r &lt;keyspacelen&gt; Use random keys for SET/GET/INCR, random values for SADD Using this option the benchmark will get/set keys in the form mykey_rand:000000012456 instead of constant keys, the &lt;keyspacelen&gt; argument determines the max number of values for the random number. For instance if set to 10 only rand:000000000000 - rand:000000000009 range will be allowed. -P &lt;numreq&gt; Pipeline &lt;numreq&gt; requests. Default 1 (no pipeline). -q Quiet. Just show query/sec values --csv Output in CSV format -l Loop. Run the tests forever -t &lt;tests&gt; Only run the comma-separated list of tests. The test names are the same as the ones produced as output. -I Idle mode. Just open N idle connections and wait. 基准测试基准的测试命令：redis-benchmark -q -n 100000结果入下：1234567891011121314151617181920root@centos1 bin]# redis-benchmark -q -n 100000-bash: redis-benchmark: command not found[root@centos1 bin]# ./redis-benchmark -q -n 100000PING_INLINE: 61576.36 requests per secondPING_BULK: 60277.28 requests per secondSET: 61349.69 requests per secondGET: 60459.49 requests per secondINCR: 58858.15 requests per secondLPUSH: 59066.75 requests per secondRPUSH: 57339.45 requests per secondLPOP: 55586.44 requests per secondRPOP: 56465.27 requests per secondSADD: 57045.07 requests per secondSPOP: 53734.55 requests per secondLPUSH (needed to benchmark LRANGE): 57012.54 requests per secondLRANGE_100 (first 100 elements): 55803.57 requests per secondLRANGE_300 (first 300 elements): 54914.88 requests per secondLRANGE_500 (first 450 elements): 53333.33 requests per secondLRANGE_600 (first 600 elements): 56529.11 requests per secondMSET (10 keys): 59276.82 requests per second 这里可以看出，单机版的redis每秒可以处理6万个请求，这已经是一个非常厉害的数据了，不得不佩服我们再来看下集群情况下是是什么情况123456789101112131415161718[root@centos1 bin]# ./redis-benchmark -q -n 100000 -p 7000PING_INLINE: 64599.48 requests per secondPING_BULK: 64184.85 requests per secondSET: 66800.27 requests per secondGET: 65616.80 requests per secondINCR: 66269.05 requests per secondLPUSH: 40273.86 requests per secondRPUSH: 40355.12 requests per secondLPOP: 43421.62 requests per secondRPOP: 45187.53 requests per secondSADD: 62539.09 requests per secondSPOP: 61538.46 requests per secondLPUSH (needed to benchmark LRANGE): 38182.51 requests per secondLRANGE_100 (first 100 elements): 25555.84 requests per secondLRANGE_300 (first 300 elements): 9571.21 requests per secondLRANGE_500 (first 450 elements): 7214.49 requests per secondLRANGE_600 (first 600 elements): 5478.85 requests per secondMSET (10 keys): 41893.59 requests per second 这里看出大部分和单机版的性能查不多，主要是lrange命令的差别是很大的 流水线测试使用流水线默认情况下，每个客户端都是在一个请求完成之后才发送下一个请求（基准会模拟50个客户端除非使用-c指定特别的数量），这意味着服务器几乎是按顺序读取每个客户端的命令。RTT也加入了其中。真实世界会更复杂，Redis支持/topics/pipelining，使得可以一次性执行多条命令成为可能。Redis流水线可以提高服务器的TPSredis-benchmark -n 1000000 -t set,get -P 16 -q 加入-P选项使用管道技术，一次执行多条命令123./redis-benchmark -n 1000000 -t set,get -P 16 -qSET: 515198.34 requests per secondGET: 613873.56 requests per second 每秒处理get/sret请求达到了60/50W,真的厉害！ 遇到的问题 安装redis集群的时候遇到了挺多问题，踩了很多坑，单单是修改配置文件就出了不少问题，那些配置文件的内容都要一一修改，有些配置不修改就会出现无法创建进程的错误 注意配置集群的时候不要加密码，否则会出现无法连接的情况 gem install的时候需要修改镜像或者翻墙 昨天启动成功，今天启动的时候报错1[ERR] Node 172.168.63.202:7001 is not empty. Either the nodealready knows other nodes (check with CLUSTER NODES) or contains some key in database 0 解决方法：1). 将需要新增的节点下aof、rdb等本地备份文件删除；2). 同时将新Node的集群配置文件删除,即：删除你redis.conf里面cluster-config-file所在的文件；3). 再次添加新节点如果还是报错，则登录新Node,执行bin/redis-cli–h x –p对数据库进行清除：1173.172.168.63.201:7001&gt; flushdb #清空当前数据库 总结之间对了Redis的了解并不是说非常多，只是简单的会用，因为现在企业里也很多都在用，刚好老大说接下来的项目可能会用到Redis集群，让我先去了解下，所以最近就在回头看，一边看文档，博客，一边实践，踩了很多的坑，出问题的时候的确是让人感到很痛苦很郁闷的，可是当运行成功的那一刻心情却是无比激动和开心的，可能这就是编程的魅力吧。]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产者消费者的五种实现方式]]></title>
    <url>%2F2017%2F06%2F30%2FJava%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BA%94%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言生产者和消费者问题是线程模型中的经典问题：生产者和消费者在同一时间段内共用同一个存储空间，生产者往存储空间中添加产品，消费者从存储空间中取走产品，当存储空间为空时，消费者阻塞，当存储空间满时，生产者阻塞。现在用五种方式来实现生产者消费者模型 wait()和notify()方法的实现这也是最简单最基础的实现，缓冲区满和为空时都调用wait()方法等待，当生产者生产了一个产品或者消费者消费了一个产品之后会唤醒所有线程。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 生产者和消费者，wait()和notify()的实现 * @author ZGJ * @date 2017年6月22日 */public class Test1 &#123; private static Integer count = 0; private static final Integer FULL = 10; private static String LOCK = "lock"; public static void main(String[] args) &#123; Test1 test1 = new Test1(); new Thread(test1.new Producer()).start(); new Thread(test1.new Consumer()).start(); new Thread(test1.new Producer()).start(); new Thread(test1.new Consumer()).start(); new Thread(test1.new Producer()).start(); new Thread(test1.new Consumer()).start(); new Thread(test1.new Producer()).start(); new Thread(test1.new Consumer()).start(); &#125; class Producer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; synchronized (LOCK) &#123; while (count == FULL) &#123; try &#123; LOCK.wait(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; count++; System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count); LOCK.notifyAll(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (LOCK) &#123; while (count == 0) &#123; try &#123; LOCK.wait(); &#125; catch (Exception e) &#123; &#125; &#125; count--; System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count); LOCK.notifyAll(); &#125; &#125; &#125; &#125;&#125; 结果:12345678910111213141516171819202122232425262728293031Thread-0生产者生产，目前总共有1Thread-4生产者生产，目前总共有2Thread-3消费者消费，目前总共有1Thread-1消费者消费，目前总共有0Thread-2生产者生产，目前总共有1Thread-6生产者生产，目前总共有2Thread-7消费者消费，目前总共有1Thread-5消费者消费，目前总共有0Thread-0生产者生产，目前总共有1Thread-4生产者生产，目前总共有2Thread-3消费者消费，目前总共有1Thread-6生产者生产，目前总共有2Thread-1消费者消费，目前总共有1Thread-7消费者消费，目前总共有0Thread-2生产者生产，目前总共有1Thread-5消费者消费，目前总共有0Thread-0生产者生产，目前总共有1Thread-4生产者生产，目前总共有2Thread-3消费者消费，目前总共有1Thread-7消费者消费，目前总共有0Thread-6生产者生产，目前总共有1Thread-2生产者生产，目前总共有2Thread-1消费者消费，目前总共有1Thread-5消费者消费，目前总共有0Thread-0生产者生产，目前总共有1Thread-4生产者生产，目前总共有2Thread-3消费者消费，目前总共有1Thread-1消费者消费，目前总共有0Thread-6生产者生产，目前总共有1Thread-7消费者消费，目前总共有0Thread-2生产者生产，目前总共有1 可重入锁ReentrantLock的实现java.util.concurrent.lock 中的 Lock 框架是锁定的一个抽象，通过对lock的lock()方法和unlock()方法实现了对锁的显示控制，而synchronize()则是对锁的隐性控制。可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响，简单来说，该锁维护这一个与获取锁相关的计数器，如果拥有锁的某个线程再次得到锁，那么获取计数器就加1，函数调用结束计数器就减1，然后锁需要被释放两次才能获得真正释放。已经获取锁的线程进入其他需要相同锁的同步代码块不会被阻塞。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 生产者和消费者，ReentrantLock的实现 * * @author ZGJ * @date 2017年6月22日 */public class Test2 &#123; private static Integer count = 0; private static final Integer FULL = 10; //创建一个锁对象 private Lock lock = new ReentrantLock(); //创建两个条件变量，一个为缓冲区非满，一个为缓冲区非空 private final Condition notFull = lock.newCondition(); private final Condition notEmpty = lock.newCondition(); public static void main(String[] args) &#123; Test2 test2 = new Test2(); new Thread(test2.new Producer()).start(); new Thread(test2.new Consumer()).start(); new Thread(test2.new Producer()).start(); new Thread(test2.new Consumer()).start(); new Thread(test2.new Producer()).start(); new Thread(test2.new Consumer()).start(); new Thread(test2.new Producer()).start(); new Thread(test2.new Consumer()).start(); &#125; class Producer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; //获取锁 lock.lock(); try &#123; while (count == FULL) &#123; try &#123; notFull.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; count++; System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count); //唤醒消费者 notEmpty.signal(); &#125; finally &#123; //释放锁 lock.unlock(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; lock.lock(); try &#123; while (count == 0) &#123; try &#123; notEmpty.await(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; count--; System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count); notFull.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; &#125;&#125; 阻塞队列BlockingQueue的实现BlockingQueue即阻塞队列，从阻塞这个词可以看出，在某些情况下对阻塞队列的访问可能会造成阻塞。被阻塞的情况主要有如下两种: 当队列满了的时候进行入队列操作 当队列空了的时候进行出队列操作 因此，当一个线程对已经满了的阻塞队列进行入队操作时会阻塞，除非有另外一个线程进行了出队操作，当一个线程对一个空的阻塞队列进行出队操作时也会阻塞，除非有另外一个线程进行了入队操作。从上可知，阻塞队列是线程安全的。下面是BlockingQueue接口的一些方法: 操作 抛异常 特定值 阻塞 超时 插入 add(o) offer(o) put(o) offer(o, timeout, timeunit) 移除 remove(o) poll(o) take(o) poll(timeout, timeunit) 检查 element(o) peek(o) 这四类方法分别对应的是：1 . ThrowsException：如果操作不能马上进行，则抛出异常2 . SpecialValue：如果操作不能马上进行，将会返回一个特殊的值，一般是true或者false3 . Blocks:如果操作不能马上进行，操作会被阻塞4 . TimesOut:如果操作不能马上进行，操作会被阻塞指定的时间，如果指定时间没执行，则返回一个特殊值，一般是true或者false下面来看由阻塞队列实现的生产者消费者模型,这里我们使用take()和put()方法，这里生产者和生产者，消费者和消费者之间不存在同步，所以会出现连续生成和连续消费的现象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;/** * 使用BlockingQueue实现生产者消费者模型 * @author ZGJ * @date 2017年6月29日 */public class Test3 &#123; private static Integer count = 0; //创建一个阻塞队列 final BlockingQueue&lt;Integer&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(10); public static void main(String[] args) &#123; Test3 test3 = new Test3(); new Thread(test3.new Producer()).start(); new Thread(test3.new Consumer()).start(); new Thread(test3.new Producer()).start(); new Thread(test3.new Consumer()).start(); new Thread(test3.new Producer()).start(); new Thread(test3.new Consumer()).start(); new Thread(test3.new Producer()).start(); new Thread(test3.new Consumer()).start(); &#125; class Producer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; try &#123; blockingQueue.put(1); count++; System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; try &#123; blockingQueue.take(); count--; System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 信号量Semaphore的实现Semaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源，在操作系统中是一个非常重要的问题，可以用来解决哲学家就餐问题。Java中的Semaphore维护了一个许可集，一开始先设定这个许可集的数量，可以使用acquire()方法获得一个许可，当许可不足时会被阻塞，release()添加一个许可。在下列代码中，还加入了另外一个mutex信号量，维护生产者消费者之间的同步关系，保证生产者和消费者之间的交替进行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.concurrent.Semaphore;/** * 使用semaphore信号量实现 * @author ZGJ * @date 2017年6月29日 */public class Test4 &#123; private static Integer count = 0; //创建三个信号量 final Semaphore notFull = new Semaphore(10); final Semaphore notEmpty = new Semaphore(0); final Semaphore mutex = new Semaphore(1); public static void main(String[] args) &#123; Test4 test4 = new Test4(); new Thread(test4.new Producer()).start(); new Thread(test4.new Consumer()).start(); new Thread(test4.new Producer()).start(); new Thread(test4.new Consumer()).start(); new Thread(test4.new Producer()).start(); new Thread(test4.new Consumer()).start(); new Thread(test4.new Producer()).start(); new Thread(test4.new Consumer()).start(); &#125; class Producer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; notFull.acquire(); mutex.acquire(); count++; System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; mutex.release(); notEmpty.release(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; try &#123; notEmpty.acquire(); mutex.acquire(); count--; System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; mutex.release(); notFull.release(); &#125; &#125; &#125; &#125;&#125; 管道输入输出流PipedInputStream和PipedOutputStream实现在java的io包下，PipedOutputStream和PipedInputStream分别是管道输出流和管道输入流。它们的作用是让多线程可以通过管道进行线程间的通讯。在使用管道通信时，必须将PipedOutputStream和PipedInputStream配套使用。使用方法：先创建一个管道输入流和管道输出流，然后将输入流和输出流进行连接，用生产者线程往管道输出流中写入数据，消费者在管道输入流中读取数据，这样就可以实现了不同线程间的相互通讯，但是这种方式在生产者和生产者、消费者和消费者之间不能保证同步，也就是说在一个生产者和一个消费者的情况下是可以生产者和消费者之间交替运行的，多个生成者和多个消费者者之间则不行12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * 使用管道实现生产者消费者模型 * @author ZGJ * @date 2017年6月30日 */public class Test5 &#123; final PipedInputStream pis = new PipedInputStream(); final PipedOutputStream pos = new PipedOutputStream(); &#123; try &#123; pis.connect(pos); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; class Producer implements Runnable &#123; @Override public void run() &#123; try &#123; while(true) &#123; Thread.sleep(1000); int num = (int) (Math.random() * 255); System.out.println(Thread.currentThread().getName() + "生产者生产了一个数字，该数字为： " + num); pos.write(num); pos.flush(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; pos.close(); pis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; class Consumer implements Runnable &#123; @Override public void run() &#123; try &#123; while(true) &#123; Thread.sleep(1000); int num = pis.read(); System.out.println("消费者消费了一个数字，该数字为：" + num); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; pos.close(); pis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; Test5 test5 = new Test5(); new Thread(test5.new Producer()).start(); new Thread(test5.new Consumer()).start(); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之——单例模式]]></title>
    <url>%2F2017%2F05%2F19%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言 单例模式是一种常用的软件设计模式。在它的核心结构中只包含一个被称为单例的特殊类。通过单例模式可以保证系统中一个类只有一个实例。即一个类只有一个对象实例。我们会理所当然的认为单例模式很简单，其实单例模式是事先方式有很多种，现在就来一一介绍一下。 懒汉式（线程不安全）1234567891011121314public class Singleton &#123; private static Singleton instance; /** * 构造函数私有化 */ private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这种写法呢，有懒加载的作用，但是在多线程环境下会线程不安全为什么会线程不安全呢？我们假设一下，假如线程A和线程B同时调用getInstance()方法，线程A先执行判断if(instance == null)，判断instance对象是空的，这时候线程B获得了CPU执行权，它也判断instance对象是控制，这个时候执行了instance = new Singleton();这段代码，创建了一个对象并把这个对象的引用赋值给了instance，这个时候线程A又获得了执行权，之前已经判断过对象为空了，所以线程A又new了一个新的对象，这个时候就不符合单例模式的要求了，所以这种懒汉式是线程不安全的 懒汉式（线程安全）1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123; &#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这个时候，我们加上了synchronized关键字之后，相当于给这个对象加上了锁，就起到了同步的作用，同时只能有一个线程访问这个方法，不过这样的话也降低了效率 饿汉式12345678public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123; &#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 这种方式基于classloder机制避免了多线程的同步问题，在类初始化的时候就实例化这个对象 静态方法块12345678910public class Singleton &#123; private Singleton instance = null; static &#123; instance = new Singleton(); &#125; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return this.instance; &#125;&#125; 这种方式和上一种区别不大，都是在类初始化即实例化instance对象 静态内部类12345678910public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123; &#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 这种方式同样利用了classloder的类加载机制来保证初始化instance时只有一个线程，它和第三种饿汉式，第四种静态方法块不同的是：第三种和第四种方式是只要Singleton类被装载了，那么instance就会被实例化（没有达到lazy loading效果），而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance 双重检查锁定1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 双重检查锁定在C语言或者其他语言中已被广泛当做多线程环境下延迟初始化的一种高效手段，但是在Java中并不能很好的实现，这个涉及到Java的内存模型，所以需要加上volatile关键字，这个关键字的作用是保证内存可见性和禁止指令重排序 枚举12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125;&#125; 枚举是JDK1.5之后才有的语法糖，实际上我们创建enum时，编译器会自动为我们生成一个继承自Java.lang.Enum的类首先，在枚举中明确了构造方法限制为私有，在我们访问枚举实例时会执行构造方法，同时每个枚举实例都是static final类型的，也就表明只能被实例化一次。在调用构造方法时，我们的单例被实例化。也就是说，因为enum中的实例被保证只会被实例化一次，所以我们的INSTANCE也被保证实例化一次。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tomcat配置]]></title>
    <url>%2F2017%2F05%2F15%2FTomcat%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言我们都知道Tomcat服务器是我们在学习JavaWeb中最常使用的服务器，所以了解Tomcat的配置文件显得很重要，昨天去面试的时候，被面试官问了几个关于Tomcat配置文件的几个问题， 以前配过，但是后来都忘记了，现在来回顾一下Tomcat中比较常用得到的配置文件吧 配置文件的位置Tomcat的配置文件在conf目录下，有context.xml、server.xml、tomcat-users.xml、web.xml这些配置文件 server.xml修改端口如果要修改连接的端口，在server.xml文件的Server标签和Connectot标签下修改如下：1&lt;Server port="8005" shutdown="SHUTDOWN"&gt; 1234&lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt;&lt;Connector port="8009" protocol="AJP/1.3" redirectPort="8443" /&gt; 将port=xxxx改成其他端口即可 修改虚拟目录在server.xml中找到Host结点1234567891011121314151617&lt;Host name="localhost" appBase="webapps" unpackWARs="true" autoDeploy="true"&gt; &lt;!-- SingleSignOn valve, share authentication between web applications Documentation at: /docs/config/valve.html --&gt; &lt;!-- &lt;Valve className="org.apache.catalina.authenticator.SingleSignOn" /&gt; --&gt; &lt;!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern="common" --&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log." suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt;&lt;/Host&gt; Host结点代表一个主机，name对应的是其域名，我们可以通过修改name的属性来改变Tomcat资源的访问路径，方便我们管理 tomcat-userstoncat-users.xml是配置用户登录Tomcat对app进行管理的配置文件，如果我们需要登录Tomcat，这需要在改配置文件的tomcat-users结点下加上如下代码123 &lt;role rolename="manager-gui"/&gt;&lt;role rolename="manager-script"/&gt;&lt;user username="tomcat" password="123456" roles="manager-gui,manager-script"/&gt; 其中，我这里配置的登录名是utomcat,密码是123456我们打开Tomcat主页，点击Manager App 输入用户名和密码，进入到管理界面 这个时候我们就可以对我们的app进行管理了]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Socket 编程那些事（1）]]></title>
    <url>%2F2017%2F05%2F02%2FJava%20Socket%20%E7%BC%96%E7%A8%8B%E9%82%A3%E4%BA%9B%E4%BA%8B%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言 最近在准备面试和笔试的一些东西，回去翻看了Java关于IO的基础，发现很多基础还是没有记牢固，现在回头重新学习，就从socket通讯开始吧，虽然说现在企业很少直接编写socket，都是使用一些封装好的框架，netty,mina等。不过对于这些基础的知识，还是需要掌握牢固的，对于以后学习更深的框架和笔试面试都很有裨益。 BIO、NIO、AIO的区别Java中的IO的方式有三种Java BIO ： 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。在JDK1.4以前采用这种方式Java NIO ： 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。Java AIO(NIO.2) ： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理， 曾经在一篇文章中看到这么很好的比喻 如果你想吃一份宫保鸡丁盖饭： 同步阻塞：你到饭馆点餐，然后在那等着，还要一边喊：好了没啊！ 同步非阻塞：在饭馆点完餐，就去遛狗了。不过溜一会儿，就回饭馆喊一声：好了没啊！ 异步阻塞：遛狗的时候，接到饭馆电话，说饭做好了，让您亲自去拿。 异步非阻塞：饭馆打电话说，我们知道您的位置，一会给你送过来，安心遛狗就可以了。 这里我们从最简单的BIO开始学习吧 单线程通讯首先我们先新建一个Server类，这个类将监听12345这个端口，等待客户端的连接,客户端与服务器连接后，客户端可以像服务器发送信息，如果发送bye，则结束通讯1234567891011121314151617181920212223242526272829303132333435363738package edu.gduf.bio.socket.demo1;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.PrintWriter;import java.net.ServerSocket;import java.net.Socket;/** * 基于bio的socket服务器端 * * @author ZGJ * @date 2017年5月4日 */public class Server &#123; public static void main(String[] args) &#123; //创建一个serversocket在10000号端口监听 try (ServerSocket server = new ServerSocket(12345)) &#123; //等待客户端连接，将一直阻塞直到有客户端连接 Socket socket = server.accept(); BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); PrintWriter out = new PrintWriter(socket.getOutputStream()); while (true) &#123; String msg = in.readLine(); System.out.println(msg); out.println("Server received " + msg); out.flush(); if (msg.equals("bye")) &#123; break; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 这里try (ServerSocket server = new ServerSocket(12345))采用了java7的try-with-resources语法，这样写的好处是try里面声明的资源在最后都会自动关闭，而不需要我们手动关闭，简化了语法 Client类1234567891011121314151617181920212223242526272829303132333435363738package edu.gduf.bio.socket.demo1;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.PrintWriter;import java.net.Socket;/** * 基于bio的socket客户端 * * @author ZGJ * @date 2017年5月4日 */public class Client &#123; public static void main(String[] args) &#123; try (Socket socket = new Socket("127.0.0.1", 12345)) &#123; //获取socket输入流 BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); //socket输出流 PrintWriter out = new PrintWriter(socket.getOutputStream()); //标准输入流 BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); while (true) &#123; String msg = reader.readLine(); out.println(msg); out.flush(); if (msg.equals("bye")) &#123; break; &#125; System.out.println(in.readLine()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果：Client: helloServer received hellohahaServer received hahabye Server:hellohahabye 多线程通讯我们发现，服务器只能处理一个客户端的请求，服务器处理了第一个客户端的请求之后，后续的Client就不能再连接，这个时候我们需要做一些改动，当服务器收到客户端的连接请求后，客户端的请求放在一个新的线程中去处理，而主线程仍然继续等待其他客户端的连接，这样就不会阻塞服务器处理其他客户端的请求了，只需要修改服务器的代码，客户端代码和原来的一样12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package edu.gduf.bio.socket.demo2;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.PrintWriter;import java.net.ServerSocket;import java.net.Socket;/** * 处理多个client的server * @author ZGJ * @date 2017年5月4日 */public class MulitClientServer &#123; public static void main(String[] args) &#123; try(ServerSocket server = new ServerSocket(12345)) &#123; //循环接受客户端请求 while(true) &#123; Socket socket = server.accept(); handle(socket); &#125; &#125; catch(IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 处理客户请求 * @param socket */ private static void handle(Socket socket) &#123; //开启一个线程,lambda表达式 new Thread(() -&gt; &#123; try(BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream())); PrintWriter writer = new PrintWriter(socket.getOutputStream(), true)) &#123; while(true) &#123; String message = in.readLine(); System.out.println(message); writer.println("Server received: " + message); if("bye".equals(message)) &#123; break; &#125; &#125; &#125; catch(IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 我们可以看到，当服务器接受到一个客户单的请求之后，采用handle()方法去处理客户端的请求，这里使用了Java8中lambda表达式来替代匿名内部类的语法，来简化我们的编程，使我们的语法更加简洁。对于每一个客户端的请求都开启一个新的线程去处理，当然，如果还想做到更加优化，可以采用Executor线程池去处理，节省创建和关闭线程的开销。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>io</tag>
        <tag>socket</tag>
      </tags>
  </entry>
</search>